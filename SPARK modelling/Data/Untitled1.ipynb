{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now not pandas but SPARK needs to read the files\n",
    "\n",
    "h_cdr = spark.read.csv(\"C:\\\\Users\\\\inkpathak\\\\Desktop\\\\Hackathon\\\\Data\\\\cdr.csv\", header = True)\n",
    "h_complaint = spark.read.csv(\"C:\\\\Users\\\\inkpathak\\\\Desktop\\\\Hackathon\\\\Data\\\\complaint.csv\", header = True)\n",
    "h_cust_churn = spark.read.csv(\"C:\\\\Users\\\\inkpathak\\\\Desktop\\\\Hackathon\\\\Data\\\\customer_churn_demography.csv\", header = True)\n",
    "h_revenue = spark.read.csv(\"C:\\\\Users\\\\inkpathak\\\\Desktop\\\\Hackathon\\\\Data\\\\revenue.csv\", header = True)\n",
    "h_signal = spark.read.csv(\"C:\\\\Users\\\\inkpathak\\\\Desktop\\\\Hackathon\\\\Data\\\\signalstrength.csv\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_df1 = (h_cust_churn.join(h_cdr, \"phone number\")\n",
    "        .join(h_complaint, \"phone number\")\n",
    "        .join(h_revenue, \"phone number\")\n",
    "        .join(h_signal, \"phone number\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "h_final = h_df1.withColumn(\"account length\", h_df1[\"account length\"].cast(FloatType()))\\\n",
    ".withColumn(\"account (days)\", h_df1[\"account (days)\"].cast(FloatType()))\\\n",
    ".withColumn(\"Age\", h_df1[\"Age\"].cast(FloatType()))\\\n",
    ".withColumn(\"%call drops\", h_df1[\"%call drops\"].cast(FloatType()))\\\n",
    ".withColumn(\"%Packet_drops\", h_df1[\"%Packet_drops\"].cast(FloatType()))\\\n",
    ".withColumn(\"number vmail messages\", h_df1[\"number vmail messages\"].cast(FloatType()))\\\n",
    ".withColumn(\"total day minutes\", h_df1[\"total day minutes\"].cast(FloatType()))\\\n",
    ".withColumn(\"total day calls\", h_df1[\"total day calls\"].cast(FloatType()))\\\n",
    ".withColumn(\"total eve minutes\", h_df1[\"total eve minutes\"].cast(FloatType()))\\\n",
    ".withColumn(\"total eve calls\", h_df1[\"total eve calls\"].cast(FloatType()))\\\n",
    ".withColumn(\"total night minutes\", h_df1[\"total night minutes\"].cast(FloatType()))\\\n",
    ".withColumn(\"total night calls\", h_df1[\"total night calls\"].cast(FloatType()))\\\n",
    ".withColumn(\"total intl minutes\", h_df1[\"total intl minutes\"].cast(FloatType()))\\\n",
    ".withColumn(\"total intl calls\", h_df1[\"total intl calls\"].cast(FloatType()))\\\n",
    ".withColumn(\"customer service calls\", h_df1[\"customer service calls\"].cast(FloatType()))\\\n",
    ".withColumn(\"Number of complaint raised\", h_df1[\"Number of complaint raised\"].cast(FloatType()))\\\n",
    ".withColumn(\"Time for resolution\", h_df1[\"Time for resolution\"].cast(FloatType()))\\\n",
    ".withColumn(\"payment_made_but_order_not_placed_and_amount_is_deducted_from_my_bank_account\", h_df1[\"payment_made_but_order_not_placed_and_amount_is_deducted_from_my_bank_account\"].cast(FloatType()))\\\n",
    ".withColumn(\"Billing_issue\", h_df1[\"Billing_issue\"].cast(FloatType()))\\\n",
    ".withColumn(\"Calls_are_not_going\", h_df1[\"Calls_are_not_going\"].cast(FloatType()))\\\n",
    ".withColumn(\"drop_calls\", h_df1[\"drop_calls\"].cast(FloatType()))\\\n",
    ".withColumn(\"No_internet_connection\", h_df1[\"No_internet_connection\"].cast(FloatType()))\\\n",
    ".withColumn(\"order\", h_df1[\"order\"].cast(FloatType()))\\\n",
    ".withColumn(\"payment_refund\", h_df1[\"payment_refund\"].cast(FloatType()))\\\n",
    ".withColumn(\"recharge_offer_service_fraud\", h_df1[\"recharge_offer_service_fraud\"].cast(FloatType()))\\\n",
    ".withColumn(\"Slow_network_coverage\", h_df1[\"Slow_network_coverage\"].cast(FloatType()))\\\n",
    ".withColumn(\"uninformed___unsubscribed_weekly_amount_deduction\", h_df1[\"uninformed___unsubscribed_weekly_amount_deduction\"].cast(FloatType()))\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h_df2= h_final.select('%call drops', 'total day minutes','total intl calls','customer service calls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_rdd=h_df2.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1=data_train_rdd.map(lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2=data_train_rdd.map(lambda x:x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "NewVector=data_train_rdd.map(lambda x:x[0:6]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o388.corr.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 29, localhost, executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109)\r\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:153)\r\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\r\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:192)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:204)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:204)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:204)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:407)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:215)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1988)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:170)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1358)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1331)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1372)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1371)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:61)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:331)\r\n\tat org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelationMatrix(PearsonCorrelation.scala:49)\r\n\tat org.apache.spark.mllib.stat.correlation.Correlations$.corrMatrix(Correlation.scala:66)\r\n\tat org.apache.spark.mllib.stat.Statistics$.corr(Statistics.scala:74)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.corr(PythonMLLibAPI.scala:846)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:483)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109)\r\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:153)\r\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\r\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:192)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:204)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:204)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:204)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:407)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:215)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1988)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:170)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-ce0ad376f125>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorrelation_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStatistics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNewVector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pearson\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\software_setup_for_hackathon\\spark\\python\\pyspark\\mllib\\stat\\_statistics.py\u001b[0m in \u001b[0;36mcorr\u001b[1;34m(x, y, method)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"corr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_to_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"corr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software_setup_for_hackathon\\spark\\python\\pyspark\\mllib\\common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software_setup_for_hackathon\\spark\\python\\pyspark\\mllib\\common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software_setup_for_hackathon\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o388.corr.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 29, localhost, executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109)\r\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:153)\r\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\r\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:192)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:204)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:204)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:204)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:407)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:215)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1988)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:170)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1358)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1331)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1372)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1371)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:61)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:331)\r\n\tat org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelationMatrix(PearsonCorrelation.scala:49)\r\n\tat org.apache.spark.mllib.stat.correlation.Correlations$.corrMatrix(Correlation.scala:66)\r\n\tat org.apache.spark.mllib.stat.Statistics$.corr(Statistics.scala:74)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.corr(PythonMLLibAPI.scala:846)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:483)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109)\r\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:153)\r\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\r\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:192)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:204)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:204)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:204)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:407)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:215)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1988)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:170)\r\n"
     ]
    }
   ],
   "source": [
    "correlation_matrix = Statistics.corr(NewVector, method=\"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.1181646  -0.05113165  0.13488199]\n",
      " [ 0.1181646   1.          0.00803336 -0.01342319]\n",
      " [-0.05113165  0.00803336  1.         -0.0175606 ]\n",
      " [ 0.13488199 -0.01342319 -0.0175606   1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# result can be used w/ seaborn's heatmap\n",
    "def compute_correlation_matrix(df, method='pearson'):\n",
    "    # wrapper around\n",
    "    # https://forums.databricks.com/questions/3092/how-to-calculate-correlation-matrix-with-all-colum.html\n",
    "    df_rdd = df.rdd.map(lambda row: row[0:])\n",
    "    corr_mat = Statistics.corr(df_rdd, method=method)\n",
    "    corr_mat_df = pd.DataFrame(corr_mat,\n",
    "                    columns=df.columns, \n",
    "                    index=df.columns)\n",
    "    return corr_mat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "jjj=compute_correlation_matrix(df=h_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>%call drops</th>\n",
       "      <th>total day minutes</th>\n",
       "      <th>total intl calls</th>\n",
       "      <th>customer service calls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>%call drops</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.118165</td>\n",
       "      <td>-0.051132</td>\n",
       "      <td>0.134882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total day minutes</th>\n",
       "      <td>0.118165</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>-0.013423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total intl calls</th>\n",
       "      <td>-0.051132</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.017561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer service calls</th>\n",
       "      <td>0.134882</td>\n",
       "      <td>-0.013423</td>\n",
       "      <td>-0.017561</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        %call drops  total day minutes  total intl calls  \\\n",
       "%call drops                1.000000           0.118165         -0.051132   \n",
       "total day minutes          0.118165           1.000000          0.008033   \n",
       "total intl calls          -0.051132           0.008033          1.000000   \n",
       "customer service calls     0.134882          -0.013423         -0.017561   \n",
       "\n",
       "                        customer service calls  \n",
       "%call drops                           0.134882  \n",
       "total day minutes                    -0.013423  \n",
       "total intl calls                     -0.017561  \n",
       "customer service calls                1.000000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jjj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
