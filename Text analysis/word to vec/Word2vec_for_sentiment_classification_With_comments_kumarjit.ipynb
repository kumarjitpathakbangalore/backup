{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this excercise we are goin to see how we can use word to vector model for sentiment classification\n",
    "\n",
    "#### Before starting let's download the google's word to vector \"GoogleNews-vectors-negative300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inkpathak\\AppData\\Local\\Continuum\\anaconda2\\envs\\py34t\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# load the google word2vec model this takes time and memory and hence it's wise to do it first and them move to other\n",
    "from gensim.models import KeyedVectors\n",
    "filename = 'Word2vec\\\\GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inkpathak\\AppData\\Local\\Continuum\\anaconda2\\envs\\py34t\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "import numpy as np \n",
    "import scipy as sp \n",
    "import matplotlib as mpl \n",
    "import matplotlib.cm as cm \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have the text files in two seperate folders.\n",
    "    * Neg (negatative)\n",
    "    * Pos (positive)\n",
    "    \n",
    "Below function will go to each folder and read the files individually and assign the label based on the folder name. so that all files in Neg(negative) folder would be assigned with Negative label and sinmilary for Pos(positive) folder all the text files will be labeled as Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_data = pd.DataFrame(columns = (\"Text\", \"Class\")) \n",
    "for cla in glob.glob(\"C:/Users/inkpathak/Desktop/Anuj Gupta/Word2vec/txt_sentoken/*\"): # Here the folder name is data set in which there are two sub-folder \"Spam\" & \"Ham\"\n",
    "    clas = cla.split(os.sep)[1]    # We are splitting the folder names as class using OS-Seperator and taking the 2nd item in the list\n",
    "    for file in glob.glob(cla + \"/*.txt\"): # Here we are deep diving in each of the folder and reading the text files one by one\n",
    "        text = open(file, \"r\", encoding = \"ISO-8859-1\").read() # Reading the file , for Windows generally we need to mention the encoding \n",
    "        text = \" \".join(text.split(\"\\n\")) # Splitting the text files and rejoining into a single text\n",
    "        senti_data = senti_data.append(pd.Series([text, clas], index = [\"Text\", \"Class\"]), ignore_index = True) # continious append to the data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's view the data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the happy bastard's quick movie review  damn t...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Class\n",
       "0  plot : two teen couples go to a church party ,...   neg\n",
       "1  the happy bastard's quick movie review  damn t...   neg\n",
       "2  it is movies like these that make a jaded movi...   neg\n",
       "3   \" quest for camelot \" is warner bros . ' firs...   neg\n",
       "4  synopsis : a mentally unstable man undergoing ...   neg"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting label to a numerical variable is on of the good practice for binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert label to a numerical variable\n",
    "senti_data['Class'] = senti_data.Class.map({'neg':0, 'pos':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the happy bastard's quick movie review  damn t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Class\n",
       "0  plot : two teen couples go to a church party ,...      0\n",
       "1  the happy bastard's quick movie review  damn t...      0\n",
       "2  it is movies like these that make a jaded movi...      0\n",
       "3   \" quest for camelot \" is warner bros . ' firs...      0\n",
       "4  synopsis : a mentally unstable man undergoing ...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's quickly do standard data cleaning:\n",
    "####    Following cleaning is applied on the current dataset-\n",
    "        * Removal of stopwords\n",
    "        * removal of alpha neumeric character \n",
    "        * removal of numbers\n",
    "        * removal of special symbols\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    doc = \" \".join([i.replace('*', '') for i in doc.lower().split()])\n",
    "    doc = \" \".join([i.replace(':', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('.', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('=', '') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('/', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace(')', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('(', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('\"', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('-', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('_', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i for i in doc.split() if not i.isdigit()])\n",
    "    doc = \" \".join([i for i in doc.split() if i.isalpha()])\n",
    "    doc = \" \".join([i for i in doc.split() if i not in stop])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "      <td>0</td>\n",
       "      <td>plot two teen couples go church party drink dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the happy bastard's quick movie review  damn t...</td>\n",
       "      <td>0</td>\n",
       "      <td>happy quick movie review damn bug got head sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "      <td>0</td>\n",
       "      <td>movies like make jaded movie viewer thankful i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
       "      <td>0</td>\n",
       "      <td>quest camelot warner bros first feature length...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "      <td>0</td>\n",
       "      <td>synopsis mentally unstable man undergoing psyc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Class  \\\n",
       "0  plot : two teen couples go to a church party ,...      0   \n",
       "1  the happy bastard's quick movie review  damn t...      0   \n",
       "2  it is movies like these that make a jaded movi...      0   \n",
       "3   \" quest for camelot \" is warner bros . ' firs...      0   \n",
       "4  synopsis : a mentally unstable man undergoing ...      0   \n",
       "\n",
       "                                          clean_text  \n",
       "0  plot two teen couples go church party drink dr...  \n",
       "1  happy quick movie review damn bug got head sta...  \n",
       "2  movies like make jaded movie viewer thankful i...  \n",
       "3  quest camelot warner bros first feature length...  \n",
       "4  synopsis mentally unstable man undergoing psyc...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_clear = [clean(doc) for doc in senti_data['Text']]\n",
    "senti_data['clean_text']=review_clear\n",
    "senti_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build sentense vector for training data-set by using the overall sum value of all word vectors in the Clean text column.\n",
    "\n",
    "This is just one of the way , you might take average as well or build other strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Below is a custom function to build sentense vector for training set by using the total value of all word vectors in the Clean text column\n",
    "\n",
    "def buildSentenceVector(text):\n",
    "    sent_vec = np.zeros(300).reshape((1, 300))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            sent_vec += model[word].reshape((1, 300))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    #if count != 0:\n",
    "    #    sent_vec /= count\n",
    "    return sent_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_vec = [buildSentenceVector(doc) for doc in senti_data['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senti_data['sentense_vector']=review_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentense_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "      <td>0</td>\n",
       "      <td>plot two teen couples go church party drink dr...</td>\n",
       "      <td>[[-293.95513916015625, 192.2789306640625, -4.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the happy bastard's quick movie review  damn t...</td>\n",
       "      <td>0</td>\n",
       "      <td>happy quick movie review damn bug got head sta...</td>\n",
       "      <td>[[-112.78167724609375, 65.20263671875, 0.33660...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "      <td>0</td>\n",
       "      <td>movies like make jaded movie viewer thankful i...</td>\n",
       "      <td>[[-242.486083984375, 163.73614501953125, 0.711...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
       "      <td>0</td>\n",
       "      <td>quest camelot warner bros first feature length...</td>\n",
       "      <td>[[-250.0531005859375, 168.8553466796875, -4.26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "      <td>0</td>\n",
       "      <td>synopsis mentally unstable man undergoing psyc...</td>\n",
       "      <td>[[-378.0009765625, 267.97918701171875, 1.84680...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Class  \\\n",
       "0  plot : two teen couples go to a church party ,...      0   \n",
       "1  the happy bastard's quick movie review  damn t...      0   \n",
       "2  it is movies like these that make a jaded movi...      0   \n",
       "3   \" quest for camelot \" is warner bros . ' firs...      0   \n",
       "4  synopsis : a mentally unstable man undergoing ...      0   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  plot two teen couples go church party drink dr...   \n",
       "1  happy quick movie review damn bug got head sta...   \n",
       "2  movies like make jaded movie viewer thankful i...   \n",
       "3  quest camelot warner bros first feature length...   \n",
       "4  synopsis mentally unstable man undergoing psyc...   \n",
       "\n",
       "                                     sentense_vector  \n",
       "0  [[-293.95513916015625, 192.2789306640625, -4.9...  \n",
       "1  [[-112.78167724609375, 65.20263671875, 0.33660...  \n",
       "2  [[-242.486083984375, 163.73614501953125, 0.711...  \n",
       "3  [[-250.0531005859375, 168.8553466796875, -4.26...  \n",
       "4  [[-378.0009765625, 267.97918701171875, 1.84680...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are going to create a data frame with the sum of the word vectors of length 300 representing the sentence along with the binary dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_vec1 = [i[0] for i in review_vec] # changing from list of list to single list\n",
    "review_vec2 = np.array(review_vec1) # Changing from single list to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_vec2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_df = pd.DataFrame(review_vec2) # changing the array to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-293.955139</td>\n",
       "      <td>192.278931</td>\n",
       "      <td>-4.972412</td>\n",
       "      <td>246.479492</td>\n",
       "      <td>-76.376129</td>\n",
       "      <td>52.847504</td>\n",
       "      <td>-149.930115</td>\n",
       "      <td>-68.620850</td>\n",
       "      <td>-81.728027</td>\n",
       "      <td>26.597717</td>\n",
       "      <td>...</td>\n",
       "      <td>116.883545</td>\n",
       "      <td>-33.414062</td>\n",
       "      <td>-166.789688</td>\n",
       "      <td>156.874390</td>\n",
       "      <td>-55.962769</td>\n",
       "      <td>-263.559921</td>\n",
       "      <td>-162.363647</td>\n",
       "      <td>-40.535278</td>\n",
       "      <td>-190.335449</td>\n",
       "      <td>264.554199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-112.781677</td>\n",
       "      <td>65.202637</td>\n",
       "      <td>0.336609</td>\n",
       "      <td>92.041504</td>\n",
       "      <td>-27.368500</td>\n",
       "      <td>18.075989</td>\n",
       "      <td>-55.470612</td>\n",
       "      <td>-36.053345</td>\n",
       "      <td>-30.213440</td>\n",
       "      <td>6.324829</td>\n",
       "      <td>...</td>\n",
       "      <td>41.426025</td>\n",
       "      <td>2.624512</td>\n",
       "      <td>-56.628387</td>\n",
       "      <td>61.097534</td>\n",
       "      <td>-24.956787</td>\n",
       "      <td>-102.484375</td>\n",
       "      <td>-68.589966</td>\n",
       "      <td>-16.991699</td>\n",
       "      <td>-71.407837</td>\n",
       "      <td>101.894714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-242.486084</td>\n",
       "      <td>163.736145</td>\n",
       "      <td>0.711853</td>\n",
       "      <td>198.322754</td>\n",
       "      <td>-68.430664</td>\n",
       "      <td>43.944977</td>\n",
       "      <td>-130.381378</td>\n",
       "      <td>-61.356079</td>\n",
       "      <td>-73.543579</td>\n",
       "      <td>25.141663</td>\n",
       "      <td>...</td>\n",
       "      <td>109.481201</td>\n",
       "      <td>-21.978760</td>\n",
       "      <td>-149.334961</td>\n",
       "      <td>124.054443</td>\n",
       "      <td>-40.955688</td>\n",
       "      <td>-225.843277</td>\n",
       "      <td>-138.615723</td>\n",
       "      <td>-27.549072</td>\n",
       "      <td>-161.180542</td>\n",
       "      <td>220.218445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-250.053101</td>\n",
       "      <td>168.855347</td>\n",
       "      <td>-4.266174</td>\n",
       "      <td>195.756836</td>\n",
       "      <td>-72.587067</td>\n",
       "      <td>44.229919</td>\n",
       "      <td>-127.833618</td>\n",
       "      <td>-64.556030</td>\n",
       "      <td>-76.520386</td>\n",
       "      <td>25.055115</td>\n",
       "      <td>...</td>\n",
       "      <td>100.104736</td>\n",
       "      <td>-21.081909</td>\n",
       "      <td>-139.455933</td>\n",
       "      <td>126.582520</td>\n",
       "      <td>-42.903931</td>\n",
       "      <td>-221.102890</td>\n",
       "      <td>-135.423828</td>\n",
       "      <td>-37.146851</td>\n",
       "      <td>-160.887817</td>\n",
       "      <td>230.279663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-378.000977</td>\n",
       "      <td>267.979187</td>\n",
       "      <td>1.846802</td>\n",
       "      <td>302.576904</td>\n",
       "      <td>-102.295349</td>\n",
       "      <td>59.879395</td>\n",
       "      <td>-188.510895</td>\n",
       "      <td>-105.857422</td>\n",
       "      <td>-103.271912</td>\n",
       "      <td>29.568604</td>\n",
       "      <td>...</td>\n",
       "      <td>149.544434</td>\n",
       "      <td>-43.593750</td>\n",
       "      <td>-232.855728</td>\n",
       "      <td>211.853516</td>\n",
       "      <td>-71.249023</td>\n",
       "      <td>-353.665085</td>\n",
       "      <td>-206.748779</td>\n",
       "      <td>-61.833618</td>\n",
       "      <td>-227.511963</td>\n",
       "      <td>374.038635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0           1         2           3           4          5    \\\n",
       "0 -293.955139  192.278931 -4.972412  246.479492  -76.376129  52.847504   \n",
       "1 -112.781677   65.202637  0.336609   92.041504  -27.368500  18.075989   \n",
       "2 -242.486084  163.736145  0.711853  198.322754  -68.430664  43.944977   \n",
       "3 -250.053101  168.855347 -4.266174  195.756836  -72.587067  44.229919   \n",
       "4 -378.000977  267.979187  1.846802  302.576904 -102.295349  59.879395   \n",
       "\n",
       "          6           7           8          9       ...             290  \\\n",
       "0 -149.930115  -68.620850  -81.728027  26.597717     ...      116.883545   \n",
       "1  -55.470612  -36.053345  -30.213440   6.324829     ...       41.426025   \n",
       "2 -130.381378  -61.356079  -73.543579  25.141663     ...      109.481201   \n",
       "3 -127.833618  -64.556030  -76.520386  25.055115     ...      100.104736   \n",
       "4 -188.510895 -105.857422 -103.271912  29.568604     ...      149.544434   \n",
       "\n",
       "         291         292         293        294         295         296  \\\n",
       "0 -33.414062 -166.789688  156.874390 -55.962769 -263.559921 -162.363647   \n",
       "1   2.624512  -56.628387   61.097534 -24.956787 -102.484375  -68.589966   \n",
       "2 -21.978760 -149.334961  124.054443 -40.955688 -225.843277 -138.615723   \n",
       "3 -21.081909 -139.455933  126.582520 -42.903931 -221.102890 -135.423828   \n",
       "4 -43.593750 -232.855728  211.853516 -71.249023 -353.665085 -206.748779   \n",
       "\n",
       "         297         298         299  \n",
       "0 -40.535278 -190.335449  264.554199  \n",
       "1 -16.991699  -71.407837  101.894714  \n",
       "2 -27.549072 -161.180542  220.218445  \n",
       "3 -37.146851 -160.887817  230.279663  \n",
       "4 -61.833618 -227.511963  374.038635  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df[\"sentiment\"] = senti_data[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-293.955139</td>\n",
       "      <td>192.278931</td>\n",
       "      <td>-4.972412</td>\n",
       "      <td>246.479492</td>\n",
       "      <td>-76.376129</td>\n",
       "      <td>52.847504</td>\n",
       "      <td>-149.930115</td>\n",
       "      <td>-68.620850</td>\n",
       "      <td>-81.728027</td>\n",
       "      <td>26.597717</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.414062</td>\n",
       "      <td>-166.789688</td>\n",
       "      <td>156.874390</td>\n",
       "      <td>-55.962769</td>\n",
       "      <td>-263.559921</td>\n",
       "      <td>-162.363647</td>\n",
       "      <td>-40.535278</td>\n",
       "      <td>-190.335449</td>\n",
       "      <td>264.554199</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-112.781677</td>\n",
       "      <td>65.202637</td>\n",
       "      <td>0.336609</td>\n",
       "      <td>92.041504</td>\n",
       "      <td>-27.368500</td>\n",
       "      <td>18.075989</td>\n",
       "      <td>-55.470612</td>\n",
       "      <td>-36.053345</td>\n",
       "      <td>-30.213440</td>\n",
       "      <td>6.324829</td>\n",
       "      <td>...</td>\n",
       "      <td>2.624512</td>\n",
       "      <td>-56.628387</td>\n",
       "      <td>61.097534</td>\n",
       "      <td>-24.956787</td>\n",
       "      <td>-102.484375</td>\n",
       "      <td>-68.589966</td>\n",
       "      <td>-16.991699</td>\n",
       "      <td>-71.407837</td>\n",
       "      <td>101.894714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-242.486084</td>\n",
       "      <td>163.736145</td>\n",
       "      <td>0.711853</td>\n",
       "      <td>198.322754</td>\n",
       "      <td>-68.430664</td>\n",
       "      <td>43.944977</td>\n",
       "      <td>-130.381378</td>\n",
       "      <td>-61.356079</td>\n",
       "      <td>-73.543579</td>\n",
       "      <td>25.141663</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.978760</td>\n",
       "      <td>-149.334961</td>\n",
       "      <td>124.054443</td>\n",
       "      <td>-40.955688</td>\n",
       "      <td>-225.843277</td>\n",
       "      <td>-138.615723</td>\n",
       "      <td>-27.549072</td>\n",
       "      <td>-161.180542</td>\n",
       "      <td>220.218445</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-250.053101</td>\n",
       "      <td>168.855347</td>\n",
       "      <td>-4.266174</td>\n",
       "      <td>195.756836</td>\n",
       "      <td>-72.587067</td>\n",
       "      <td>44.229919</td>\n",
       "      <td>-127.833618</td>\n",
       "      <td>-64.556030</td>\n",
       "      <td>-76.520386</td>\n",
       "      <td>25.055115</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.081909</td>\n",
       "      <td>-139.455933</td>\n",
       "      <td>126.582520</td>\n",
       "      <td>-42.903931</td>\n",
       "      <td>-221.102890</td>\n",
       "      <td>-135.423828</td>\n",
       "      <td>-37.146851</td>\n",
       "      <td>-160.887817</td>\n",
       "      <td>230.279663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-378.000977</td>\n",
       "      <td>267.979187</td>\n",
       "      <td>1.846802</td>\n",
       "      <td>302.576904</td>\n",
       "      <td>-102.295349</td>\n",
       "      <td>59.879395</td>\n",
       "      <td>-188.510895</td>\n",
       "      <td>-105.857422</td>\n",
       "      <td>-103.271912</td>\n",
       "      <td>29.568604</td>\n",
       "      <td>...</td>\n",
       "      <td>-43.593750</td>\n",
       "      <td>-232.855728</td>\n",
       "      <td>211.853516</td>\n",
       "      <td>-71.249023</td>\n",
       "      <td>-353.665085</td>\n",
       "      <td>-206.748779</td>\n",
       "      <td>-61.833618</td>\n",
       "      <td>-227.511963</td>\n",
       "      <td>374.038635</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0           1         2           3           4          5  \\\n",
       "0 -293.955139  192.278931 -4.972412  246.479492  -76.376129  52.847504   \n",
       "1 -112.781677   65.202637  0.336609   92.041504  -27.368500  18.075989   \n",
       "2 -242.486084  163.736145  0.711853  198.322754  -68.430664  43.944977   \n",
       "3 -250.053101  168.855347 -4.266174  195.756836  -72.587067  44.229919   \n",
       "4 -378.000977  267.979187  1.846802  302.576904 -102.295349  59.879395   \n",
       "\n",
       "            6           7           8          9    ...            291  \\\n",
       "0 -149.930115  -68.620850  -81.728027  26.597717    ...     -33.414062   \n",
       "1  -55.470612  -36.053345  -30.213440   6.324829    ...       2.624512   \n",
       "2 -130.381378  -61.356079  -73.543579  25.141663    ...     -21.978760   \n",
       "3 -127.833618  -64.556030  -76.520386  25.055115    ...     -21.081909   \n",
       "4 -188.510895 -105.857422 -103.271912  29.568604    ...     -43.593750   \n",
       "\n",
       "          292         293        294         295         296        297  \\\n",
       "0 -166.789688  156.874390 -55.962769 -263.559921 -162.363647 -40.535278   \n",
       "1  -56.628387   61.097534 -24.956787 -102.484375  -68.589966 -16.991699   \n",
       "2 -149.334961  124.054443 -40.955688 -225.843277 -138.615723 -27.549072   \n",
       "3 -139.455933  126.582520 -42.903931 -221.102890 -135.423828 -37.146851   \n",
       "4 -232.855728  211.853516 -71.249023 -353.665085 -206.748779 -61.833618   \n",
       "\n",
       "          298         299  sentiment  \n",
       "0 -190.335449  264.554199          0  \n",
       "1  -71.407837  101.894714          0  \n",
       "2 -161.180542  220.218445          0  \n",
       "3 -160.887817  230.279663          0  \n",
       "4 -227.511963  374.038635          0  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 300)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "X = review_df.iloc[:,0:300]\n",
    "y = review_df.sentiment\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's devide the data into training and testing:\n",
    "* Training=75%\n",
    "* Testing =25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 300)\n",
      "(500, 300)\n",
      "(1500,)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. import\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 2. instantiate a logistic regression model\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 622 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the model is built and it's time to test the classification accuracy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.588"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_prob = logreg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[165,  90],\n",
       "       [116, 129]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking how many words are there in google Word to vec and out of the words we have in our corpus what is the percentage common. \n",
    "\n",
    "If the % common is low between the google's word vector and the corpus we have, then the classification model would not work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word2vec_vocab = model.vocab.keys()\n",
    "word2vec_vocab_lower = [item.lower() for item in word2vec_vocab]\n",
    "\n",
    "print(len(word2vec_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['synopsis',\n",
       " 'mentally',\n",
       " 'unstable',\n",
       " 'man',\n",
       " 'undergoing',\n",
       " 'psychotherapy',\n",
       " 'saves',\n",
       " 'boy',\n",
       " 'potentially',\n",
       " 'fatal',\n",
       " 'accident',\n",
       " 'falls',\n",
       " 'love',\n",
       " 'mother',\n",
       " 'fledgling',\n",
       " 'restauranteur',\n",
       " 'unsuccessfully',\n",
       " 'attempting',\n",
       " 'gain',\n",
       " 'favor',\n",
       " 'takes',\n",
       " 'pictures',\n",
       " 'kills',\n",
       " 'number',\n",
       " 'people',\n",
       " 'way',\n",
       " 'comments',\n",
       " 'stalked',\n",
       " 'yet',\n",
       " 'another',\n",
       " 'seemingly',\n",
       " 'endless',\n",
       " 'string',\n",
       " 'spurned',\n",
       " 'psychos',\n",
       " 'getting',\n",
       " 'revenge',\n",
       " 'type',\n",
       " 'movies',\n",
       " 'stable',\n",
       " 'category',\n",
       " 'film',\n",
       " 'industry',\n",
       " 'theatrical',\n",
       " 'direct',\n",
       " 'video',\n",
       " 'proliferation',\n",
       " 'may',\n",
       " 'due',\n",
       " 'part',\n",
       " 'fact',\n",
       " 'typically',\n",
       " 'inexpensive',\n",
       " 'produce',\n",
       " 'special',\n",
       " 'effects',\n",
       " 'big',\n",
       " 'name',\n",
       " 'stars',\n",
       " 'serve',\n",
       " 'vehicles',\n",
       " 'flash',\n",
       " 'nudity',\n",
       " 'allowing',\n",
       " 'frequent',\n",
       " 'late',\n",
       " 'night',\n",
       " 'cable',\n",
       " 'television',\n",
       " 'stalked',\n",
       " 'wavers',\n",
       " 'slightly',\n",
       " 'norm',\n",
       " 'one',\n",
       " 'respect',\n",
       " 'psycho',\n",
       " 'never',\n",
       " 'actually',\n",
       " 'affair',\n",
       " 'contrary',\n",
       " 'rejected',\n",
       " 'rather',\n",
       " 'quickly',\n",
       " 'psycho',\n",
       " 'typically',\n",
       " 'ex',\n",
       " 'lover',\n",
       " 'ex',\n",
       " 'wife',\n",
       " 'ex',\n",
       " 'husband',\n",
       " 'stalked',\n",
       " 'another',\n",
       " 'redundant',\n",
       " 'entry',\n",
       " 'doomed',\n",
       " 'collect',\n",
       " 'dust',\n",
       " 'video',\n",
       " 'shelves',\n",
       " 'viewed',\n",
       " 'midnight',\n",
       " 'cable',\n",
       " 'stalked',\n",
       " 'provide',\n",
       " 'much',\n",
       " 'suspense',\n",
       " 'though',\n",
       " 'sets',\n",
       " 'interspersed',\n",
       " 'throughout',\n",
       " 'opening',\n",
       " 'credits',\n",
       " 'instance',\n",
       " 'serious',\n",
       " 'sounding',\n",
       " 'narrator',\n",
       " 'spouts',\n",
       " 'statistics',\n",
       " 'stalkers',\n",
       " 'ponders',\n",
       " 'may',\n",
       " 'cause',\n",
       " 'man',\n",
       " 'stalk',\n",
       " 'implicitly',\n",
       " 'implied',\n",
       " 'stalkers',\n",
       " 'men',\n",
       " 'pictures',\n",
       " 'boy',\n",
       " 'shown',\n",
       " 'screen',\n",
       " 'credits',\n",
       " 'snapshot',\n",
       " 'actor',\n",
       " 'jay',\n",
       " 'underwood',\n",
       " 'appears',\n",
       " 'narrator',\n",
       " 'states',\n",
       " 'story',\n",
       " 'daryl',\n",
       " 'gleason',\n",
       " 'tells',\n",
       " 'audience',\n",
       " 'stalker',\n",
       " 'course',\n",
       " 'really',\n",
       " 'story',\n",
       " 'restauranteur',\n",
       " 'brooke',\n",
       " 'daniels',\n",
       " 'movie',\n",
       " 'meant',\n",
       " 'daryl',\n",
       " 'called',\n",
       " 'stalker',\n",
       " 'stalked',\n",
       " 'okay',\n",
       " 'know',\n",
       " 'stalker',\n",
       " 'even',\n",
       " 'movie',\n",
       " 'starts',\n",
       " 'guesswork',\n",
       " 'required',\n",
       " 'stalked',\n",
       " 'proceeds',\n",
       " 'begins',\n",
       " 'obvious',\n",
       " 'obvious',\n",
       " 'obvious',\n",
       " 'opening',\n",
       " 'sequence',\n",
       " 'contrived',\n",
       " 'quite',\n",
       " 'bit',\n",
       " 'brings',\n",
       " 'daryl',\n",
       " 'brooke',\n",
       " 'victim',\n",
       " 'together',\n",
       " 'daryl',\n",
       " 'obsesses',\n",
       " 'brooke',\n",
       " 'follows',\n",
       " 'around',\n",
       " 'tries',\n",
       " 'woo',\n",
       " 'ultimately',\n",
       " 'rejected',\n",
       " 'plans',\n",
       " 'become',\n",
       " 'desperate',\n",
       " 'elaborate',\n",
       " 'plans',\n",
       " 'include',\n",
       " 'time',\n",
       " 'psycho',\n",
       " 'love',\n",
       " 'cliche',\n",
       " 'murdered',\n",
       " 'pet',\n",
       " 'reason',\n",
       " 'films',\n",
       " 'require',\n",
       " 'dead',\n",
       " 'pet',\n",
       " 'found',\n",
       " 'victim',\n",
       " 'stalked',\n",
       " 'stalked',\n",
       " 'exception',\n",
       " 'cat',\n",
       " 'time',\n",
       " 'found',\n",
       " 'shower',\n",
       " 'events',\n",
       " 'like',\n",
       " 'lead',\n",
       " 'inevitable',\n",
       " 'showdown',\n",
       " 'stalker',\n",
       " 'stalked',\n",
       " 'one',\n",
       " 'survives',\n",
       " 'guess',\n",
       " 'invariably',\n",
       " 'always',\n",
       " 'guess',\n",
       " 'conclusion',\n",
       " 'turkey',\n",
       " 'cast',\n",
       " 'uniformly',\n",
       " 'adequate',\n",
       " 'anything',\n",
       " 'write',\n",
       " 'home',\n",
       " 'also',\n",
       " 'bad',\n",
       " 'either',\n",
       " 'jay',\n",
       " 'underwood',\n",
       " 'stalker',\n",
       " 'turns',\n",
       " 'toward',\n",
       " 'melodrama',\n",
       " 'bit',\n",
       " 'much',\n",
       " 'overdoes',\n",
       " 'words',\n",
       " 'still',\n",
       " 'manages',\n",
       " 'creepy',\n",
       " 'enough',\n",
       " 'pass',\n",
       " 'type',\n",
       " 'stalker',\n",
       " 'story',\n",
       " 'demands',\n",
       " 'maryam',\n",
       " 'actor',\n",
       " 'close',\n",
       " 'star',\n",
       " 'played',\n",
       " 'bond',\n",
       " 'chick',\n",
       " 'living',\n",
       " 'daylights',\n",
       " 'equally',\n",
       " 'adequate',\n",
       " 'stalked',\n",
       " 'title',\n",
       " 'even',\n",
       " 'though',\n",
       " 'seems',\n",
       " 'ditzy',\n",
       " 'times',\n",
       " 'strong',\n",
       " 'independent',\n",
       " 'business',\n",
       " 'owner',\n",
       " 'brooke',\n",
       " 'needs',\n",
       " 'ditzy',\n",
       " 'however',\n",
       " 'plot',\n",
       " 'proceed',\n",
       " 'toward',\n",
       " 'end',\n",
       " 'example',\n",
       " 'brooke',\n",
       " 'suspicions',\n",
       " 'daryl',\n",
       " 'ensure',\n",
       " 'use',\n",
       " 'another',\n",
       " 'excuse',\n",
       " 'see',\n",
       " 'brooke',\n",
       " 'decides',\n",
       " 'return',\n",
       " 'toolbox',\n",
       " 'left',\n",
       " 'place',\n",
       " 'house',\n",
       " 'leave',\n",
       " 'toolbox',\n",
       " 'door',\n",
       " 'one',\n",
       " 'answers',\n",
       " 'course',\n",
       " 'tries',\n",
       " 'door',\n",
       " 'opens',\n",
       " 'wanders',\n",
       " 'around',\n",
       " 'house',\n",
       " 'daryl',\n",
       " 'returns',\n",
       " 'enters',\n",
       " 'house',\n",
       " 'course',\n",
       " 'heroine',\n",
       " 'danger',\n",
       " 'somehow',\n",
       " 'even',\n",
       " 'though',\n",
       " 'car',\n",
       " 'parked',\n",
       " 'front',\n",
       " 'house',\n",
       " 'right',\n",
       " 'front',\n",
       " 'door',\n",
       " 'daryl',\n",
       " 'oblivious',\n",
       " 'presence',\n",
       " 'inside',\n",
       " 'whole',\n",
       " 'episode',\n",
       " 'places',\n",
       " 'incredible',\n",
       " 'strain',\n",
       " 'suspension',\n",
       " 'disbelief',\n",
       " 'questions',\n",
       " 'validity',\n",
       " 'either',\n",
       " 'intelligence',\n",
       " 'stalked',\n",
       " 'receives',\n",
       " 'two',\n",
       " 'stars',\n",
       " 'even',\n",
       " 'though',\n",
       " 'highly',\n",
       " 'derivative',\n",
       " 'somewhat',\n",
       " 'boring',\n",
       " 'bad',\n",
       " 'cannot',\n",
       " 'watched',\n",
       " 'rated',\n",
       " 'r',\n",
       " 'mostly',\n",
       " 'several',\n",
       " 'murder',\n",
       " 'scenes',\n",
       " 'brief',\n",
       " 'nudity',\n",
       " 'strip',\n",
       " 'bar',\n",
       " 'offensive',\n",
       " 'many',\n",
       " 'thrillers',\n",
       " 'genre',\n",
       " 'mood',\n",
       " 'good',\n",
       " 'suspense',\n",
       " 'film',\n",
       " 'though',\n",
       " 'stake',\n",
       " 'something',\n",
       " 'else']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_data[\"clean_text\"][4].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [item for sublist in word_list for item in sublist] unlisting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "689259"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "unique_words = list(set(word_list))  #this will give unique list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38333\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31474\n"
     ]
    }
   ],
   "source": [
    "kk=0\n",
    "for word in unique_words:\n",
    "    try:\n",
    "        \n",
    "        kp= model[word]\n",
    "        kk +=1\n",
    "    except KeyError:\n",
    "        continue\n",
    "print(kk) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8210680092870373\n"
     ]
    }
   ],
   "source": [
    "print(kk/len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### We can see we have close to 82% words has vector representation in google word vector. This is a good representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
