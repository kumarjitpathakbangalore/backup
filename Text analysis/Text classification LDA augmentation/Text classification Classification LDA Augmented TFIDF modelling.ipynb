{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries needed to manipulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pyodbc\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "connect to the DailyMLResults database and get the data needed for this run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15910239 entries, 0 to 15910238\n",
      "Data columns (total 4 columns):\n",
      "Title                       object\n",
      "SourceLink                  object\n",
      "Modified_IsSecurity         bool\n",
      "RequirementCollectionDID    object\n",
      "dtypes: bool(1), object(3)\n",
      "memory usage: 379.3+ MB\n"
     ]
    }
   ],
   "source": [
    "conn = pyodbc.connect(r'Driver={SQL Server};Server=uatdotdatarpt;Database=DailyMLResults;Trusted_Connection=yes;OPTION=3;applicationintent=readonly;')\n",
    "data = pd.read_sql(\"\"\"SELECT Title, SourceLink, Modified_IsSecurity, RequirementCollectionDID \n",
    "\n",
    "FROM MSSecurity_3 WITH (NOLOCK)\n",
    "\n",
    "        WHERE AreaPath NOT LIKE '%RD\\OneComplianceTest%' \n",
    "\n",
    "        AND AreaPath NOT LIKE '%\\ClientManagement%'\n",
    "\n",
    "        AND (IsDataDraft = 0 OR IsDataDraft IS NULL) \n",
    "\n",
    " \n",
    "\n",
    "UNION\n",
    "\n",
    "SELECT Title, SourceLink, Modified_IsSecurity, RequirementCollectionDID \n",
    "\n",
    "FROM Not_MSSecurity_3 WITH (NOLOCK)\n",
    "\n",
    "        WHERE AreaPath NOT LIKE '%RD\\OneComplianceTest%' \n",
    "\n",
    "        AND AreaPath NOT LIKE '%\\ClientManagement%'\n",
    "\n",
    "        AND (IsDataDraft = 0 OR IsDataDraft IS NULL) \n",
    "\n",
    " \n",
    "\n",
    "UNION\n",
    "\n",
    "SELECT Title, SourceLink, Modified_IsSecurity, RequirementCollectionDID \n",
    "\n",
    "FROM Not_Labeled_3 WITH (NOLOCK)\n",
    "\n",
    "        WHERE AreaPath NOT LIKE '%RD\\OneComplianceTest%' \n",
    "\n",
    "        AND AreaPath NOT LIKE '%\\ClientManagement%'\n",
    "\n",
    "        AND (IsDataDraft = 0 OR IsDataDraft IS NULL) \"\"\", conn)\n",
    "conn.close()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Rmoves aldd urls from a title\n",
    "import re\n",
    "def clean_review_html(raw_data):\n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\$|\\?|\\=|\\&|\\%)*\\b',' ', raw_data, flags=re.MULTILINE)\n",
    "    letters_only=re.sub('\\s+',' ', text)\n",
    "    return letters_only\n",
    "\n",
    "#removes all paths from the titles\n",
    "def clean_review_path(raw_data):\n",
    "    raw_text = re.sub(r'(\\w+):(\\\\[A-Z0-9a-z_-]*\\s*[A-Za-z0-9_-]*)?(\\\\[A-Za-z0-9_-]*\\s*[A-Za-z0-9_-]*)*\\b', '', raw_data, flags=re.MULTILINE)\n",
    "    removed_path = re.sub(r'(\\w+):(\\[A-Z0-9a-z_-]*\\s*[A-Za-z0-9_-]*)?(\\[A-Za-z0-9_-]*\\s*[A-Za-z0-9_-]*)*\\b', '', raw_text, flags=re.MULTILINE)\n",
    "    return removed_path\n",
    "\n",
    "#Remove all punctuation marks from a string including underscores.\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r\"[\\\\]\", ' ',(' '.join(re.sub(r\"[!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~]\", ' ',text).split())))\n",
    "\n",
    "#remove all words attached to '/' or '\\'. this will remove all links that have not been removed\n",
    "def remove_paths_urls(text):#'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    return ' '.join([word for word in text.split() if '\\\\' not in word and '/' not in word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split Title at upper case takes IAmTHEBoy you know => I Am THE Boy you know.\n",
    "def split_on_uppercase(s, keep_contiguous=True):\n",
    "    \"\"\"  Args:\n",
    "        s (str): string\n",
    "        keep_contiguous (bool): flag to indicate we want to \n",
    "                                keep contiguous uppercase chars together\n",
    "    \"\"\"\n",
    "\n",
    "    string_length = len(s)\n",
    "    is_lower_around = (lambda: s[i-1].islower() or string_length > (i + 1) and s[i + 1].islower())\n",
    "    start = 0\n",
    "    parts = []\n",
    "    for i in range(1, string_length):\n",
    "        if s[i].isupper() and (not keep_contiguous or is_lower_around()):\n",
    "            parts.append(s[start: i])\n",
    "            start = i\n",
    "    parts.append(s[start:])\n",
    "\n",
    "    return \" \".join(parts)\n",
    "#print(split_on_uppercase('TheLongANDANdeWindingRoad', True))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSAzureIntune Svc IWPortal Release 1608 CTIP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MSAzureIntune Svc IWPortal Release CTIP'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove all integers from titles.\n",
    "def remove_integers(s):\n",
    "    return re.sub(' \\d+ ', ' ', s)\n",
    "print(remove_punctuation('MSAzureIntune-Svc-IWPortal_Release_1608_CTIP'))\n",
    "remove_integers(remove_punctuation('MSAzureIntune-Svc-IWPortal_Release_1608_CTIP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following our discussions with Alok, we had decided to make all instances of PoliCheck to 'policheck'. here, I create a function to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ DotNet-CoreFx-Trusted_master ][  policheck  ] - Defect :'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def deal_with_policheck(title):\n",
    "    return re.sub('(PoliCheck)|(POLICHECK)|(poliCheck)', ' policheck ', title)\n",
    "deal_with_policheck('''[ DotNet-CoreFx-Trusted_master ][ PoliCheck ] - Defect :''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Title'] = data['Title'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Title'] = data['Title'].apply(deal_with_policheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['No Urls'] = data['Title'].apply(clean_review_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['No Paths'] = data['No Urls'].apply(clean_review_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['No Links and Paths'] = data['No Paths'].apply(remove_paths_urls)\n",
    "#This function is just to ensure that if some paths and urls were not removedby the \n",
    "#regular expression, they must be removed now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Split Upper'] = data['No Links and Paths'].apply(split_on_uppercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['No Punctuation'] = data['Split Upper'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Clean Title'] = data['No Punctuation'].apply(remove_integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.drop(['No Punctuation','Split Upper','No Links and Paths','No Paths','No Urls'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model BuildingÂ¶\n",
    "\n",
    "We are going to build a bunch of models starting with unigrams and ending with trigrams (3=grams)\n",
    "\n",
    "We fist import the neccessary libraries, build a tfidf and a model.\n",
    "\n",
    "We end by validating the model on the validation set and getting output on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1907\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvectorizer = CountVectorizer(min_df=0.001,max_df=0.85,stop_words='english',ngram_range = (1,2))\n",
    "cvz = cvectorizer.fit_transform(data['Clean Title'])#.toarray()\n",
    "vocab = cvectorizer.get_feature_names()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lda\n",
    "import pickle\n",
    "\n",
    "pickle_out=open(\"cvz_TFIDF_LDA\",\"wb\")\n",
    "pickle.dump(cvz,pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out=open(\"cvectorizer_TFIDF_LDA\",\"wb\")\n",
    "pickle.dump(cvectorizer,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 15910239\n",
      "INFO:lda:vocab_size: 1907\n",
      "INFO:lda:n_words: 141465808\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 500\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -1550719970\n",
      "INFO:lda:<10> log likelihood: -1046782738\n",
      "INFO:lda:<20> log likelihood: -911758659\n",
      "INFO:lda:<30> log likelihood: -888794456\n",
      "INFO:lda:<40> log likelihood: -882415878\n",
      "INFO:lda:<50> log likelihood: -879721652\n",
      "INFO:lda:<60> log likelihood: -878715269\n",
      "INFO:lda:<70> log likelihood: -878245435\n",
      "INFO:lda:<80> log likelihood: -877909442\n",
      "INFO:lda:<90> log likelihood: -877651407\n",
      "INFO:lda:<100> log likelihood: -877549360\n",
      "INFO:lda:<110> log likelihood: -877427814\n",
      "INFO:lda:<120> log likelihood: -877274522\n",
      "INFO:lda:<130> log likelihood: -877198077\n",
      "INFO:lda:<140> log likelihood: -877144808\n",
      "INFO:lda:<150> log likelihood: -877049376\n",
      "INFO:lda:<160> log likelihood: -877014170\n",
      "INFO:lda:<170> log likelihood: -876889061\n",
      "INFO:lda:<180> log likelihood: -876790589\n",
      "INFO:lda:<190> log likelihood: -876795229\n",
      "INFO:lda:<200> log likelihood: -876740765\n",
      "INFO:lda:<210> log likelihood: -876791792\n",
      "INFO:lda:<220> log likelihood: -876737173\n",
      "INFO:lda:<230> log likelihood: -876695196\n",
      "INFO:lda:<240> log likelihood: -876662058\n",
      "INFO:lda:<250> log likelihood: -876674806\n",
      "INFO:lda:<260> log likelihood: -876704919\n",
      "INFO:lda:<270> log likelihood: -876567902\n",
      "INFO:lda:<280> log likelihood: -876580449\n",
      "INFO:lda:<290> log likelihood: -876550311\n",
      "INFO:lda:<300> log likelihood: -876528572\n",
      "INFO:lda:<310> log likelihood: -876563168\n",
      "INFO:lda:<320> log likelihood: -876528032\n",
      "INFO:lda:<330> log likelihood: -876493648\n",
      "INFO:lda:<340> log likelihood: -876516187\n",
      "INFO:lda:<350> log likelihood: -876477328\n",
      "INFO:lda:<360> log likelihood: -876371592\n",
      "INFO:lda:<370> log likelihood: -876307653\n",
      "INFO:lda:<380> log likelihood: -876300119\n",
      "INFO:lda:<390> log likelihood: -876329576\n",
      "INFO:lda:<400> log likelihood: -876357909\n",
      "INFO:lda:<410> log likelihood: -876350185\n",
      "INFO:lda:<420> log likelihood: -876360291\n",
      "INFO:lda:<430> log likelihood: -876357415\n",
      "INFO:lda:<440> log likelihood: -876361549\n",
      "INFO:lda:<450> log likelihood: -876383008\n",
      "INFO:lda:<460> log likelihood: -876376258\n",
      "INFO:lda:<470> log likelihood: -876392268\n",
      "INFO:lda:<480> log likelihood: -876356218\n",
      "INFO:lda:<490> log likelihood: -876389754\n",
      "INFO:lda:<499> log likelihood: -876306701\n"
     ]
    }
   ],
   "source": [
    "Lda_model_20 = lda.LDA(n_topics=20, n_iter=500,random_state=42)\n",
    "X_topics = Lda_model_20.fit_transform(cvz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_out=open(\"Lda_model_TFIDF_20\",\"wb\")\n",
    "pickle.dump(Lda_model_20,pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "file = open(\"C:\\\\Users\\\\v-prgana\\\\Desktop\\\\Scripts_prabhu\\\\X_topics_whole\",'wb')\n",
    "pickle.dump(X_topics,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lda\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out=open(\"Lda_model_TFIDF_20\",\"rb\")\n",
    "Lda_model_TFIDF_20=pickle.load(pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_topics = Lda_model_TFIDF_20.fit_transform(cvz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open(\"C:\\\\Users\\\\v-prgana\\\\Desktop\\\\Scripts_prabhu\\\\X_topics_whole_v1\",'wb')\n",
    "pickle.dump(X_topics,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words=15\n",
    "topic_summaries=[]\n",
    "topic_word = Lda_model_TFIDF_20.topic_word_  # all topic words\n",
    "vocab = cvectorizer.get_feature_names()\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    #print(i,topic_dist)\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words + 1):-1] # get!\n",
    "    topic_summaries.append(','.join(topic_words)) \n",
    "#topic_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_topics_df = pd.DataFrame()\n",
    "\n",
    "# Get main topic in each document\n",
    "for i, row in enumerate(X_topics):\n",
    "    #print(type(row))\n",
    "    row=list(row)\n",
    "    #print(row)\n",
    "    row = sorted(enumerate(row), key=lambda x: (x[1]), reverse=True)\n",
    "    # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "    for j, (topic_num, prop_topic) in enumerate(row):\n",
    "        if j == 0:  # => dominant topic\n",
    "            topic_keywords = topic_summaries[topic_num]\n",
    "            sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "        else:\n",
    "            break\n",
    "sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "# Add original text to the end of the output\n",
    "contents = pd.Series(data['Clean Title'])\n",
    "sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = sent_topics_df.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n"
     ]
    }
   ],
   "source": [
    "print(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_12, y_12 = data['Clean Title'], data['Modified_IsSecurity']\n",
    "tv_12 = TfidfVectorizer(min_df = 0.00009, max_df = 0.5, use_idf = True, stop_words = 'english', ngram_range = (1,2))\n",
    "X_12 = tv_12.fit_transform(X_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del Lda_model_TFIDF_20,cvectorizer,cvz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "X_t=sparse.csr_matrix(X_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_t=sparse.hstack((X_12, X_t), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_df=pd.DataFrame(X_topics)\n",
    "#x_df.columns=[\"topic_\"+str(i) for i in range(0,20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13844\n"
     ]
    }
   ],
   "source": [
    "X_train_12, X_12_0, y_train_12, y_12_0 = train_test_split(X_t, y_12, test_size = 0.2, random_state = 42)\n",
    "X_test_12, X_val_12, y_test_12, y_val_12 = train_test_split(X_12_0, y_12_0, test_size = 0.5, random_state = 42)\n",
    "vocab_12 = tv_12.get_feature_names()\n",
    "print(len(vocab_12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression model and Get predictions on the training, validation and whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr_12 = LogisticRegression(n_jobs = -1)\n",
    "model_lr_12.fit(X_train_12, y_train_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Results on Test set ***************\n",
      "accuracy:   0.995\n",
      "f1_score:   0.971\n",
      "precision_score:   0.990\n",
      "recall_score:   0.952\n",
      "roc_auc_score:   0.976\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00   1447309\n",
      "       True       0.99      0.95      0.97    143715\n",
      "\n",
      "avg / total       0.99      0.99      0.99   1591024\n",
      "\n",
      "confusion matrix:\n",
      "[[1445931    1378]\n",
      " [   6905  136810]]\n",
      "*************** Results on Validation set *************** \n",
      "\n",
      "accuracy:   0.995\n",
      "f1_score:   0.970\n",
      "precision_score:   0.990\n",
      "recall_score:   0.951\n",
      "roc_auc_score:   0.975\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00   1447647\n",
      "       True       0.99      0.95      0.97    143377\n",
      "\n",
      "avg / total       0.99      0.99      0.99   1591024\n",
      "\n",
      "confusion matrix:\n",
      "[[1446317    1330]\n",
      " [   6965  136412]]\n",
      "************************* END! *************************\n"
     ]
    }
   ],
   "source": [
    "print('*'*15 + ' Results on Test set ' +'*'*15)\n",
    "pred_12 = model_lr_12.predict(X_test_12)\n",
    "\n",
    "score = metrics.accuracy_score(y_test_12, pred_12)\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "f1_score = metrics.f1_score(y_test_12, pred_12)\n",
    "print(\"f1_score:   %0.3f\" % f1_score)\n",
    "\n",
    "precision_score = metrics.precision_score(y_test_12, pred_12)\n",
    "print(\"precision_score:   %0.3f\" % precision_score)\n",
    "\n",
    "recall_score = metrics.recall_score(y_test_12, pred_12)\n",
    "print(\"recall_score:   %0.3f\" % recall_score)\n",
    "\n",
    "roc_auc_score = metrics.roc_auc_score(y_test_12, pred_12)\n",
    "print(\"roc_auc_score:   %0.3f\" % roc_auc_score)\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test_12, pred_12)\n",
    "\n",
    "print(\"classification report:\")\n",
    "print(metrics.classification_report(y_test_12, pred_12))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test_12, pred_12))\n",
    "\n",
    "print('*'*15 + ' Results on Validation set ' +'*'*15, '\\n')\n",
    "pred_0_12 = model_lr_12.predict(X_val_12)\n",
    "\n",
    "score = metrics.accuracy_score(y_val_12, pred_0_12)\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "f1_score = metrics.f1_score(y_val_12, pred_0_12)\n",
    "print(\"f1_score:   %0.3f\" % f1_score)\n",
    "\n",
    "precision_score = metrics.precision_score(y_val_12, pred_0_12)\n",
    "print(\"precision_score:   %0.3f\" % precision_score)\n",
    "\n",
    "recall_score = metrics.recall_score(y_val_12, pred_0_12)\n",
    "print(\"recall_score:   %0.3f\" % recall_score)\n",
    "\n",
    "roc_auc_score = metrics.roc_auc_score(y_val_12, pred_0_12)\n",
    "print(\"roc_auc_score:   %0.3f\" % roc_auc_score)\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_val_12, pred_0_12)\n",
    "\n",
    "print(\"classification report:\")\n",
    "print(metrics.classification_report(y_val_12, pred_0_12))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_val_12, pred_0_12))\n",
    "print('*'*25 + ' END! ' +'*'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Results on the entire set *************** \n",
      "\n",
      "accuracy:   0.995\n",
      "f1_score:   0.971\n",
      "precision_score:   0.991\n",
      "recall_score:   0.952\n",
      "roc_auc_score:   0.975\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00  14474640\n",
      "       True       0.99      0.95      0.97   1435599\n",
      "\n",
      "avg / total       0.99      0.99      0.99  15910239\n",
      "\n",
      "confusion matrix:\n",
      "[[14461558    13082]\n",
      " [   69054  1366545]]\n",
      "************************* END! *************************\n"
     ]
    }
   ],
   "source": [
    "print('*'*15 + ' Results on the entire set ' +'*'*15, '\\n')\n",
    "pred__12 = model_lr_12.predict(X_t)\n",
    "\n",
    "score = metrics.accuracy_score(y_12, pred__12)\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "f1_score = metrics.f1_score(y_12, pred__12)\n",
    "print(\"f1_score:   %0.3f\" % f1_score)\n",
    "\n",
    "precision_score = metrics.precision_score(y_12, pred__12)\n",
    "print(\"precision_score:   %0.3f\" % precision_score)\n",
    "\n",
    "recall_score = metrics.recall_score(y_12, pred__12)\n",
    "print(\"recall_score:   %0.3f\" % recall_score)\n",
    "\n",
    "roc_auc_score = metrics.roc_auc_score(y_12, pred__12)\n",
    "print(\"roc_auc_score:   %0.3f\" % roc_auc_score)\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_12, pred__12)\n",
    "\n",
    "print(\"classification report:\")\n",
    "print(metrics.classification_report(y_12, pred__12))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_12, pred__12))\n",
    "print('*'*25 + ' END! ' +'*'*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validate the model on the validation set using \n",
    "\n",
    "1) accuracy,  <br>\n",
    "  2) f1_macro,  <br>\n",
    "  3) recall, and <br>\n",
    "  4) precision <br>asscoring methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import cross_val_score\n",
    "#scores = cross_val_score(model_lr_12, X_val_12, y_val_12, scoring = 'f1_macro', cv = 10)\n",
    "#print(scores)\n",
    "#print('Mean f1 score of the 10 scores: %s'%(round(100*scores.mean(), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model_lr_12, X_val_12, y_val_12, scoring = 'accuracy', cv = 10)\n",
    "print(scores)\n",
    "print('Mean accuracy score of the 10 scores: %s'%(round(100*scores.mean(), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model_lr_12, X_val_12, y_val_12, scoring = 'recall', cv = 10)\n",
    "print(scores)\n",
    "print('Mean recall score of the 10 scores: %s'%(round(100*scores.mean(), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model_lr_12, X_val_12, y_val_12, scoring = 'precision', cv = 10)\n",
    "print(scores)\n",
    "print('Mean precision score of the 10 scores: %s'%(round(100*scores.mean(), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add columns for the predictions and the probability of prediction for each title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Predictions'], data['Pred_Prob0'], data['Pred_Prob1']= model_lr_12.predict(X_t), model_lr_12.predict_proba(X_t)[:,0], model_lr_12.predict_proba(X_t)[:,1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the false Positives and false negatives to a csv file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[(data['Modified_IsSecurity'] == True) & (data['Predictions'] == False)][['Title','SourceLink','Modified_IsSecurity','Predictions','Pred_Prob0','Pred_Prob1']].to_csv('False Negatives bug,isue,compileETC06-07-18.csv', index = False)\n",
    "data[(data['Modified_IsSecurity'] == False) & (data['Predictions'] == True)][['Title','SourceLink','Modified_IsSecurity','Predictions','Pred_Prob0','Pred_Prob1']].to_csv('False Positives bug,isue,compileETC06-07-18.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the tfidf and the the model for future references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tv_12_TFIDF_lda.pkl']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(model_lr_12, 'model_lr_12_TFIDF_lda.pkl')\n",
    "joblib.dump(tv_12, 'tv_12_TFIDF_lda.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr_13 = LogisticRegression(n_jobs = -1)\n",
    "model_lr_13.fit(X_train_12[:,:((X_train_12.shape)[1]-20)], y_train_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Results on Test set ***************\n",
      "accuracy:   0.995\n",
      "f1_score:   0.971\n",
      "precision_score:   0.990\n",
      "recall_score:   0.952\n",
      "roc_auc_score:   0.975\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00   1447309\n",
      "       True       0.99      0.95      0.97    143715\n",
      "\n",
      "avg / total       0.99      0.99      0.99   1591024\n",
      "\n",
      "confusion matrix:\n",
      "[[1445938    1371]\n",
      " [   6913  136802]]\n",
      "*************** Results on Validation set *************** \n",
      "\n",
      "accuracy:   0.995\n",
      "f1_score:   0.970\n",
      "precision_score:   0.990\n",
      "recall_score:   0.951\n",
      "roc_auc_score:   0.975\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00   1447647\n",
      "       True       0.99      0.95      0.97    143377\n",
      "\n",
      "avg / total       0.99      0.99      0.99   1591024\n",
      "\n",
      "confusion matrix:\n",
      "[[1446319    1328]\n",
      " [   6969  136408]]\n",
      "************************* END! *************************\n"
     ]
    }
   ],
   "source": [
    "print('*'*15 + ' Results on Test set ' +'*'*15)\n",
    "pred_12 = model_lr_13.predict(X_test_12[:,:((X_test_12.shape)[1]-20)])\n",
    "\n",
    "score = metrics.accuracy_score(y_test_12, pred_12)\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "f1_score = metrics.f1_score(y_test_12, pred_12)\n",
    "print(\"f1_score:   %0.3f\" % f1_score)\n",
    "\n",
    "precision_score = metrics.precision_score(y_test_12, pred_12)\n",
    "print(\"precision_score:   %0.3f\" % precision_score)\n",
    "\n",
    "recall_score = metrics.recall_score(y_test_12, pred_12)\n",
    "print(\"recall_score:   %0.3f\" % recall_score)\n",
    "\n",
    "roc_auc_score = metrics.roc_auc_score(y_test_12, pred_12)\n",
    "print(\"roc_auc_score:   %0.3f\" % roc_auc_score)\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test_12, pred_12)\n",
    "\n",
    "print(\"classification report:\")\n",
    "print(metrics.classification_report(y_test_12, pred_12))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test_12, pred_12))\n",
    "\n",
    "print('*'*15 + ' Results on Validation set ' +'*'*15, '\\n')\n",
    "pred_0_12 = model_lr_13.predict(X_val_12[:,:((X_val_12.shape)[1]-20)])\n",
    "\n",
    "score = metrics.accuracy_score(y_val_12, pred_0_12)\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "f1_score = metrics.f1_score(y_val_12, pred_0_12)\n",
    "print(\"f1_score:   %0.3f\" % f1_score)\n",
    "\n",
    "precision_score = metrics.precision_score(y_val_12, pred_0_12)\n",
    "print(\"precision_score:   %0.3f\" % precision_score)\n",
    "\n",
    "recall_score = metrics.recall_score(y_val_12, pred_0_12)\n",
    "print(\"recall_score:   %0.3f\" % recall_score)\n",
    "\n",
    "roc_auc_score = metrics.roc_auc_score(y_val_12, pred_0_12)\n",
    "print(\"roc_auc_score:   %0.3f\" % roc_auc_score)\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_val_12, pred_0_12)\n",
    "\n",
    "print(\"classification report:\")\n",
    "print(metrics.classification_report(y_val_12, pred_0_12))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_val_12, pred_0_12))\n",
    "print('*'*25 + ' END! ' +'*'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a 10-fold cross-balidation and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.94064309  0.93534212  0.93938337  0.94091797  0.93924386  0.93736049\n",
      "  0.93680246  0.93694196  0.93882533  0.93805804]\n",
      "Mean recall score of the 10 scores: 93.84\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model_lr_12, X_val_12, y_val_12, scoring = 'recall', cv = 10)\n",
    "print(scores)\n",
    "print('Mean recall score of the 10 scores: %s'%(round(100*scores.mean(), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.99368338  0.99321199  0.99351988  0.99355131  0.99347588  0.99326847\n",
      "  0.99337532  0.99326218  0.99336275  0.99340046]\n",
      "Mean accuracy score of the 10 scores: 99.34\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model_lr_12, X_val_12, y_val_12, scoring = 'accuracy', cv = 10)\n",
    "print(scores)\n",
    "print('Mean accuracy score of the 10 scores: %s'%(round(100*scores.mean(), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.98870968  0.98871931  0.98811358  0.98690372  0.98774941  0.98728969\n",
      "  0.98910001  0.98764706  0.98687491  0.98809699]\n",
      "Mean precision score of the 10 scores: 98.79\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model_lr_12, X_val_12, y_val_12, scoring = 'precision', cv = 10)\n",
    "print(scores)\n",
    "print('Mean precision score of the 10 scores: %s'%(round(100*scores.mean(), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-6a7cea5551a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_lr_12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val_12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'f1_macro'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Mean f1_macro score of the 10 scores: %s'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[0;32m    343\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             return_times=True)\n\u001b[1;32m--> 206\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[0mis_multimetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m_safe_split\u001b[1;34m(estimator, X, y, indices, train_indices)\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[0mX_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0mX_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[1;34m(X, indices)\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\sparse\\csr.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                 \u001b[0mP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# [[1,2],j] or [[1,2],1:2]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m                 \u001b[0mextracted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mP\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mextracted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[1;31m# If it's a list or whatever, treat it like a matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m_mul_sparse_matrix\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    492\u001b[0m            \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m            \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m            \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m            indptr)\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \"\"\"\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model_lr_12, X_val_12, y_val_12, scoring = 'f1_macro', cv = 10)\n",
    "print(scores)\n",
    "print('Mean f1_macro score of the 10 scores: %s'%(round(100*scores.mean(), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'SourceLink', 'Modified_IsSecurity',\n",
       "       'RequirementCollectionDID', 'Clean Title', 'Predictions', 'Pred_Prob0',\n",
       "       'Pred_Prob1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predictions for each title and the probability attributed to that prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 13844 features per sample; expecting 13864",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-f9bbabec16e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Predictions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Pred_Proba0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Pred_Proba1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_lr_12\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_lr_12\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_lr_12\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \"\"\"\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 305\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 13844 features per sample; expecting 13864"
     ]
    }
   ],
   "source": [
    "data['Predictions'], data['Pred_Proba0'], data['Pred_Proba1']=model_lr_12.predict(X_12), model_lr_12.predict_proba(X_12)[:,0], model_lr_12.predict_proba(X_12)[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Pred_Proba0' 'Pred_Proba1' 'Type'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-81ff973d1ada>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Modified_IsSecurity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Predictions'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SourceLink'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Modified_IsSecurity'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Predictions'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Pred_Proba0'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Pred_Proba1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Modified_IsSecurity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Predictions'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SourceLink'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Modified_IsSecurity'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Predictions'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Pred_Proba0'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Pred_Proba1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2131\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2132\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2133\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2134\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2135\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2175\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2176\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2177\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2178\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1267\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[1;32m-> 1269\u001b[1;33m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[0;32m   1270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Pred_Proba0' 'Pred_Proba1' 'Type'] not in index\""
     ]
    }
   ],
   "source": [
    "fn,fp = data[(data['Modified_IsSecurity'] == True) & (data['Predictions'] == False)][['Title', 'SourceLink','Modified_IsSecurity','Predictions','Pred_Proba0','Pred_Proba1','Type']], data[(data['Modified_IsSecurity'] == False) & (data['Predictions'] == True)][['Title', 'SourceLink','Modified_IsSecurity','Predictions','Pred_Proba0','Pred_Proba1','Type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group all the Titles, false negatives and false positives by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped_type = pd.DataFrame(data.groupby(by = ['Type']).count()['Title'])\n",
    "grouped_type_fp = pd.DataFrame(fp.groupby(by = ['Type']).count()['Title'])\n",
    "grouped_type_fn = pd.DataFrame(fn.groupby(by = ['Type']).count()['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped_type.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped_type_fn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped_type.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all three dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_grouped_by = pd.concat([grouped_type, grouped_type_fn, grouped_type_fp], axis = 1)\n",
    "concat_grouped_by.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_grouped_by.columns=['Title', 'fn_title', 'fp_title']\n",
    "concat_grouped_by.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_grouped_by['Total_Type'] = concat_grouped_by.sum()\n",
    "concat_grouped_by.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a column for the percentage of false negatives and false positives for each work Item type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_grouped_by['Percent_fn'] = round(100*concat_grouped_by['fn_title']/concat_grouped_by['Title'],3)\n",
    "concat_grouped_by['Percent_fp'] = round(100*concat_grouped_by['fp_title']/concat_grouped_by['Title'],3)\n",
    "concat_grouped_by.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the columns to more intuitive names and checkout the head of the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_grouped_by.rename(inplace = True, columns={'Title':'numTitles', 'fn_title':'numFN', 'fp_title':'numFP', 'Total_Type':'Total_Type', 'Percent_fn':'Percent_fn','Percent_fp':'Percent_fp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_grouped_by.drop('Total_Type', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_grouped_by.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the grouped data to csv files for further analylis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_grouped_by.to_csv('False Negatives and false Positives grouped by workItem Type.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the false negatives and false positives of this model to a csv file for further anlysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn.to_csv('False Negatives all work item type 06-08-2018.csv', index = False)\n",
    "fp.to_csv('False Positives all work item type 06-08-2018.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the logistic regression model and its tfidf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
