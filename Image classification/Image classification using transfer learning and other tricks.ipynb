{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from random import shuffle\n",
    "from keras import optimizers\n",
    "from skimage.color import gray2rgb\n",
    "from scipy.misc import imread, imsave\n",
    "from keras.applications import InceptionV3\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Stop training if validation loss doesn't improve for 10 epochs\n",
    "earlystop = EarlyStopping(monitor = \"val_loss\", \n",
    "                          patience = 10, \n",
    "                          verbose = 1, \n",
    "                          mode = \"auto\")\n",
    "\n",
    "# Save the best model after every epoch\n",
    "checkpoint = ModelCheckpoint(filepath = \"inceptionv3.hdf5\", \n",
    "                             verbose = 1, \n",
    "                             save_best_only = True)\n",
    "\n",
    "# Reduce the learning rate after validation loss plateaus\n",
    "reducelr = ReduceLROnPlateau(monitor = \"val_loss\", \n",
    "                             factor = 0.2,\n",
    "                             patience = 5)\n",
    "\n",
    "TARGET_SIZE = (299, 299) # Input shape for Inception v3\n",
    "BATCH_SIZE = 32 # Batch size for training\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def define_model():\n",
    "    ''' Load a pre-trained inception V3 model and change the top layers to \n",
    "    match the number of classes of our problem'''\n",
    "    base_model = InceptionV3(weights = \"imagenet\", \n",
    "                         include_top = False, \n",
    "                         input_shape = (299, 299, 3))\n",
    "    for i in range(len(base_model.layers)):\n",
    "        base_model.layers[i].trainable = False\n",
    "    add_model = Sequential()\n",
    "    add_model.add(Flatten(input_shape = base_model.output_shape[1:]))\n",
    "    add_model.add(Dense(256, activation = \"relu\"))\n",
    "    add_model.add(Dense(len(os.listdir(\"dataset//train\")), activation=\"softmax\"))\n",
    "    model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
    "    model.compile(loss = \"categorical_crossentropy\", \n",
    "              optimizer = optimizers.SGD(lr = 1e-4, momentum = 0.9),\n",
    "              metrics = [\"accuracy\"])\n",
    "    model.summary()\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_train(train_folder):\n",
    "    '''Removes duplicates in train folder where the same images appears in\n",
    "    more than one class'''\n",
    "    hashes = {}\n",
    "    labels = {}\n",
    "\n",
    "    print(\"computing md5 of training data\")\n",
    "\n",
    "    for fname in glob(train_folder+\"/*/*.jpg\"):\n",
    "        labels[fname] = fname.split(\"//\")[-2]\n",
    "        h = hashlib.md5(open(fname,\"rb\").read()).hexdigest()  \n",
    "        if h in hashes:\n",
    "            hashes[h].append(fname)\n",
    "        else:\n",
    "            hashes[h] = [fname]\n",
    "    \n",
    "    # Find duplicates\n",
    "    repeated = sum(1 for k,v in hashes.items() if len(v) > 1 )\n",
    "    print(\"Files appearing more than once in train: \", repeated)\n",
    "    \n",
    "    del_files = []\n",
    "    \n",
    "    # Find duplicate images with different class names\n",
    "    for k,v in hashes.items():\n",
    "        if len(v) > 1:\n",
    "            c = set([labels[x] for x in v])\n",
    "            if len(c) > 1:\n",
    "                del_files = del_files.append(v)\n",
    "    \n",
    "    for x in del_files:\n",
    "        os.remove(x)\n",
    "\n",
    "    print(len(del_files), \"images deleted from training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_images(test_folder):\n",
    "    ''' Function to convert test images to 3 channels (for images having\n",
    "    4 channels or less than 3 channels)'''\n",
    "    for img in os.listdir(test_folder):\n",
    "        img_path = os.path.join(test_folder, img)\n",
    "        img_file = imread(img_path)\n",
    "        if len(img_file.shape) < 3:\n",
    "            img_file = gray2rgb(img_file)\n",
    "            img_file = img_file.astype(np.float32, copy = False)\n",
    "            imsave(img_path, img_file)\n",
    "        if len(img_file.shape) == 4:\n",
    "            img_file = img_file[:,:,:-1]\n",
    "            img_file = img_file.astype(np.float32, copy = False)\n",
    "            imsave(img_path, img_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_leak(train_folder, test_folder):\n",
    "    '''Finds images present in both training and test set'''\n",
    "\n",
    "    hashes = {}\n",
    "    labels = {}\n",
    "\n",
    "    print(\"computing md5 of training data\")\n",
    "\n",
    "    for fname in glob(train_folder+\"/*/*.jpg\"):\n",
    "        labels[fname] = fname.split(\"//\")[-2]\n",
    "        h = hashlib.md5(open(fname,\"rb\").read()).hexdigest()  \n",
    "        if h in hashes:\n",
    "            hashes[h].append(fname)\n",
    "        else:\n",
    "            hashes[h] = [fname]\n",
    "\n",
    "    print(\"comparing training and test set\")\n",
    "    \n",
    "    leaks = []\n",
    "    for fname in glob(test_folder+\"/*.jpg\"):\n",
    "        h = hashlib.md5(open(fname,\"rb\").read()).hexdigest()\n",
    "        if h in hashes:\n",
    "            leaks.append((fname.split(\"//\")[-1],hashes[h][0].split(\"//\")[-2]))\n",
    "\n",
    "    print(\"Number of test images present in train:{}\".format(len(leaks)))\n",
    "    return leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train_images(train_folder):\n",
    "    ''' Function to convert training images to 3 channels (for images having\n",
    "    4 channels or less than 3 channels)''' \n",
    "    \n",
    "    classes = os.listdir(train_folder)\n",
    "    for cla in classes:\n",
    "        cla_path = os.path.join(\"dataset\", \"train\", cla)\n",
    "        for img in os.listdir(cla_path):\n",
    "            img_path = os.path.join(\"dataset\", \"train\", cla, img)\n",
    "            img_file = imread(img_path)\n",
    "            if len(img_file.shape) < 3:\n",
    "                img_file = gray2rgb(img_file)\n",
    "                img_file = img_file.astype(np.float32, copy = False)\n",
    "                imsave(img_path, img_file)\n",
    "            if len(img_file.shape) == 4:\n",
    "                img_file = img_file[:,:,:-1]\n",
    "                img_file = img_file.astype(np.float32, copy = False)\n",
    "                imsave(img_path, img_file)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing function for Inception v3 model\n",
    "def preprocess_input(x):\n",
    "    x /= 255.\n",
    "    x -= 0.5\n",
    "    x *= 2.\n",
    "    return x\n",
    "\n",
    "def remove_percentage(list_a, percentage):\n",
    "    ''' Function to randomly pick x percentage from a list'''\n",
    "    shuffle(list_a)\n",
    "    count = int(len(list_a) * percentage)\n",
    "    if not count: \n",
    "        return []\n",
    "    list_a[-count:], list_b = [], list_a[-count:]\n",
    "    return list_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "           \n",
    "\n",
    "\n",
    "def create_val_set(val_size):\n",
    "    '''Function to create a validation set from training images'''\n",
    "    if not os.path.exists(\"dataset//valid\"):\n",
    "        os.makedirs(\"dataset//valid\")\n",
    "    class_list = os.listdir(\"dataset//train\")\n",
    "    for cla in class_list:\n",
    "        if os.path.exists(os.path.join(\"dataset\", \"valid\", cla)):\n",
    "            if len(os.listdir(os.path.join(\"dataset\", \"valid\", cla))) == 0:\n",
    "                new_files = os.listdir(os.path.join(\"dataset\", \"train\", cla))\n",
    "                new_files = remove_percentage(new_files, val_size)\n",
    "                for nf in new_files:\n",
    "                    os.rename(os.path.join(\"dataset\", \"train\", cla, nf), \n",
    "                              os.path.join(\"dataset\", \"valid\", cla, nf))\n",
    "            else:\n",
    "                new_files = os.listdir(os.path.join(\"dataset\", \"valid\", cla))\n",
    "                for nf in new_files:\n",
    "                    os.rename(os.path.join(\"dataset\", \"valid\", cla, nf),\n",
    "                              os.path.join(\"dataset\", \"train\", cla, nf))\n",
    "                new_files = os.listdir(os.path.join(\"dataset\", \"train\", cla))\n",
    "                new_files = remove_percentage(new_files, val_size)\n",
    "                for nf in new_files:\n",
    "                    os.rename(os.path.join(\"dataset\", \"train\", cla, nf), \n",
    "                              os.path.join(\"dataset\", \"valid\", cla, nf))\n",
    "        else:\n",
    "            os.makedirs(os.path.join(\"dataset\", \"valid\", cla))\n",
    "            new_files = os.listdir(os.path.join(\"dataset\", \"train\", cla))\n",
    "            new_files = remove_percentage(new_files, val_size)\n",
    "            for nf in new_files:\n",
    "                os.rename(os.path.join(\"dataset\", \"train\", cla, nf), \n",
    "                          os.path.join(\"dataset\", \"valid\", cla, nf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model():\n",
    "    ''' Load a pre-trained inception V3 model and change the top layers to \n",
    "    match the number of classes of our problem'''\n",
    "    base_model = InceptionV3(weights = \"imagenet\", \n",
    "                         include_top = False, \n",
    "                         input_shape = (299, 299, 3))\n",
    "    for i in range(len(base_model.layers)):\n",
    "        base_model.layers[i].trainable = False\n",
    "    add_model = Sequential()\n",
    "    add_model.add(Flatten(input_shape = base_model.output_shape[1:]))\n",
    "    add_model.add(Dense(256, activation = \"relu\"))\n",
    "    add_model.add(Dense(len(os.listdir(\"dataset//train\")), activation=\"softmax\"))\n",
    "    model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
    "    model.compile(loss = \"categorical_crossentropy\", \n",
    "              optimizer = optimizers.SGD(lr = 1e-4, momentum = 0.9),\n",
    "              metrics = [\"accuracy\"])\n",
    "    model.summary()\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UFFF ENOUGH OF FUNCTIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install keras==2.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from google.colab import auth\n",
    "from google.colab import files\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = drive.CreateFile({'id':'1Pqjs7c5MAgPQLiMA3YR8dFg_YHLnPdlO'})\n",
    "file1.GetContentFile('dataset.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip dataset.zip -dq ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file2 = drive.CreateFile({'id':'18H1VBB5It4tneouiOhTqSQjFscOomAum'})\n",
    "#file2.GetContentFile('ernest.py')\n",
    "#from ernest import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete files appearing in more than one class in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train(\"dataset//train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Check whether any file appears in both training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = find_leak(\"dataset//train\", \"dataset//test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all training images to 3 channels\n",
    "process_train_images(\"dataset//train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Convert all test images to 3 channels\n",
    "process_test_images(\"dataset//test//data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_val_set(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Load the pre-trained Inception V3 model\n",
    "model = define_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VVI  Define data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range = 40,\n",
    "        width_shift_range = 0.2,\n",
    "        height_shift_range = 0.2,\n",
    "        shear_range = 0.2,\n",
    "        zoom_range = 0.2,\n",
    "        horizontal_flip = True,\n",
    "        preprocessing_function = preprocess_input\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        \"dataset//train\",\n",
    "        target_size = TARGET_SIZE, \n",
    "        batch_size = BATCH_SIZE, \n",
    "        class_mode = \"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        \"dataset//valid\", \n",
    "        target_size = TARGET_SIZE, \n",
    "        batch_size = BATCH_SIZE, \n",
    "        class_mode = \"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        \"dataset//test\", \n",
    "        target_size = TARGET_SIZE, \n",
    "        batch_size = BATCH_SIZE, \n",
    "        class_mode = None, \n",
    "        shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Define callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor = \"val_loss\", \n",
    "                          patience = 10, \n",
    "                          verbose = 1, \n",
    "                          mode = \"auto\")\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath = \"inceptionv3.hdf5\", \n",
    "                             verbose = 1, \n",
    "                             save_best_only = True)\n",
    "\n",
    "reducelr = ReduceLROnPlateau(monitor = \"val_loss\", \n",
    "                             factor = 0.2,\n",
    "                             patience = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit_generator(\n",
    "        generator = train_generator,\n",
    "        epochs = 10,\n",
    "        callbacks = [checkpoint, earlystop, reducelr],\n",
    "        validation_data = validation_generator,\n",
    "        verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = model.predict_generator(\n",
    "        test_generator,\n",
    "        verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(predictions, axis = 1)\n",
    "labels = train_generator.class_indices\n",
    "predictions = [list(labels.keys())[list(labels.values()).index(i)] for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model object\n",
    "json_model = model.to_json()\n",
    "with open(\"inceptionV3.json\", \"w\") as json_file:\n",
    "    json_file.write(json_model)\n",
    "    \n",
    "model.save_weights(\"inceptionV3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save prediction file\n",
    "new_subm = pd.DataFrame({\"filename\": test_generator.filenames, \"Superhero\": predictions})\n",
    "new_subm[\"filename\"] = new_subm[\"filename\"].apply(lambda x: x.split(\"/\")[1])\n",
    "new_subm[\"filename\"] = new_subm[\"filename\"].apply(lambda x: x.split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file3 = drive.CreateFile({\"title\": \"inceptionV3\"})\n",
    "file3.Upload()\n",
    "print('title: %s, id: %s' % (file3['title'], file3['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file3 = drive.CreateFile({\"id\": \"193czxV0V5nJa9qJtv7c-_UAUkUfcdTEf\"})\n",
    "file3.SetContentFile('./inceptionV3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file3.Upload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
