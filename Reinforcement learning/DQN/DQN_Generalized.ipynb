{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# getting in all the imports\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import keras\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util functions\n",
    "from PIL import Image\n",
    "def time_it(some_function):\n",
    "\n",
    "    \"\"\"\n",
    "    Outputs the time a function takes\n",
    "    to execute.\n",
    "    \"\"\"\n",
    "\n",
    "    def wrapper():\n",
    "        t1 = time.time()\n",
    "        some_function()\n",
    "        t2 = time.time()\n",
    "        return \"Time it took to run the function: \" + str((t2 - t1)) + \"\\n\"\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterators\n",
    "\n",
    "# Eplsilon Greedy Iterator\n",
    "def select_action(state,iteration, num_actions, start_value = 1, end_value = 0.1, \n",
    "                        num_iterations = 5e6, decay = 0.1,repeat = False):\n",
    "    \n",
    "    if repeat:\n",
    "        iteration %= num_iterations\n",
    "        \n",
    "    if iteration < num_iterations:\n",
    "        epsilon = (iteration * ((end_value - start_value) / num_iterations) + start_value) * \\\n",
    "                    np.exp(-iteration*decay/num_iterations)\n",
    "    else:\n",
    "        epsilon = end_value * np.exp(-iteration*decay/num_iterations)\n",
    "    \n",
    "    if np.random.random() < epsilon:\n",
    "        # Select a random action.\n",
    "        action = np.random.randint(low=0, high=num_actions)\n",
    "    else:\n",
    "        # Otherwise select the action that has the highest Q-value.\n",
    "        action = np.argmax(state)\n",
    "\n",
    "    return action, epsilon\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-20 22:53:11,016] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "#### Setting the working and environment variables\n",
    "\n",
    "# Game to play with\n",
    "env_name = 'Breakout-v0'\n",
    "\n",
    "# Create the game-environment using OpenAI Gym.\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# The number of possible actions that the agent may take in every step.\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# getting actions names\n",
    "# action_names = env.unwrapped.get_action_meanings()\n",
    "\n",
    "\n",
    "# Height of each image-frame in the state.\n",
    "state_height = 105\n",
    "\n",
    "# Width of each image-frame in the state.\n",
    "state_width = 80\n",
    "\n",
    "# Number of images in the state.\n",
    "state_channels = 2\n",
    "\n",
    "# Number of images in the state.\n",
    "state_channels = 4\n",
    "\n",
    "# Shape of the state-array.\n",
    "state_shape = [state_height, state_width, state_channels]\n",
    "\n",
    "# Set replay memory size based on RAM to be used\n",
    "replay_ram_space = 4\n",
    "\n",
    "# getting replay size based on RAM space available\n",
    "rp_size = int(np.ceil((replay_ram_space*1024*1024)/(state_height*state_width*state_channels*100))*100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Motion tracer for arbitary channels\n",
    "\n",
    "def _pre_process_image(image,state_img_size):\n",
    "    \"\"\"Pre-process a raw image from the game-environment.\"\"\"\n",
    "\n",
    "    # Convert image to gray-scale.\n",
    "    # Get the separate colour-channels.\n",
    "    r, g, b = image[:, :, 0], image[:, :, 1], image[:, :, 2]\n",
    "\n",
    "    # Convert to gray-scale using the Wikipedia formula.\n",
    "    img = 0.2990 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "    # Resize to the desired size using SciPy for convenience.\n",
    "    img = scipy.misc.imresize(img, size=state_img_size, interp='bicubic')\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "class MotionTracer:\n",
    "\n",
    "    def __init__(self, image, decay=0.75, num_images=2, state_height=105, state_width = 80):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param image:\n",
    "            First image from the game-environment,\n",
    "            used for resetting the motion detector.\n",
    "        :param decay:\n",
    "            Parameter for how long the tail should be on the motion-trace.\n",
    "            This is a float between 0.0 and 1.0 where higher values means\n",
    "            the trace / tail is longer.\n",
    "        :param num_images:\n",
    "            This is the array of image to be stored for detecting the motion \n",
    "            of the input\n",
    "        \"\"\"\n",
    "        \n",
    "        # Size of each image in the state.\n",
    "        self.state_img_size = np.array([state_height, state_width])\n",
    "        \n",
    "        # Preprocessing the image\n",
    "        img = _pre_process_image(image,self.state_img_size)\n",
    "        \n",
    "        # Initializing the imageset\n",
    "#         self.imageset = np.zeros(shape=[num_images] + np.zeros_like(img), dtype=np.float)\n",
    "        \n",
    "        # initialising image for first set and rest will be zeros\n",
    "#         self.imageset[num_images-1] = img.astype(np.float) \n",
    "        self.last_input = img.astype(np.float)\n",
    "\n",
    "#         # Set the last output to zero.\n",
    "#         for k in range(num_images-2):\n",
    "#             self.imageset[k] = np.zeros_like(img) \n",
    "            \n",
    "        self.last_output = np.zeros_like(img)\n",
    "        \n",
    "        # Storint the inputs to class\n",
    "        self.decay = decay\n",
    "        self.num_images = num_images\n",
    "\n",
    "    def process(self, image):\n",
    "        \"\"\"Process a raw image-frame from the game-environment.\"\"\"\n",
    "\n",
    "        # Pre-process the image so it is gray-scale and resized.\n",
    "        img = _pre_process_image(image,self.state_img_size)\n",
    "\n",
    "        # Subtract the previous input. This only leaves the\n",
    "        # pixels that have changed in the two image-frames.\n",
    "        img_dif = img - self.last_input\n",
    "        \n",
    "#         [for k in reversed(range(1,num_images-1))]\n",
    "            \n",
    "\n",
    "        # Copy the contents of the input-image to the last input.\n",
    "        self.last_input[:] = img[:]\n",
    "\n",
    "        # If the pixel-difference is greater than a threshold then\n",
    "        # set the output pixel-value to the highest value (white),\n",
    "        # otherwise set the output pixel-value to the lowest value (black).\n",
    "        # So that we merely detect motion, and don't care about details.\n",
    "        img_motion = np.where(np.abs(img_dif) > 20, 255.0, 0.0)\n",
    "\n",
    "        # Add some of the previous output. This recurrent formula\n",
    "        # is what gives the trace / tail.\n",
    "        output = img_motion + self.decay * self.last_output\n",
    "\n",
    "        # Ensure the pixel-values are within the allowed bounds.\n",
    "        output = np.clip(output, 0.0, 255.0)\n",
    "\n",
    "        # Set the last output.\n",
    "        self.last_output = output\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Get a state that can be used as input to the Neural Network.\n",
    "        It is basically just the last input and the last output of the\n",
    "        motion-tracer. This means it is the last image-frame of the\n",
    "        game-environment, as well as the motion-trace. This shows\n",
    "        the current location of all the objects in the game-environment\n",
    "        as well as trajectories / traces of where they have been.\n",
    "        \"\"\"\n",
    "\n",
    "        # Stack the last input and output images.\n",
    "        state = np.dstack([self.last_input, self.last_output])\n",
    "\n",
    "        # Convert to 8-bit integer.\n",
    "        # This is done to save space in the replay-memory.\n",
    "        state = state.astype(np.uint8)\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementig cart screen getter\n",
    "screen_width = 600\n",
    "\n",
    "\n",
    "def get_cart_location(env):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def get_screen(env):\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))  # transpose into torch order (CHW)\n",
    "    # Strip off the top and bottom of the screen\n",
    "    screen = screen[:, 160:320]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location(env)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescare, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return _pre_process_image(screen,np.array([600, 300]))\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import Conv2D, Dense,Flatten, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adagrad\n",
    "\n",
    "def buildmodel(num_actions, batch_size, img_rows=105, img_cols=80, img_channels = 2):\n",
    "    \n",
    "    init = keras.initializers.TruncatedNormal(mean=0.0, stddev=0.02, seed=None)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=16, kernel_size=3, strides=2, padding = 'same',activation = 'relu',\n",
    "                    kernel_initializer = init,data_format=\"channels_last\",\n",
    "                     input_shape=(img_rows, img_cols, img_channels)))\n",
    "    model.add(Conv2D(filters=32, kernel_size=3, strides=2, padding = 'same',activation = 'relu',\n",
    "                    kernel_initializer = init))\n",
    "    model.add(Conv2D(filters=64, kernel_size=2, strides=1, padding = 'same',activation = 'relu',\n",
    "                    kernel_initializer = init))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu',kernel_initializer = init))\n",
    "    model.add(Dense(1024, activation='relu',kernel_initializer = init))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu',kernel_initializer = init))\n",
    "    model.add(Dense(512, activation='relu',kernel_initializer = init))\n",
    "    model.add(Dense(num_actions,activation=None))\n",
    "   \n",
    "    adam = Adagrad(lr=1e-6)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    print(\"Model is built\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    \n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        # memory[i] = [[state_t, action_t, reward_t, state_t+1], game_over?]\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        \n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = model.output_shape[-1]\n",
    "        env_dim = [self.memory[0][0][0].shape[0], self.memory[0][0][0].shape[1], self.memory[0][0][0].shape[2]]\n",
    "        \n",
    "        \n",
    "#         inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "        inputs = np.zeros(shape=[min(len_memory, batch_size)] + env_dim, dtype=np.uint8)\n",
    "        temp_input = np.zeros(shape=[min(len_memory, batch_size)] + env_dim, dtype=np.uint8)\n",
    "#         print(inputs.shape)\n",
    "        targets = np.zeros((min(len_memory, batch_size), num_actions))\n",
    "        \n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory,size=inputs.shape[0])):\n",
    "            \n",
    "#             print(len(self.memory[idx][0]))\n",
    "#             print(self.memory[idx][0][3])\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "            \n",
    "#             print(state_t.shape)\n",
    "#             print(state_tpl.shape)\n",
    "\n",
    "            inputs[i:i+1] = state_t\n",
    "            temp_input[i:i+1] = self.memory[idx][0][3]\n",
    "            \n",
    "            # There should be no target values for actions not taken.\n",
    "            # Thou shalt not correct actions not taken #deep\n",
    "            targets[i] = model.predict(inputs)[0]\n",
    "            Q_sa = np.max(model.predict(temp_input)[0])\n",
    "            \n",
    "            if game_over:  # if game_over is True\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                # reward_t + gamma * max_a' Q(s', a')\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        return inputs, targets\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_initate()\n",
    "# model_load()\n",
    "# def run():\n",
    "#     env_execute()\n",
    "#     train_model()\n",
    "#     update_q_values()\n",
    "#     save_mode()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dqn(env,model,memory,episodes,iterations,batch_size = 32):\n",
    "    \n",
    "    num_action = env.action_space.n\n",
    "#     env.reset()\n",
    "    win_cnt = 0\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        loss = []\n",
    "#         env.reset()\n",
    "        game_over = False\n",
    "        \n",
    "        # get initial input\n",
    "#         screen = env.render(mode='rgb_array')\n",
    "#         screen = \n",
    "        env.reset()\n",
    "        try:\n",
    "            screen = env.render(mode='rgb_array')\n",
    "        except:\n",
    "            screen=env.reset()\n",
    "            print(screen.shape)\n",
    "#         get_screen(env)\n",
    "        motion_tracer = MotionTracer(screen)\n",
    "        \n",
    "        netReward = 0\n",
    "        for itr in range(iterations):\n",
    "            state = motion_tracer.get_state()\n",
    "            action,_ = select_action(state,itr,num_action)\n",
    "            img, reward, game_over, info = env.step(action=action)\n",
    "            \n",
    "            if not game_over:\n",
    "                motion_tracer.process(img)\n",
    "                next_state = motion_tracer.get_state()\n",
    "            else:\n",
    "                motion_tracer.process(np.zeros_like(state))\n",
    "                next_state = motion_tracer.get_state()\n",
    "            \n",
    "            # Adding to replay memory\n",
    "            memory.remember([state, action, reward, next_state], game_over)\n",
    "\n",
    "            # adapt model\n",
    "            inputs, targets = memory.get_batch(model, batch_size=batch_size)\n",
    "            loss.append(model.train_on_batch(inputs, targets))\n",
    "            if len(loss) % 10 == 0:\n",
    "                 print(\"Episode {:02d} | Iteration {:03d} | Loss {:.8f} | Win count {}\".format(\n",
    "                     e, itr,np.mean(loss), netReward))\n",
    "#             model.summary()\n",
    "#             loss += model.train_on_batch(inputs, targets)[0]\n",
    "            \n",
    "\n",
    "            netReward += reward\n",
    "            env.render()\n",
    "#             time.sleep(0.01)\n",
    "#             get_screen(env)\n",
    "#             plt.figure()\n",
    "# #             plt.imshow(get_screen(env).cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "# #                        interpolation='none')\n",
    "#             plt.imshow(env.render())\n",
    "#             plt.title('Example extracted screen')\n",
    "#             plt.show()\n",
    "        print(\"Episode {:02d} | Iteration {:03d} | Loss {:.8f} | Win count {}\".format(\n",
    "                     e, itr,np.mean(loss), netReward))\n",
    "            \n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    model.save_weights(\"model.h5\", overwrite=True)\n",
    "    with open(\"model.json\", \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is built\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/.local/lib/python3.5/site-packages/ipykernel_launcher.py:14: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 00 | Iteration 009 | Loss 0.00019140 | Win count 0.0\n",
      "Episode 00 | Iteration 019 | Loss 0.00019742 | Win count 0.0\n",
      "Episode 00 | Iteration 029 | Loss 0.00018928 | Win count 0.0\n",
      "Episode 00 | Iteration 039 | Loss 0.00018398 | Win count 0.0\n",
      "Episode 00 | Iteration 049 | Loss 0.00017931 | Win count 0.0\n"
     ]
    }
   ],
   "source": [
    "# exp_replay = ExperienceReplay(max_memory=rp_size)\n",
    "model = buildmodel(num_actions,32)\n",
    "\n",
    "memory = ExperienceReplay(rp_size)\n",
    "# memory = ReplayMemory(rp_size)\n",
    "\n",
    "run_dqn(env,model,memory,episodes = 10,iterations =  100,batch_size = 32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
