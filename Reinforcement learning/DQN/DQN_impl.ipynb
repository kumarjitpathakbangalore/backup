{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing everything at once\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import scipy.ndimage\n",
    "# import skimage.\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import math\n",
    "# import argparse\n",
    "# import download\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/jitin/Data/Misc/Exploration/Implementations'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting Environment to play with\n",
    "\n",
    "env_name = 'Breakout-v0'\n",
    "\n",
    "# Setting location of checkpoints\n",
    "# Note this directory will also take model backup for the chekpoint training\n",
    "checkpoint_base_dir = 'checkpoints/'\n",
    "\n",
    "# Combination of base-dir and environment-name.\n",
    "checkpoint_dir = None\n",
    "\n",
    "# Full path for the log-file for rewards.\n",
    "log_reward_path = None\n",
    "\n",
    "# Full path for the log-file for Q-values.\n",
    "log_q_values_path = None\n",
    "\n",
    "\n",
    "# Resetting values based on inputs\n",
    "if checkpoint_dir is None:\n",
    "    checkpoint_dir = os.path.join(checkpoint_base_dir, env_name)\n",
    "else:\n",
    "    checkpoint_dir = os.path.join(checkpoint_base_dir, '{}/{}'.format(checkpoint_dir,env_name))\n",
    "    \n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Setting logReward path\n",
    "if log_reward_path is None:\n",
    "    log_reward_path = os.path.join(checkpoint_dir, \"log_reward.txt\")\n",
    "else:\n",
    "    log_reward_path = os.path.join(checkpoint_dir, log_reward_path)\n",
    "    \n",
    "\n",
    "# File-path for the log-file for Q-values.\n",
    "if log_q_values_path is None:\n",
    "    log_q_values_path = os.path.join(checkpoint_dir, \"log_q_values.txt\")\n",
    "else:\n",
    "    log_q_values_path = os.path.join(checkpoint_dir, log_q_values_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Setting the working and environment variables\n",
    "\n",
    "# Create the game-environment using OpenAI Gym.\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# The number of possible actions that the agent may take in every step.\n",
    "num_actions = env.action_space.n\n",
    "# num_actions = 4\n",
    "\n",
    "# Whether we are training (True) or testing (False).\n",
    "training = True\n",
    "\n",
    "# Whether to render each image-frame of the game-environment to screen.\n",
    "render = False\n",
    "\n",
    "# Whether to use logging during training.\n",
    "use_logging = False\n",
    "\n",
    "# Height of each image-frame in the state.\n",
    "state_height = 105\n",
    "\n",
    "# Width of each image-frame in the state.\n",
    "state_width = 80\n",
    "\n",
    "# Size of each image in the state.\n",
    "state_img_size = np.array([state_height, state_width])\n",
    "\n",
    "# Number of images in the state.\n",
    "state_channels = 4\n",
    "\n",
    "# Shape of the state-array.\n",
    "state_shape = [state_height, state_width, state_channels]\n",
    "\n",
    "# Set replay memory size based on RAM to be used\n",
    "replay_ram_space = 4\n",
    "\n",
    "# getting replay size based on RAM space available\n",
    "rp_size = int(np.ceil((replay_ram_space*1024*1024)/(state_height*state_width*state_channels*100))*100000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting Replay memory Size, States, Q_atates and outcomes\n",
    "\n",
    "# Array for the previous states of the game-environment.\n",
    "states = np.zeros(shape=[rp_size] + state_shape, dtype=np.uint8)\n",
    "\n",
    "# Array for the Q-values corresponding to the states.\n",
    "q_values = np.zeros(shape=[rp_size, num_actions], dtype=np.float)\n",
    "\n",
    "# Array for the Q-values before being updated.\n",
    "# This is used to compare the Q-values before and after the update.\n",
    "q_values_old = np.zeros(shape=[rp_size, num_actions], dtype=np.float)\n",
    "\n",
    "# Actions taken for each of the states in the memory.\n",
    "actions = np.zeros(shape=rp_size, dtype=np.int)\n",
    "\n",
    "# Rewards observed for each of the states in the memory.\n",
    "rewards = np.zeros(shape=rp_size, dtype=np.float)\n",
    "\n",
    "# Whether the life had ended in each state of the game-environment.\n",
    "end_lifes = np.zeros(shape=rp_size, dtype=np.bool)\n",
    "\n",
    "# Whether the episode had ended (aka. game over) in each state.\n",
    "end_episodes = np.zeros(shape=rp_size, dtype=np.bool)\n",
    "\n",
    "# Estimation errors for the Q-values. .\n",
    "estimation_errors = np.zeros(shape=rp_size, dtype=np.float)\n",
    "\n",
    "# Discount-factor for calculating Q-values.\n",
    "discount_factor = 0.97\n",
    "\n",
    "# Reset the number of used states in the replay-memory.\n",
    "replay_num_used = 0\n",
    "\n",
    "# Threshold for splitting between low and high estimation errors.\n",
    "error_threshold = 0.1\n",
    "\n",
    "\n",
    "# Defining list for replay_memory\n",
    "# This Method might be time consuming but is easy to comprehend\n",
    "obj = [state_shape, q_values, action, reward, end_life, end_episode]\n",
    "replay_memory = []\n",
    "\n",
    "# Defining functions that will play with replay memory ;)\n",
    "def add_to_replay(state, q_value, action, reward, end_life, end_episode):\n",
    "       \n",
    "    # Calling globals varaibels and manuplating them\n",
    "    global replay_num_used\n",
    "    global rp_size\n",
    "    global states\n",
    "    global q_values\n",
    "    global actions\n",
    "    global end_lifes\n",
    "    global end_episodes\n",
    "    global rewards\n",
    "    \n",
    "    # if replay memory is not full we can add the new values\n",
    "    if replay_num_used != rp_size:\n",
    "            # Index into the arrays for convenience.\n",
    "            k = replay_num_used\n",
    "\n",
    "            # Increase the number of used elements in the replay-memory.\n",
    "            replay_num_used += 1\n",
    "\n",
    "            # Store all the values in the replay-memory.\n",
    "            states[k] = state\n",
    "            q_values[k] = q_value\n",
    "            actions[k] = action\n",
    "            end_lifes[k] = end_life\n",
    "            end_episodes[k] = end_episode\n",
    "\n",
    "            # Note that the reward is limited. This is done to stabilize\n",
    "            # the training of the Neural Network.\n",
    "            rewards[k] = np.clip(reward, -1.0, 1.0)\n",
    "\n",
    "def update_q_values():\n",
    "    # Calling globals varaibels and manuplating them\n",
    "    global replay_num_used\n",
    "    global rp_size\n",
    "    global states\n",
    "    global q_values\n",
    "    global q_values_old\n",
    "    global actions\n",
    "    global end_lifes\n",
    "    global end_episodes\n",
    "    global rewards\n",
    "    global discount_factor\n",
    "    global estimation_errors\n",
    "    \n",
    "    # taking backup of q_values\n",
    "    q_values_old[:] = q_values[:]\n",
    "    \n",
    "    # updating q_value in reverse order\n",
    "    for k in reversed(range(replay_num_used-1)):\n",
    "        action = actions[k]\n",
    "        reward = rewards[k]\n",
    "        end_life = end_lifes[k]\n",
    "        end_episode = end_episodes[k]\n",
    "        if end_life or end_episode:\n",
    "            action_value = reward\n",
    "        else:\n",
    "            action_value = reward + discount_factor * np.max(q_values[k + 1])\n",
    "        estimation_errors[k] = abs(action_value - q_values[k, action])\n",
    "        q_values[k, action] = action_value\n",
    "#         print_statistics()\n",
    "\n",
    "def get_replay_random_sample(repaly_memory,num_used,batch=64,samples=0.5):\n",
    "        \n",
    "    # getting indexes\n",
    "    indexes = np.arange(replay_num_used)\n",
    "    \n",
    "    # gettting random indexes\n",
    "    idx = np.random.choice(indexs,size=np.ceil(len(indexes)*samples),replace=False)\n",
    "    \n",
    "    return states[idx],q_values[idx]\n",
    "    \n",
    "def get_replay_stratified_sample(samples = 0.5):\n",
    "    \n",
    "    #TODO: To be implemented\n",
    "    \n",
    "    # including globals\n",
    "    global states\n",
    "    global q_values\n",
    "    global replay_num_used\n",
    "    \n",
    "    # getting indexes\n",
    "    indexes = np.arange(replay_num_used)\n",
    "    \n",
    "    # gettting random indexes\n",
    "    idx = np.random.choice(indexs,size=np.ceil(len(indexes)*samples),replace=False)\n",
    "    \n",
    "    return states[idx],q_values[idx]\n",
    "\n",
    "def estimate_q_values(model,batch_size=128):\n",
    "    \n",
    "    # including globals\n",
    "    global states\n",
    "    global q_values\n",
    "    global replay_num_used\n",
    "    \n",
    "    begin = 0\n",
    "    # this implementation is for doing iterative calculations n batch mode\n",
    "    while begin < replay_num_used:\n",
    "        # Setting the end index for the batch run\n",
    "        end = begin + batch_size\n",
    "        \n",
    "        # Ensure the batch does not exceed the used replay-memory.\n",
    "        if end > replay_num_used:\n",
    "            end = replay_num_used\n",
    "        \n",
    "        # Scoring q vlaues from the model\n",
    "        q_values[begin:end] = model.get_q_values(states[begin:end])\n",
    "        \n",
    "        # Set the start-index for the next batch to the end of this batch.\n",
    "        begin = end\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining Learning Rate, Loss Limit, Max Epochs, Replay Fraction and Epsilon Decay Greedy Iterators\n",
    "\n",
    "# Learning Rate iterator\n",
    "def learning_rate_itr(iteration, start_value = 1e-3, end_value = 1e-5, num_iterations = 5e6, repeat = False):\n",
    "    \n",
    "    if repeat:\n",
    "        iteration %= num_iterations\n",
    "        \n",
    "    if iteration < num_iterations:\n",
    "        value = iteration * ((end_value - start_value) / num_iterations) + start_value\n",
    "    else:\n",
    "        value = end_value\n",
    "\n",
    "    return value\n",
    "\n",
    "# Loss Limit Iterator\n",
    "def loss_limit_itr(iteration, start_value = 0.1, end_value = 0.015, num_iterations = 5e6, repeat = False):\n",
    "    \n",
    "    if repeat:\n",
    "        iteration %= num_iterations\n",
    "        \n",
    "    if iteration < num_iterations:\n",
    "        value = iteration * ((end_value - start_value) / num_iterations) + start_value\n",
    "    else:\n",
    "        value = end_value\n",
    "\n",
    "    return value\n",
    "\n",
    "# Max Epochs Iterator\n",
    "def max_epoch_itr(iteration, start_value = 5, end_value = 10, num_iterations = 5e6, repeat = False):\n",
    "    \n",
    "    if repeat:\n",
    "        iteration %= num_iterations\n",
    "        \n",
    "    if iteration < num_iterations:\n",
    "        value = iteration * ((end_value - start_value) / num_iterations) + start_value\n",
    "    else:\n",
    "        value = end_value\n",
    "\n",
    "    return value\n",
    "\n",
    "# Replay Fraction Iterator\n",
    "def replay_frac_itr(iteration, start_value = 0.1, end_value = 1, num_iterations = 5e6, repeat = False):\n",
    "    \n",
    "    if repeat:\n",
    "        iteration %= num_iterations\n",
    "        \n",
    "    if iteration < num_iterations:\n",
    "        value = iteration * ((end_value - start_value) / num_iterations) + start_value\n",
    "    else:\n",
    "        value = end_value\n",
    "\n",
    "    return value\n",
    "\n",
    "# Eplsilon Greedy Iterator\n",
    "def eplsilon_greedy_itr(iteration, num_actions, q_values,start_value = 1, end_value = 0.1, \n",
    "                        num_iterations = 5e6, repeat = False):\n",
    "    \n",
    "    if repeat:\n",
    "        iteration %= num_iterations\n",
    "        \n",
    "    if iteration < num_iterations:\n",
    "        epsilon = iteration * ((end_value - start_value) / num_iterations) + start_value\n",
    "    else:\n",
    "        epsilon = end_value\n",
    "    \n",
    "    if np.random.random() < epsilon:\n",
    "        # Select a random action.\n",
    "        action = np.random.randint(low=0, high=num_actions)\n",
    "    else:\n",
    "        # Otherwise select the action that has the highest Q-value.\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "    return action, epsilon\n",
    "\n",
    "\n",
    "# Eplsilon Decay Iterator\n",
    "def eplsilon_decay_itr(iteration, num_actions, q_values, decay, start_value = 1, end_value = 0.1, \n",
    "                        num_iterations = 5e6, repeat = False):\n",
    "    \n",
    "    # TODO: Implement Decay factor\n",
    "    \n",
    "    if repeat:\n",
    "        iteration %= num_iterations\n",
    "        \n",
    "    if iteration < num_iterations:\n",
    "        epsilon = iteration * ((end_value - start_value) / num_iterations) + start_value\n",
    "    else:\n",
    "        epsilon = end_value\n",
    "    \n",
    "    if np.random.random() < epsilon:\n",
    "        # Select a random action.\n",
    "        action = np.random.randint(low=0, high=num_actions)\n",
    "    else:\n",
    "        # Otherwise select the action that has the highest Q-value.\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "    return action, epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Motion tracer for arbitary channels\n",
    "# Also removing comments fo reasy comprehension\n",
    "\n",
    "def _rgb_to_grayscale(image):\n",
    "    \"\"\"\n",
    "    Convert an RGB-image into gray-scale using a formula from Wikipedia:\n",
    "    https://en.wikipedia.org/wiki/Grayscale\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the separate colour-channels.\n",
    "    r, g, b = image[:, :, 0], image[:, :, 1], image[:, :, 2]\n",
    "\n",
    "    # Convert to gray-scale using the Wikipedia formula.\n",
    "    img_gray = 0.2990 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "    return img_gray\n",
    "\n",
    "\n",
    "def _pre_process_image(image):\n",
    "    \"\"\"Pre-process a raw image from the game-environment.\"\"\"\n",
    "\n",
    "    # Convert image to gray-scale.\n",
    "    img = _rgb_to_grayscale(image)\n",
    "\n",
    "    # Resize to the desired size using SciPy for convenience.\n",
    "    img = scipy.misc.imresize(img, size=state_img_size, interp='bicubic')\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "class MotionTracer:\n",
    "\n",
    "    def __init__(self, image, decay=0.75, num_images=4):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param image:\n",
    "            First image from the game-environment,\n",
    "            used for resetting the motion detector.\n",
    "        :param decay:\n",
    "            Parameter for how long the tail should be on the motion-trace.\n",
    "            This is a float between 0.0 and 1.0 where higher values means\n",
    "            the trace / tail is longer.\n",
    "        :param num_images:\n",
    "            This is the array of image to be stored for detecting the motion \n",
    "            of the input\n",
    "        \"\"\"\n",
    "        \n",
    "        # Preprocessing the image\n",
    "        img = _pre_process_image(image=image)\n",
    "        \n",
    "        # Initializing the imageset\n",
    "        self.imageset = np.zeros(shape=[num_images] + np.zeros_like(img), dtype=np.float)\n",
    "        \n",
    "        # initialising image for first set and rest will be zeros\n",
    "        self.imageset[num_images-1] = img.astype(np.float) \n",
    "        self.last_input = img.astype(np.float)\n",
    "\n",
    "        # Set the last output to zero.\n",
    "        for k in range(num_images-2):\n",
    "            self.imageset[k] = np.zeros_like(img) \n",
    "            \n",
    "        self.last_output = np.zeros_like(img)\n",
    "        \n",
    "        # Storint the inputs to class\n",
    "        self.decay = decay\n",
    "        self.num_images = num_images\n",
    "\n",
    "    def process(self, image):\n",
    "        \"\"\"Process a raw image-frame from the game-environment.\"\"\"\n",
    "\n",
    "        # Pre-process the image so it is gray-scale and resized.\n",
    "        img = _pre_process_image(image=image)\n",
    "\n",
    "        # Subtract the previous input. This only leaves the\n",
    "        # pixels that have changed in the two image-frames.\n",
    "        img_dif = img - self.last_input\n",
    "        \n",
    "#         [for k in reversed(range(1,num_images-1))]\n",
    "            \n",
    "\n",
    "        # Copy the contents of the input-image to the last input.\n",
    "        self.last_input[:] = img[:]\n",
    "\n",
    "        # If the pixel-difference is greater than a threshold then\n",
    "        # set the output pixel-value to the highest value (white),\n",
    "        # otherwise set the output pixel-value to the lowest value (black).\n",
    "        # So that we merely detect motion, and don't care about details.\n",
    "        img_motion = np.where(np.abs(img_dif) > 20, 255.0, 0.0)\n",
    "\n",
    "        # Add some of the previous output. This recurrent formula\n",
    "        # is what gives the trace / tail.\n",
    "        output = img_motion + self.decay * self.last_output\n",
    "\n",
    "        # Ensure the pixel-values are within the allowed bounds.\n",
    "        output = np.clip(output, 0.0, 255.0)\n",
    "\n",
    "        # Set the last output.\n",
    "        self.last_output = output\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Get a state that can be used as input to the Neural Network.\n",
    "        It is basically just the last input and the last output of the\n",
    "        motion-tracer. This means it is the last image-frame of the\n",
    "        game-environment, as well as the motion-trace. This shows\n",
    "        the current location of all the objects in the game-environment\n",
    "        as well as trajectories / traces of where they have been.\n",
    "        \"\"\"\n",
    "\n",
    "        # Stack the last input and output images.\n",
    "        state = np.dstack([self.last_input, self.last_output])\n",
    "\n",
    "        # Convert to 8-bit integer.\n",
    "        # This is done to save space in the replay-memory.\n",
    "        state = state.astype(np.uint8)\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will build the neural network architecture with keras\n",
    "\n",
    "from keras.layers import Conv2D, Dense,Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "def neural_net(state_shape, num_actions):\n",
    "    \n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None] + state_shape, name='x')\n",
    "\n",
    "    # Placeholder variable for inputting the learning-rate to the optimizer.\n",
    "    learning_rate = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "\n",
    "    # Placeholder variable for inputting the target Q-values\n",
    "    # that we want the Neural Network to be able to estimate.\n",
    "    q_values_new = tf.placeholder(tf.float32, shape=[None, num_actions], name='q_values_new')\n",
    "\n",
    "    # This is a hack that allows us to save/load the counter for\n",
    "    # the number of states processed in the game-environment.\n",
    "    # We will keep it as a variable in the TensorFlow-graph\n",
    "    # even though it will not actually be used by TensorFlow.\n",
    "    count_states = tf.Variable(initial_value=0, trainable=False, dtype=tf.int64, name='count_states')\n",
    "\n",
    "    # Similarly, this is the counter for the number of episodes.\n",
    "    count_episodes = tf.Variable(initial_value=0, trainable=False, dtype=tf.int64, name='count_episodes')\n",
    "\n",
    "    # TensorFlow operation for increasing count_states.\n",
    "    count_states_increase = tf.assign(self.count_states, self.count_states + 1)\n",
    "\n",
    "    # TensorFlow operation for increasing count_episodes.\n",
    "    count_episodes_increase = tf.assign(self.count_episodes, self.count_episodes + 1)\n",
    "    \n",
    "    # Now we will build the architecture\n",
    "\n",
    "    # initializing the network with initializer\n",
    "    init = keras.initializers.TruncatedNormal(mean=0.0, stddev=0.02, seed=None)\n",
    "    \n",
    "    # Making the network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=16, kernel_size=3, strides=2, padding = 'same',activation = 'relu',\n",
    "                    kernel_initializer = init))\n",
    "    model.add(Conv2D(filters=32, kernel_size=3, strides=2, padding = 'same',activation = 'relu',\n",
    "                    kernel_initializer = init))\n",
    "    model.add(Conv2D(filters=64, kernel_size=3, strides=1, padding = 'same',activation = 'relu',\n",
    "                    kernel_initializer = init))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu',kernel_initializer = init))\n",
    "    model.add(Dense(1024, activation='relu',kernel_initializer = init))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu',kernel_initializer = init))\n",
    "    model.add(Dense(512, activation='relu',kernel_initializer = init))\n",
    "    model.add(Dense(num_actions,activation=None))\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
