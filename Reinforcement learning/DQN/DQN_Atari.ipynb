{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing reinforcement learning module in jupyter\n",
    "\n",
    "# importing all required libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import scipy.ndimage\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "#import argparse\n",
    "import download\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lenovo/Desktop/jitin'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluting the initial supporting function\n",
    "\n",
    "# Defining checkpoint directory\n",
    "checkpoint_base_dir = '/home/lenovo/Desktop/jitin/DQNExperiment_JKKP/'\n",
    "\n",
    "\n",
    "# Defining subdirectory under base directory\n",
    "# Combination of base-dir and environment-name.\n",
    "checkpoint_dir = None\n",
    "\n",
    "# Full path for the log-file for rewards.\n",
    "log_reward_path = None\n",
    "\n",
    "# Full path for the log-file for Q-values.\n",
    "log_q_values_path = None\n",
    "\n",
    "\n",
    "# Completing the paths for each directory\n",
    "def update_paths(env_name):\n",
    "    \"\"\"\n",
    "    Update the path-names for the checkpoint-dir and log-files.\n",
    "    \n",
    "    Call this after you have changed checkpoint_base_dir and\n",
    "    before you create the Neural Network.\n",
    "    \n",
    "    :param env_name:\n",
    "        Name of the game-environment you will use in OpenAI Gym.\n",
    "    \"\"\"\n",
    "    global checkpoint_base_dir\n",
    "    global checkpoint_dir # these are varaibels whihc are available globally across all functions and classes\n",
    "    global log_reward_path\n",
    "    global log_q_values_path\n",
    "\n",
    "    # Add the environment-name to the checkpoint-dir.\n",
    "    checkpoint_dir = os.path.join(checkpoint_base_dir, env_name)\n",
    "\n",
    "    # Create the checkpoint-dir if it does not already exist.\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    # File-path for the log-file for episode rewards.\n",
    "    log_reward_path = os.path.join(checkpoint_dir, \"log_reward.txt\")\n",
    "\n",
    "    # File-path for the log-file for Q-values.\n",
    "    log_q_values_path = os.path.join(checkpoint_dir, \"log_q_values.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lenovo/Desktop/jitin\n",
      "/home/lenovo/Desktop/jitin/log_reward.txt\n",
      "/home/lenovo/Desktop/jitin/log_q_values.txt\n"
     ]
    }
   ],
   "source": [
    "update_paths(os.getcwd())\n",
    "print(checkpoint_dir)\n",
    "print(log_reward_path)\n",
    "print(log_q_values_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For downloading or loading the files\n",
    "# # URL's for the checkpoint-files.\n",
    "# _checkpoint_url = {\n",
    "#     \"Breakout-v0\": \"http://hvass-labs.org/projects/tensorflow/tutorial16/Breakout-v0.tar.gz\",\n",
    "#     \"SpaceInvaders-v0\": \"http://hvass-labs.org/projects/tensorflow/tutorial16/SpaceInvaders-v0.tar.gz\"\n",
    "# }\n",
    "\n",
    "\n",
    "# def maybe_download_checkpoint(env_name):\n",
    "#     \"\"\"\n",
    "#     Download and extract the TensorFlow checkpoint for the given\n",
    "#     environment-name, if it does not already exist in checkpoint_base_dir.\n",
    "#     You should first set this dir and call update_paths().\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Get the url for the game-environment.\n",
    "#     url = _checkpoint_url[env_name]\n",
    "\n",
    "#     # Download and extract the file if it does not already exist.\n",
    "#     download.maybe_download_and_extract(url=url,\n",
    "#                                         download_dir=checkpoint_base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Log class\n",
    "\n",
    "# In a class , __init__ define the varaibles and may pass valus to these variables of a class object\n",
    "# Also 'self' is an identifier of class own behaviour and variabels \n",
    "\n",
    "class Log:\n",
    "    \"\"\"\n",
    "    Base-class for logging data to a text-file during training.\n",
    "\n",
    "    It is possible to use TensorFlow / TensorBoard for this,\n",
    "    but it is quite awkward to implement, as it was intended\n",
    "    for logging variables and other aspects of the TensorFlow graph.\n",
    "    We want to log the reward and Q-values which are not in that graph.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path): \n",
    "        \"\"\"Set the path for the log-file. Nothing is saved or loaded yet.\"\"\"\n",
    "\n",
    "        # Path for the log-file.\n",
    "        self.file_path = file_path\n",
    "\n",
    "        # Data to be read from the log-file by the _read() function.\n",
    "        self.count_episodes = None\n",
    "        self.count_states = None\n",
    "        self.data = None\n",
    "        \n",
    "    def _write(self, count_episodes, count_states, msg):\n",
    "        \"\"\"\n",
    "        Write a line to the log-file. This is only called by sub-classes.\n",
    "        \n",
    "        :param count_episodes:\n",
    "            Counter for the number of episodes processed during training.\n",
    "\n",
    "        :param count_states: \n",
    "            Counter for the number of states processed during training.\n",
    "\n",
    "        :param msg:\n",
    "            Message to write in the log.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(file=self.file_path, mode='a', buffering=1) as file:\n",
    "            msg_annotated = \"{0}\\t{1}\\t{2}\\n\".format(count_episodes, count_states, msg)\n",
    "            file.write(msg_annotated)\n",
    "\n",
    "    def _read(self):\n",
    "        \"\"\"\n",
    "        Read the log-file into memory so it can be plotted.\n",
    "\n",
    "        It sets self.count_episodes, self.count_states and self.data\n",
    "        \"\"\"\n",
    "\n",
    "        # Open and read the log-file.\n",
    "        with open(self.file_path) as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\")\n",
    "            self.count_episodes, self.count_states, *data = zip(*reader) \n",
    "            # iteratively storing the objects in the data line by line\n",
    "\n",
    "        # Convert the remaining log-data to a NumPy float-array.\n",
    "        self.data = np.array(data, dtype='float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(shape = [10] + [105,80,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros_like(np.ones((3,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impelment Log Behaviour for KP Sir\n",
    "# How the logs for Rewards will written and retrieved\n",
    "\n",
    "class LogReward(Log):\n",
    "    \"\"\"Log the rewards obtained for episodes during training.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # These will be set in read() below.\n",
    "        self.episode = None\n",
    "        self.mean = None\n",
    "\n",
    "        # Super-class init.\n",
    "        Log.__init__(self, file_path=log_reward_path)\n",
    "\n",
    "    def write(self, count_episodes, count_states, reward_episode, reward_mean):\n",
    "        \"\"\"\n",
    "        Write the episode and mean reward to file.\n",
    "        \n",
    "        :param count_episodes:\n",
    "            Counter for the number of episodes processed during training.\n",
    "\n",
    "        :param count_states: \n",
    "            Counter for the number of states processed during training.\n",
    "\n",
    "        :param reward_episode:\n",
    "            Reward for one episode.\n",
    "\n",
    "        :param reward_mean:\n",
    "            Mean reward for the last e.g. 30 episodes.\n",
    "        \"\"\"\n",
    "\n",
    "        msg = \"{0:.1f}\\t{1:.1f}\".format(reward_episode, reward_mean)\n",
    "        self._write(count_episodes=count_episodes, count_states=count_states, msg=msg)\n",
    "\n",
    "    def read(self):\n",
    "        \"\"\"\n",
    "        Read the log-file into memory so it can be plotted.\n",
    "\n",
    "        It sets self.count_episodes, self.count_states, self.episode and self.mean\n",
    "        \"\"\"\n",
    "\n",
    "        # Read the log-file using the super-class.\n",
    "        self._read()\n",
    "\n",
    "        # Get the episode reward.\n",
    "        self.episode = self.data[0]\n",
    "\n",
    "        # Get the mean reward.\n",
    "        self.mean = self.data[1]\n",
    "\n",
    "class LogQValues(Log):\n",
    "    \"\"\"Log the Q-Values during training.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # These will be set in read() below.\n",
    "        self.min = None\n",
    "        self.mean = None\n",
    "        self.max = None\n",
    "        self.std = None\n",
    "\n",
    "        # Super-class init.\n",
    "        Log.__init__(self, file_path=log_q_values_path)\n",
    "\n",
    "    def write(self, count_episodes, count_states, q_values):\n",
    "        \"\"\"\n",
    "        Write basic statistics for the Q-values to file.\n",
    "\n",
    "        :param count_episodes:\n",
    "            Counter for the number of episodes processed during training.\n",
    "\n",
    "        :param count_states: \n",
    "            Counter for the number of states processed during training.\n",
    "\n",
    "        :param q_values:\n",
    "            Numpy array with Q-values from the replay-memory.\n",
    "        \"\"\"\n",
    "\n",
    "        msg = \"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(np.min(q_values),\n",
    "                                                          np.mean(q_values),\n",
    "                                                          np.max(q_values),\n",
    "                                                          np.std(q_values))\n",
    "\n",
    "        self._write(count_episodes=count_episodes,\n",
    "                    count_states=count_states,\n",
    "                    msg=msg)\n",
    "\n",
    "    def read(self):\n",
    "        \"\"\"\n",
    "        Read the log-file into memory so it can be plotted.\n",
    "\n",
    "        It sets self.count_episodes, self.count_states, self.min / mean / max / std.\n",
    "        \"\"\"\n",
    "\n",
    "        # Read the log-file using the super-class.\n",
    "        self._read()\n",
    "\n",
    "        # Get the logged statistics for the Q-values.\n",
    "        self.min = self.data[0]\n",
    "        self.mean = self.data[1]\n",
    "        self.max = self.data[2]\n",
    "        self.std = self.data[3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining print function for progress in all os\n",
    "def print_progress(msg):\n",
    "    \"\"\"\n",
    "    Print progress on a single line and overwrite the line.\n",
    "    Used during optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    sys.stdout.write(\"\\r\" + msg)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Variables fo image to all models/ learning games\n",
    "\n",
    "\n",
    "# Height of each image-frame in the state.\n",
    "state_height = 105\n",
    "\n",
    "# Width of each image-frame in the state.\n",
    "state_width = 80\n",
    "\n",
    "# Size of each image in the state.\n",
    "state_img_size = np.array([state_height, state_width])\n",
    "\n",
    "# Number of images in the state.\n",
    "state_channels = 2\n",
    "\n",
    "# Shape of the state-array.\n",
    "state_shape = [state_height, state_width, state_channels]\n",
    "\n",
    "def _rgb_to_grayscale(image):\n",
    "    \"\"\"\n",
    "    Convert an RGB-image into gray-scale using a formula from Wikipedia:\n",
    "    https://en.wikipedia.org/wiki/Grayscale\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the separate colour-channels.\n",
    "    r, g, b = image[:, :, 0], image[:, :, 1], image[:, :, 2] # picking matrix at color values from th image\n",
    "\n",
    "    # Convert to gray-scale using the Wikipedia formula.\n",
    "    img_gray = 0.2990 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "    return img_gray\n",
    "\n",
    "\n",
    "def _pre_process_image(image):\n",
    "    \"\"\"Pre-process a raw image from the game-environment.\"\"\"\n",
    "\n",
    "    # Convert image to gray-scale.\n",
    "    img = _rgb_to_grayscale(image)\n",
    "\n",
    "    # Resize to the desired size using SciPy for convenience.\n",
    "    img = scipy.misc.imresize(img, size=state_img_size, interp='bicubic')\n",
    "\n",
    "    return img\n",
    "\n",
    "# Tracking Class of motion of object\n",
    "\n",
    "class MotionTracer:\n",
    "    \"\"\"\n",
    "    Used for processing raw image-frames from the game-environment.\n",
    "\n",
    "    The image-frames are converted to gray-scale, resized, and then\n",
    "    the background is removed using filtering of the image-frames\n",
    "    so as to detect motions.\n",
    "\n",
    "    This is needed because a single image-frame of the game environment\n",
    "    is insufficient to determine the direction of moving objects.\n",
    "    \n",
    "    The original DeepMind implementation used the last 4 image-frames\n",
    "    of the game-environment to allow the Neural Network to learn how\n",
    "    to detect motion. This implementation could make it a little easier\n",
    "    for the Neural Network to learn how to detect motion, but it has\n",
    "    only been tested on Breakout and Space Invaders, and may not work\n",
    "    for games with more complicated graphics such as Doom. This remains\n",
    "    to be tested.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image, decay=0.75):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param image:\n",
    "            First image from the game-environment,\n",
    "            used for resetting the motion detector.\n",
    "\n",
    "        :param decay:\n",
    "            Parameter for how long the tail should be on the motion-trace.\n",
    "            This is a float between 0.0 and 1.0 where higher values means\n",
    "            the trace / tail is longer.\n",
    "        \"\"\"\n",
    "\n",
    "        # Pre-process the image and save it for later use.\n",
    "        # The input image may be 8-bit integers but internally\n",
    "        # we need to use floating-point to avoid image-noise\n",
    "        # caused by recurrent rounding-errors.\n",
    "        img = _pre_process_image(image=image)\n",
    "        self.last_input = self.sec_last = self.third_last = self.first_input = img.astype(np.float)\n",
    "        \n",
    "        #TODO: To be implemented\n",
    "        \n",
    "        # Set the last output to zero.\n",
    "        self.last_output = np.zeros_like(img)\n",
    "\n",
    "        self.decay = decay\n",
    "\n",
    "    def process(self, image):\n",
    "        \"\"\"Process a raw image-frame from the game-environment.\"\"\"\n",
    "\n",
    "        # Pre-process the image so it is gray-scale and resized.\n",
    "        img = _pre_process_image(image=image)\n",
    "\n",
    "        # Subtract the previous input. This only leaves the\n",
    "        # pixels that have changed in the two image-frames.\n",
    "        img_dif = img - self.last_input\n",
    "\n",
    "        # Copy the contents of the input-image to the last input.\n",
    "        self.last_input[:] = img[:]\n",
    "\n",
    "        # If the pixel-difference is greater than a threshold then\n",
    "        # set the output pixel-value to the highest value (white),\n",
    "        # otherwise set the output pixel-value to the lowest value (black).\n",
    "        # So that we merely detect motion, and don't care about details.\n",
    "        img_motion = np.where(np.abs(img_dif) > 20, 255.0, 0.0) \n",
    "        # threshold can be cha\n",
    "\n",
    "        # Add some of the previous output. This recurrent formula\n",
    "        # is what gives the trace / tail.\n",
    "        output = img_motion + self.decay * self.last_output\n",
    "\n",
    "        # Ensure the pixel-values are within the allowed bounds.\n",
    "        output = np.clip(output, 0.0, 255.0)\n",
    "\n",
    "        # Set the last output.\n",
    "        self.last_output = output\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Get a state that can be used as input to the Neural Network.\n",
    "\n",
    "        It is basically just the last input and the last output of the\n",
    "        motion-tracer. This means it is the last image-frame of the\n",
    "        game-environment, as well as the motion-trace. This shows\n",
    "        the current location of all the objects in the game-environment\n",
    "        as well as trajectories / traces of where they have been.\n",
    "        \"\"\"\n",
    "\n",
    "        # Stack the last input and output images.\n",
    "        state = np.dstack([self.last_input, self.last_output])\n",
    "\n",
    "        # Convert to 8-bit integer.\n",
    "        # This is done to save space in the replay-memory.\n",
    "        state = state.astype(np.uint8)\n",
    "\n",
    "        return state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Replay Memory theory with Real Experience\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    The replay-memory holds many previous all states of the game-environment.\n",
    "    This helps stabilize training of the Neural Network because the data\n",
    "    is more diverse when sampled over thousands of different states.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, num_actions, discount_factor=0.97):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param size:\n",
    "            Capacity of the replay-memory. This is the number of states.\n",
    "\n",
    "        :param num_actions:\n",
    "            Number of possible actions in the game-environment. \n",
    "\n",
    "        :param discount_factor:\n",
    "            Discount-factor used for updating Q-values.\n",
    "        \"\"\"\n",
    "\n",
    "        # Array for the previous states of the game-environment.\n",
    "        self.states = np.zeros(shape=[size] + [state_height, state_width, state_channels], dtype=np.uint8)\n",
    "\n",
    "        # Array for the Q-values corresponding to the states.\n",
    "        self.q_values = np.zeros(shape=[size, num_actions], dtype=np.float)\n",
    "\n",
    "        # Array for the Q-values before being updated.\n",
    "        # This is used to compare the Q-values before and after the update.\n",
    "        self.q_values_old = np.zeros(shape=[size, num_actions], dtype=np.float)\n",
    "\n",
    "        # Actions taken for each of the states in the memory.\n",
    "        self.actions = np.zeros(shape=size, dtype=np.int)\n",
    "\n",
    "        # Rewards observed for each of the states in the memory.\n",
    "        self.rewards = np.zeros(shape=size, dtype=np.float)\n",
    "\n",
    "        # Whether the life had ended in each state of the game-environment.\n",
    "        self.end_life = np.zeros(shape=size, dtype=np.bool)\n",
    "\n",
    "        # Whether the episode had ended (aka. game over) in each state.\n",
    "        self.end_episode = np.zeros(shape=size, dtype=np.bool)\n",
    "\n",
    "        # Estimation errors for the Q-values. This is used to balance\n",
    "        # the sampling of batches for training the Neural Network,\n",
    "        # so we get a balanced combination of states with high and low\n",
    "        # estimation errors for their Q-values.\n",
    "        self.estimation_errors = np.zeros(shape=size, dtype=np.float)\n",
    "\n",
    "        # Capacity of the replay-memory as the number of states.\n",
    "        self.size = size\n",
    "\n",
    "        # Discount-factor for calculating Q-values.\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        # Reset the number of used states in the replay-memory.\n",
    "        self.num_used = 0\n",
    "\n",
    "        # Threshold for splitting between low and high estimation errors.\n",
    "        self.error_threshold = 0.1\n",
    "\n",
    "    def is_full(self):\n",
    "        \"\"\"Return boolean whether the replay-memory is full.\"\"\"\n",
    "        return self.num_used == self.size\n",
    "\n",
    "    def used_fraction(self):\n",
    "        \"\"\"Return the fraction of the replay-memory that is used.\"\"\"\n",
    "        return self.num_used / self.size\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the replay-memory so it is empty.\"\"\"\n",
    "        self.num_used = 0\n",
    "\n",
    "    def add(self, state, q_values, action, reward, end_life, end_episode):\n",
    "        \"\"\"\n",
    "        Add an observed state from the game-environment, along with the\n",
    "        estimated Q-values, action taken, observed reward, etc. \n",
    "        \n",
    "        :param state:\n",
    "            Current state of the game-environment.\n",
    "            This is the output of the MotionTracer-class.\n",
    "\n",
    "        :param q_values: \n",
    "            The estimated Q-values for the state.\n",
    "\n",
    "        :param action: \n",
    "            The action taken by the agent in this state of the game.\n",
    "\n",
    "        :param reward:\n",
    "            The reward that was observed from taking this action\n",
    "            and moving to the next state.\n",
    "\n",
    "        :param end_life:\n",
    "            Boolean whether the agent has lost a life in this state.\n",
    "         \n",
    "        :param end_episode: \n",
    "            Boolean whether the agent has lost all lives aka. game over\n",
    "            aka. end of episode.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.is_full():\n",
    "            # Index into the arrays for convenience.\n",
    "            k = self.num_used\n",
    "\n",
    "            # Increase the number of used elements in the replay-memory.\n",
    "            self.num_used += 1\n",
    "\n",
    "            # Store all the values in the replay-memory.\n",
    "            self.states[k] = state\n",
    "            self.q_values[k] = q_values\n",
    "            self.actions[k] = action\n",
    "            self.end_life[k] = end_life\n",
    "            self.end_episode[k] = end_episode\n",
    "\n",
    "            # Note that the reward is limited. This is done to stabilize\n",
    "            # the training of the Neural Network.\n",
    "            self.rewards[k] = np.clip(reward, -1.0, 1.0)\n",
    "\n",
    "    def update_all_q_values(self):\n",
    "        \"\"\"\n",
    "        Update all Q-values in the replay-memory.\n",
    "        \n",
    "        When states and Q-values are added to the replay-memory, the\n",
    "        Q-values have been estimated by the Neural Network. But we now\n",
    "        have more data available that we can use to improve the estimated\n",
    "        Q-values, because we now know which actions were taken and the\n",
    "        observed rewards. We sweep backwards through the entire replay-memory\n",
    "        to use the observed data to improve the estimated Q-values.\n",
    "        \"\"\"\n",
    "\n",
    "        # Copy old Q-values so we can print their statistics later.\n",
    "        # Note that the contents of the arrays are copied.\n",
    "        self.q_values_old[:] = self.q_values[:]\n",
    "\n",
    "        # Process the replay-memory backwards and update the Q-values.\n",
    "        # This loop could be implemented entirely in NumPy for higher speed,\n",
    "        # but it is probably only a small fraction of the overall time usage,\n",
    "        # and it is much easier to understand when implemented like this.\n",
    "        for k in reversed(range(self.num_used-1)):\n",
    "            # Get the data for the k'th state in the replay-memory.\n",
    "            action = self.actions[k]\n",
    "            reward = self.rewards[k]\n",
    "            end_life = self.end_life[k]\n",
    "            end_episode = self.end_episode[k]\n",
    "\n",
    "            # Calculate the Q-value for the action that was taken in this state.\n",
    "            if end_life or end_episode:\n",
    "                # If the agent lost a life or it was game over / end of episode,\n",
    "                # then the value of taking the given action is just the reward\n",
    "                # that was observed in this single step. This is because the\n",
    "                # Q-value is defined as the discounted value of all future game\n",
    "                # steps in a single life of the agent. When the life has ended,\n",
    "                # there will be no future steps.\n",
    "                action_value = reward\n",
    "            else:\n",
    "                # Otherwise the value of taking the action is the reward that\n",
    "                # we have observed plus the discounted value of future rewards\n",
    "                # from continuing the game. We use the estimated Q-values for\n",
    "                # the following state and take the maximum, because we will\n",
    "                # generally take the action that has the highest Q-value.\n",
    "                action_value = reward + self.discount_factor * np.max(self.q_values[k + 1])\n",
    "\n",
    "            # Error of the Q-value that was estimated using the Neural Network.\n",
    "            self.estimation_errors[k] = abs(action_value - self.q_values[k, action])\n",
    "\n",
    "            # Update the Q-value with the better estimate.\n",
    "            self.q_values[k, action] = action_value\n",
    "\n",
    "        self.print_statistics()\n",
    "\n",
    "    def prepare_sampling_prob(self, batch_size=128):\n",
    "        \"\"\"\n",
    "        Prepare the probability distribution for random sampling of states\n",
    "        and Q-values for use in training of the Neural Network.\n",
    "\n",
    "        The probability distribution is just a simple binary split of the\n",
    "        replay-memory based on the estimation errors of the Q-values.\n",
    "        The idea is to create a batch of samples that are balanced somewhat\n",
    "        evenly between Q-values that the Neural Network already knows how to\n",
    "        estimate quite well because they have low estimation errors, and\n",
    "        Q-values that are poorly estimated by the Neural Network because\n",
    "        they have high estimation errors.\n",
    "        \n",
    "        The reason for this balancing of Q-values with high and low estimation\n",
    "        errors, is that if we train the Neural Network mostly on data with\n",
    "        high estimation errors, then it will tend to forget what it already\n",
    "        knows and hence become over-fit so the training becomes unstable.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the errors between the Q-values that were estimated using\n",
    "        # the Neural Network, and the Q-values that were updated with the\n",
    "        # reward that was actually observed when an action was taken.\n",
    "        err = self.estimation_errors[0:self.num_used]\n",
    "\n",
    "        # Create an index of the estimation errors that are low.\n",
    "        idx = err<self.error_threshold\n",
    "        self.idx_err_lo = np.squeeze(np.where(idx))\n",
    "\n",
    "        # Create an index of the estimation errors that are high.\n",
    "        self.idx_err_hi = np.squeeze(np.where(np.logical_not(idx)))\n",
    "\n",
    "        # Probability of sampling Q-values with high estimation errors.\n",
    "        # This is either set to the fraction of the replay-memory that\n",
    "        # has high estimation errors - or it is set to 0.5. So at least\n",
    "        # half of the batch has high estimation errors.\n",
    "        prob_err_hi = len(self.idx_err_hi) / self.num_used\n",
    "        prob_err_hi = max(prob_err_hi, 0.5)\n",
    "\n",
    "        # Number of samples in a batch that have high estimation errors.\n",
    "        self.num_samples_err_hi = int(prob_err_hi * batch_size)\n",
    "\n",
    "        # Number of samples in a batch that have low estimation errors.\n",
    "        self.num_samples_err_lo = batch_size - self.num_samples_err_hi\n",
    "\n",
    "    def random_batch(self):\n",
    "        \"\"\"\n",
    "        Get a random batch of states and Q-values from the replay-memory.\n",
    "        You must call prepare_sampling_prob() before calling this function,\n",
    "        which also sets the batch-size.\n",
    "\n",
    "        The batch has been balanced so it contains states and Q-values\n",
    "        that have both high and low estimation errors for the Q-values.\n",
    "        This is done to both speed up and stabilize training of the\n",
    "        Neural Network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Random index of states and Q-values in the replay-memory.\n",
    "        # These have LOW estimation errors for the Q-values.\n",
    "        idx_lo = np.random.choice(self.idx_err_lo,\n",
    "                                  size=self.num_samples_err_lo,\n",
    "                                  replace=False)\n",
    "\n",
    "        # Random index of states and Q-values in the replay-memory.\n",
    "        # These have HIGH estimation errors for the Q-values.\n",
    "        idx_hi = np.random.choice(self.idx_err_hi,\n",
    "                                  size=self.num_samples_err_hi,\n",
    "                                  replace=False)\n",
    "\n",
    "        # Combine the indices.\n",
    "        idx = np.concatenate((idx_lo, idx_hi))\n",
    "\n",
    "        # Get the batches of states and Q-values.\n",
    "        states_batch = self.states[idx]\n",
    "        q_values_batch = self.q_values[idx]\n",
    "\n",
    "        return states_batch, q_values_batch\n",
    "    \n",
    "    \n",
    "    # TODO : implement simpler version of this random sampling error\n",
    "\n",
    "    def all_batches(self, batch_size=128):\n",
    "        \"\"\"\n",
    "        Iterator for all the states and Q-values in the replay-memory.\n",
    "        It returns the indices for the beginning and end, as well as\n",
    "        a progress-counter between 0.0 and 1.0.\n",
    "        \n",
    "        This function is not currently being used except by the function\n",
    "        estimate_all_q_values() below. These two functions are merely\n",
    "        included to make it easier for you to experiment with the code\n",
    "        by showing you an easy and efficient way to loop over all the\n",
    "        data in the replay-memory.\n",
    "        \"\"\"\n",
    "\n",
    "        # Start index for the current batch.\n",
    "        begin = 0\n",
    "\n",
    "        # Repeat until all batches have been processed.\n",
    "        while begin < self.num_used:\n",
    "            # End index for the current batch.\n",
    "            end = begin + batch_size\n",
    "\n",
    "            # Ensure the batch does not exceed the used replay-memory.\n",
    "            if end > self.num_used:\n",
    "                end = self.num_used\n",
    "\n",
    "            # Progress counter.\n",
    "            progress = end / self.num_used\n",
    "\n",
    "            # Yield the batch indices and completion-counter.\n",
    "            yield begin, end, progress\n",
    "\n",
    "            # Set the start-index for the next batch to the end of this batch.\n",
    "            begin = end\n",
    "\n",
    "    def estimate_all_q_values(self, model):\n",
    "        \"\"\"\n",
    "        Estimate all Q-values for the states in the replay-memory\n",
    "        using the model / Neural Network.\n",
    "\n",
    "        Note that this function is not currently being used. It is provided\n",
    "        to make it easier for you to experiment with this code, by showing\n",
    "        you an efficient way to iterate over all the states and Q-values.\n",
    "\n",
    "        :param model:\n",
    "            Instance of the NeuralNetwork-class.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Re-calculating all Q-values in replay memory ...\")\n",
    "\n",
    "        # Process the entire replay-memory in batches.\n",
    "        for begin, end, progress in self.all_batches():\n",
    "            # Print progress.\n",
    "            msg = \"\\tProgress: {0:.0%}\"\n",
    "            msg = msg.format(progress)\n",
    "            print_progress(msg)\n",
    "\n",
    "            # Get the states for the current batch.\n",
    "            states = self.states[begin:end]\n",
    "\n",
    "            # Calculate the Q-values using the Neural Network\n",
    "            # and update the replay-memory.\n",
    "            self.q_values[begin:end] = model.get_q_values(states=states)\n",
    "\n",
    "        # Newline.\n",
    "        print()\n",
    "\n",
    "    def print_statistics(self):\n",
    "        \"\"\"Print statistics for the contents of the replay-memory.\"\"\"\n",
    "\n",
    "        print(\"Replay-memory statistics:\")\n",
    "\n",
    "        # Print statistics for the Q-values before they were updated\n",
    "        # in update_all_q_values().\n",
    "        msg = \"\\tQ-values Before, Min: {0:5.2f}, Mean: {1:5.2f}, Max: {2:5.2f}\"\n",
    "        print(msg.format(np.min(self.q_values_old),\n",
    "                         np.mean(self.q_values_old),\n",
    "                         np.max(self.q_values_old)))\n",
    "\n",
    "        # Print statistics for the Q-values after they were updated\n",
    "        # in update_all_q_values().\n",
    "        msg = \"\\tQ-values After,  Min: {0:5.2f}, Mean: {1:5.2f}, Max: {2:5.2f}\"\n",
    "        print(msg.format(np.min(self.q_values),\n",
    "                         np.mean(self.q_values),\n",
    "                         np.max(self.q_values)))\n",
    "\n",
    "        # Print statistics for the difference in Q-values before and\n",
    "        # after the update in update_all_q_values().\n",
    "        q_dif = self.q_values - self.q_values_old\n",
    "        msg = \"\\tQ-values Diff.,  Min: {0:5.2f}, Mean: {1:5.2f}, Max: {2:5.2f}\"\n",
    "        print(msg.format(np.min(q_dif),\n",
    "                         np.mean(q_dif),\n",
    "                         np.max(q_dif)))\n",
    "\n",
    "        # Print statistics for the number of large estimation errors.\n",
    "        # Don't use the estimation error for the last state in the memory,\n",
    "        # because its Q-values have not been updated.\n",
    "        err = self.estimation_errors[:-1]\n",
    "        err_count = np.count_nonzero(err > self.error_threshold)\n",
    "        msg = \"\\tNumber of large errors > {0}: {1} / {2} ({3:.1%})\"\n",
    "        print(msg.format(self.error_threshold, err_count,\n",
    "                         self.num_used, err_count / self.num_used))\n",
    "\n",
    "        # How much of the replay-memory is used by states with end_life.\n",
    "        end_life_pct = np.count_nonzero(self.end_life) / self.num_used\n",
    "\n",
    "        # How much of the replay-memory is used by states with end_episode.\n",
    "        end_episode_pct = np.count_nonzero(self.end_episode) / self.num_used\n",
    "\n",
    "        # How much of the replay-memory is used by states with non-zero reward.\n",
    "        reward_nonzero_pct = np.count_nonzero(self.rewards) / self.num_used\n",
    "\n",
    "        # Print those statistics.\n",
    "        msg = \"\\tend_life: {0:.1%}, end_episode: {1:.1%}, reward non-zero: {2:.1%}\"\n",
    "        print(msg.format(end_life_pct, end_episode_pct, reward_nonzero_pct))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Linear controller\n",
    "\n",
    "class LinearControlSignal:\n",
    "    \"\"\"\n",
    "    A control signal that changes linearly over time.\n",
    "\n",
    "    This is used to change e.g. the learning-rate for the optimizer\n",
    "    of the Neural Network, as well as other parameters.\n",
    "    \n",
    "    TensorFlow has functionality for doing this, but it uses the\n",
    "    global_step counter inside the TensorFlow graph, while we\n",
    "    want the control signals to use a state-counter for the\n",
    "    game-environment. So it is easier to make this in Python.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, start_value, end_value, num_iterations, repeat=False):\n",
    "        \"\"\"\n",
    "        Create a new object.\n",
    "\n",
    "        :param start_value:\n",
    "            Start-value for the control signal.\n",
    "\n",
    "        :param end_value:\n",
    "            End-value for the control signal.\n",
    "\n",
    "        :param num_iterations:\n",
    "            Number of iterations it takes to reach the end_value\n",
    "            from the start_value.\n",
    "\n",
    "        :param repeat:\n",
    "            Boolean whether to reset the control signal back to the start_value\n",
    "            after the end_value has been reached.\n",
    "        \"\"\"\n",
    "\n",
    "        # Store arguments in this object.\n",
    "        self.start_value = start_value\n",
    "        self.end_value = end_value\n",
    "        self.num_iterations = num_iterations\n",
    "        self.repeat = repeat\n",
    "\n",
    "        # Calculate the linear coefficient.\n",
    "        self._coefficient = (end_value - start_value) / num_iterations\n",
    "\n",
    "    def get_value(self, iteration):\n",
    "        \"\"\"Get the value of the control signal for the given iteration.\"\"\"\n",
    "\n",
    "        if self.repeat:\n",
    "            iteration %= self.num_iterations\n",
    "\n",
    "        if iteration < self.num_iterations:\n",
    "            value = iteration * self._coefficient + self.start_value\n",
    "        else:\n",
    "            value = self.end_value\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    \"\"\"\n",
    "    The epsilon-greedy policy either takes a random action with\n",
    "    probability epsilon, or it takes the action for the highest\n",
    "    Q-value.\n",
    "    \n",
    "    If epsilon is 1.0 then the actions are always random.\n",
    "    If epsilon is 0.0 then the actions are always argmax for the Q-values.\n",
    "\n",
    "    Epsilon is typically decreased linearly from 1.0 to 0.1\n",
    "    and this is also implemented in this class.\n",
    "\n",
    "    During testing, epsilon is usually chosen lower, e.g. 0.05 or 0.01\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_actions,\n",
    "                 epsilon_testing=0.0,\n",
    "                 num_iterations=1e6,\n",
    "                 start_value=1.0, end_value=0.1,\n",
    "                 repeat=False):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param num_actions:\n",
    "            Number of possible actions in the game-environment.\n",
    "\n",
    "        :param epsilon_testing:\n",
    "            Epsilon-value when testing.\n",
    "\n",
    "        :param num_iterations:\n",
    "            Number of training iterations required to linearly\n",
    "            decrease epsilon from start_value to end_value.\n",
    "            \n",
    "        :param start_value:\n",
    "            Starting value for linearly decreasing epsilon.\n",
    "\n",
    "        :param end_value:\n",
    "            Ending value for linearly decreasing epsilon.\n",
    "\n",
    "        :param repeat:\n",
    "            Boolean whether to repeat and restart the linear decrease\n",
    "            when the end_value is reached, or only do it once and then\n",
    "            output the end_value forever after.\n",
    "        \"\"\"\n",
    "\n",
    "        # Store parameters.\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon_testing = epsilon_testing\n",
    "\n",
    "        # Create a control signal for linearly decreasing epsilon.\n",
    "        self.epsilon_linear = LinearControlSignal(num_iterations=num_iterations,\n",
    "                                                  start_value=start_value,\n",
    "                                                  end_value=end_value,\n",
    "                                                  repeat=repeat)\n",
    "\n",
    "    def get_epsilon(self, iteration, training):\n",
    "        \"\"\"\n",
    "        Return the epsilon for the given iteration.\n",
    "        If training==True then epsilon is linearly decreased,\n",
    "        otherwise epsilon is a fixed number.\n",
    "        \"\"\"\n",
    "\n",
    "        if training:\n",
    "            epsilon = self.epsilon_linear.get_value(iteration=iteration)\n",
    "        else:\n",
    "            epsilon = self.epsilon_testing\n",
    "\n",
    "        return epsilon\n",
    "\n",
    "    def get_action(self, q_values, iteration, training):\n",
    "        \"\"\"\n",
    "        Use the epsilon-greedy policy to select an action.\n",
    "        \n",
    "        :param q_values:\n",
    "            These are the Q-values that are estimated by the Neural Network\n",
    "            for the current state of the game-environment.\n",
    "         \n",
    "        :param iteration:\n",
    "            This is an iteration counter. Here we use the number of states\n",
    "            that has been processed in the game-environment.\n",
    "\n",
    "        :param training:\n",
    "            Boolean whether we are training or testing the\n",
    "            Reinforcement Learning agent.\n",
    "\n",
    "        :return:\n",
    "            action (integer), epsilon (float)\n",
    "        \"\"\"\n",
    "\n",
    "        epsilon = self.get_epsilon(iteration=iteration, training=training)\n",
    "\n",
    "        # With probability epsilon.\n",
    "        if np.random.random() < epsilon:\n",
    "            # Select a random action.\n",
    "            action = np.random.randint(low=0, high=self.num_actions)\n",
    "        else:\n",
    "            # Otherwise select the action that has the highest Q-value.\n",
    "            action = np.argmax(q_values)\n",
    "\n",
    "        return action, epsilon\n",
    "\n",
    "    \n",
    "    # TODO: Implement Epsilon decay\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Creates a Neural Network for Reinforcement Learning (Q-Learning).\n",
    "\n",
    "    Functions are provided for estimating Q-values from states of the\n",
    "    game-environment, and for optimizing the Neural Network so it becomes\n",
    "    better at estimating the Q-values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_actions, replay_memory, use_pretty_tensor=True):\n",
    "        \"\"\"\n",
    "        :param num_actions:\n",
    "            Number of discrete actions for the game-environment.\n",
    "\n",
    "        :param replay_memory: \n",
    "            Object-instance of the ReplayMemory-class.\n",
    "\n",
    "        :param use_pretty_tensor:\n",
    "            Boolean whether to use PrettyTensor (True) which must then be\n",
    "            installed, or use the tf.layers API (False) which is already\n",
    "            built into TensorFlow.\n",
    "        \"\"\"\n",
    "\n",
    "        # Whether to use the PrettyTensor API (True) or tf.layers (False).\n",
    "        self.use_pretty_tensor = use_pretty_tensor\n",
    "\n",
    "        # Replay-memory used for sampling random batches.\n",
    "        self.replay_memory = replay_memory\n",
    "\n",
    "        # Path for saving/restoring checkpoints.\n",
    "        self.checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "\n",
    "        # Placeholder variable for inputting states into the Neural Network.\n",
    "        # A state is a multi-dimensional array holding image-frames from\n",
    "        # the game-environment.\n",
    "        self.x = tf.placeholder(dtype=tf.float32, shape=[None] + state_shape, name='x')\n",
    "\n",
    "        # Placeholder variable for inputting the learning-rate to the optimizer.\n",
    "        self.learning_rate = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "\n",
    "        # Placeholder variable for inputting the target Q-values\n",
    "        # that we want the Neural Network to be able to estimate.\n",
    "        self.q_values_new = tf.placeholder(tf.float32,\n",
    "                                           shape=[None, num_actions],\n",
    "                                           name='q_values_new')\n",
    "\n",
    "        # This is a hack that allows us to save/load the counter for\n",
    "        # the number of states processed in the game-environment.\n",
    "        # We will keep it as a variable in the TensorFlow-graph\n",
    "        # even though it will not actually be used by TensorFlow.\n",
    "        self.count_states = tf.Variable(initial_value=0,\n",
    "                                        trainable=False, dtype=tf.int64,\n",
    "                                        name='count_states')\n",
    "\n",
    "        # Similarly, this is the counter for the number of episodes.\n",
    "        self.count_episodes = tf.Variable(initial_value=0,\n",
    "                                          trainable=False, dtype=tf.int64,\n",
    "                                          name='count_episodes')\n",
    "\n",
    "        # TensorFlow operation for increasing count_states.\n",
    "        self.count_states_increase = tf.assign(self.count_states,\n",
    "                                               self.count_states + 1)\n",
    "\n",
    "        # TensorFlow operation for increasing count_episodes.\n",
    "        self.count_episodes_increase = tf.assign(self.count_episodes,\n",
    "                                                 self.count_episodes + 1)\n",
    "\n",
    "        # The Neural Network will be constructed in the following.\n",
    "        # Note that the architecture of this Neural Network is very\n",
    "        # different from that used in the original DeepMind papers,\n",
    "        # which was something like this:\n",
    "        # Input image:      84 x 84 x 4 (4 gray-scale images of 84 x 84 pixels).\n",
    "        # Conv layer 1:     16 filters 8 x 8, stride 4, relu.\n",
    "        # Conv layer 2:     32 filters 4 x 4, stride 2, relu.\n",
    "        # Fully-conn. 1:    256 units, relu. (Sometimes 512 units).\n",
    "        # Fully-conn. 2:    num-action units, linear.\n",
    "\n",
    "        # The DeepMind architecture does a very aggressive downsampling of\n",
    "        # the input images so they are about 10 x 10 pixels after the final\n",
    "        # convolutional layer. I found that this resulted in significantly\n",
    "        # distorted Q-values when using the training method further below.\n",
    "        # The reason DeepMind could get it working was perhaps that they\n",
    "        # used a very large replay memory (5x as big as here), and a single\n",
    "        # optimization iteration was performed after each step of the game,\n",
    "        # and some more tricks.\n",
    "\n",
    "        # Initializer for the layers in the Neural Network.\n",
    "        # If you change the architecture of the network, particularly\n",
    "        # if you add or remove layers, then you may have to change\n",
    "        # the stddev-parameter here. The initial weights must result\n",
    "        # in the Neural Network outputting Q-values that are very close\n",
    "        # to zero - but the network weights must not be too low either\n",
    "        # because it will make it hard to train the network.\n",
    "        # You can experiment with values between 1e-2 and 1e-3.\n",
    "        init = tf.truncated_normal_initializer(mean=0.0, stddev=2e-2)\n",
    "\n",
    "        if self.use_pretty_tensor:\n",
    "            # This builds the Neural Network using the PrettyTensor API,\n",
    "            # which is a very elegant builder API, but some people are\n",
    "            # having problems installing and using it.\n",
    "\n",
    "            import prettytensor as pt\n",
    "\n",
    "            # Wrap the input to the Neural Network in a PrettyTensor object.\n",
    "            x_pretty = pt.wrap(self.x)\n",
    "\n",
    "            # Create the convolutional Neural Network using Pretty Tensor.\n",
    "            with pt.defaults_scope(activation_fn=tf.nn.relu):\n",
    "                self.q_values = x_pretty. \\\n",
    "                    conv2d(kernel=3, depth=16, stride=2, name='layer_conv1', weights=init). \\\n",
    "                    conv2d(kernel=3, depth=32, stride=2, name='layer_conv2', weights=init). \\\n",
    "                    conv2d(kernel=3, depth=64, stride=1, name='layer_conv3', weights=init). \\\n",
    "                    flatten(). \\\n",
    "                    fully_connected(size=1024, name='layer_fc1', weights=init). \\\n",
    "                    fully_connected(size=1024, name='layer_fc2', weights=init). \\\n",
    "                    fully_connected(size=1024, name='layer_fc3', weights=init). \\\n",
    "                    fully_connected(size=1024, name='layer_fc4', weights=init). \\\n",
    "                    fully_connected(size=num_actions, name='layer_fc_out', weights=init,\n",
    "                                    activation_fn=None)\n",
    "\n",
    "            # Loss-function which must be optimized. This is the mean-squared\n",
    "            # error between the Q-values that are output by the Neural Network\n",
    "            # and the target Q-values.\n",
    "            self.loss = self.q_values.l2_regression(target=self.q_values_new)\n",
    "        else:\n",
    "            # This builds the Neural Network using the tf.layers API,\n",
    "            # which is very verbose and inelegant, but should work for everyone.\n",
    "\n",
    "            # Note that the checkpoints for Tutorial #16 which can be\n",
    "            # downloaded from the internet only support PrettyTensor.\n",
    "            # Although the Neural Networks appear to be identical when\n",
    "            # built using the PrettyTensor and tf.layers APIs,\n",
    "            # they actually create somewhat different TensorFlow graphs\n",
    "            # where the variables have different names, which means the\n",
    "            # checkpoints are incompatible for the two builder APIs.\n",
    "\n",
    "            # Padding used for the convolutional layers.\n",
    "            padding = 'SAME'\n",
    "\n",
    "            # Activation function for all convolutional and fully-connected\n",
    "            # layers, except the last.\n",
    "            activation = tf.nn.relu\n",
    "\n",
    "            # Reference to the lastly added layer of the Neural Network.\n",
    "            # This makes it easy to add or remove layers.\n",
    "            net = self.x\n",
    "\n",
    "            # First convolutional layer.\n",
    "            net = tf.layers.conv2d(inputs=net, name='layer_conv1',\n",
    "                                   filters=16, kernel_size=3, strides=2,\n",
    "                                   padding=padding,\n",
    "                                   kernel_initializer=init, activation=activation)\n",
    "\n",
    "            # Second convolutional layer.\n",
    "            net = tf.layers.conv2d(inputs=net, name='layer_conv2',\n",
    "                                   filters=32, kernel_size=3, strides=2,\n",
    "                                   padding=padding,\n",
    "                                   kernel_initializer=init, activation=activation)\n",
    "\n",
    "            # Third convolutional layer.\n",
    "            net = tf.layers.conv2d(inputs=net, name='layer_conv3',\n",
    "                                   filters=64, kernel_size=3, strides=1,\n",
    "                                   padding=padding,\n",
    "                                   kernel_initializer=init, activation=activation)\n",
    "\n",
    "            # Flatten output of the last convolutional layer so it can\n",
    "            # be input to a fully-connected (aka. dense) layer.\n",
    "            # TODO: For some bizarre reason, this function is not yet in tf.layers\n",
    "            # TODO: net = tf.layers.flatten(net)\n",
    "            net = tf.contrib.layers.flatten(net)\n",
    "\n",
    "            # First fully-connected (aka. dense) layer.\n",
    "            net = tf.layers.dense(inputs=net, name='layer_fc1', units=1024,\n",
    "                                  kernel_initializer=init, activation=activation)\n",
    "\n",
    "            # Second fully-connected layer.\n",
    "            net = tf.layers.dense(inputs=net, name='layer_fc2', units=1024,\n",
    "                                  kernel_initializer=init, activation=activation)\n",
    "\n",
    "            # Third fully-connected layer.\n",
    "            net = tf.layers.dense(inputs=net, name='layer_fc3', units=1024,\n",
    "                                  kernel_initializer=init, activation=activation)\n",
    "\n",
    "            # Fourth fully-connected layer.\n",
    "            net = tf.layers.dense(inputs=net, name='layer_fc4', units=1024,\n",
    "                                  kernel_initializer=init, activation=activation)\n",
    "\n",
    "            # Final fully-connected layer.\n",
    "            net = tf.layers.dense(inputs=net, name='layer_fc_out', units=num_actions,\n",
    "                                  kernel_initializer=init, activation=None)\n",
    "\n",
    "            # The output of the Neural Network is the estimated Q-values\n",
    "            # for each possible action in the game-environment.\n",
    "            self.q_values = net\n",
    "\n",
    "            # TensorFlow has a built-in loss-function for doing regression:\n",
    "            # self.loss = tf.nn.l2_loss(self.q_values - self.q_values_new)\n",
    "            # But it uses tf.reduce_sum() rather than tf.reduce_mean()\n",
    "            # which is used by PrettyTensor. This means the scale of the\n",
    "            # gradient is different and hence the hyper-parameters\n",
    "            # would have to be re-tuned. So instead we calculate the\n",
    "            # L2-loss similarly to how it is done in PrettyTensor.\n",
    "            squared_error = tf.square(self.q_values - self.q_values_new)\n",
    "            sum_squared_error = tf.reduce_sum(squared_error, axis=1)\n",
    "            self.loss = tf.reduce_mean(sum_squared_error)\n",
    "\n",
    "        # Optimizer used for minimizing the loss-function.\n",
    "        # Note the learning-rate is a placeholder variable so we can\n",
    "        # lower the learning-rate as optimization progresses.\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        # Used for saving and loading checkpoints.\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # Create a new TensorFlow session so we can run the Neural Network.\n",
    "        self.session = tf.Session()\n",
    "\n",
    "        # Load the most recent checkpoint if it exists,\n",
    "        # otherwise initialize all the variables in the TensorFlow graph.\n",
    "        self.load_checkpoint()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the TensorFlow session.\"\"\"\n",
    "        self.session.close()\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"\n",
    "        Load all variables of the TensorFlow graph from a checkpoint.\n",
    "        If the checkpoint does not exist, then initialize all variables.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            print(\"Trying to restore last checkpoint ...\")\n",
    "\n",
    "            # Use TensorFlow to find the latest checkpoint - if any.\n",
    "            last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=checkpoint_dir)\n",
    "\n",
    "            # Try and load the data in the checkpoint.\n",
    "            self.saver.restore(self.session, save_path=last_chk_path)\n",
    "\n",
    "            # If we get to this point, the checkpoint was successfully loaded.\n",
    "            print(\"Restored checkpoint from:\", last_chk_path)\n",
    "        except:\n",
    "            # If the above failed for some reason, simply\n",
    "            # initialize all the variables for the TensorFlow graph.\n",
    "            print(\"Failed to restore checkpoint from:\", checkpoint_dir)\n",
    "            print(\"Initializing variables instead.\")\n",
    "            self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def save_checkpoint(self, current_iteration):\n",
    "        \"\"\"Save all variables of the TensorFlow graph to a checkpoint.\"\"\"\n",
    "\n",
    "        self.saver.save(self.session,\n",
    "                        save_path=self.checkpoint_path,\n",
    "                        global_step=current_iteration)\n",
    "\n",
    "        print(\"Saved checkpoint.\")\n",
    "\n",
    "    def get_q_values(self, states):\n",
    "        \"\"\"\n",
    "        Calculate and return the estimated Q-values for the given states.\n",
    "\n",
    "        A single state contains two images (or channels): The most recent\n",
    "        image-frame from the game-environment, and a motion-tracing image.\n",
    "        See the MotionTracer-class for details.\n",
    "\n",
    "        The input to this function is an array of such states which allows\n",
    "        for batch-processing of the states. So the input is a 4-dim\n",
    "        array with shape: [batch, height, width, state_channels].\n",
    "        \n",
    "        The output of this function is an array of Q-value-arrays.\n",
    "        There is a Q-value for each possible action in the game-environment.\n",
    "        So the output is a 2-dim array with shape: [batch, num_actions]\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a feed-dict for inputting the states to the Neural Network.\n",
    "        feed_dict = {self.x: states}\n",
    "\n",
    "        # Use TensorFlow to calculate the estimated Q-values for these states.\n",
    "        values = self.session.run(self.q_values, feed_dict=feed_dict)\n",
    "\n",
    "        return values\n",
    "\n",
    "    def optimize(self, min_epochs=1.0, max_epochs=10,\n",
    "                 batch_size=128, loss_limit=0.015,\n",
    "                 learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Optimize the Neural Network by sampling states and Q-values\n",
    "        from the replay-memory.\n",
    "\n",
    "        The original DeepMind paper performed one optimization iteration\n",
    "        after processing each new state of the game-environment. This is\n",
    "        an un-natural way of doing optimization of Neural Networks.\n",
    "\n",
    "        So instead we perform a full optimization run every time the\n",
    "        Replay Memory is full (or it is filled to the desired fraction).\n",
    "        This also gives more efficient use of a GPU for the optimization.\n",
    "\n",
    "        The problem is that this may over-fit the Neural Network to whatever\n",
    "        is in the replay-memory. So we use several tricks to try and adapt\n",
    "        the number of optimization iterations.\n",
    "\n",
    "        :param min_epochs:\n",
    "            Minimum number of optimization epochs. One epoch corresponds\n",
    "            to the replay-memory being used once. However, as the batches\n",
    "            are sampled randomly and biased somewhat, we may not use the\n",
    "            whole replay-memory. This number is just a convenient measure.\n",
    "\n",
    "        :param max_epochs:\n",
    "            Maximum number of optimization epochs.\n",
    "\n",
    "        :param batch_size:\n",
    "            Size of each random batch sampled from the replay-memory.\n",
    "\n",
    "        :param loss_limit:\n",
    "            Optimization continues until the average loss-value of the\n",
    "            last 100 batches is below this value (or max_epochs is reached).\n",
    "\n",
    "        :param learning_rate:\n",
    "            Learning-rate to use for the optimizer.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Optimizing Neural Network to better estimate Q-values ...\")\n",
    "        print(\"\\tLearning-rate: {0:.1e}\".format(learning_rate))\n",
    "        print(\"\\tLoss-limit: {0:.3f}\".format(loss_limit))\n",
    "        print(\"\\tMax epochs: {0:.1f}\".format(max_epochs))\n",
    "\n",
    "        # Prepare the probability distribution for sampling the replay-memory.\n",
    "        self.replay_memory.prepare_sampling_prob(batch_size=batch_size)\n",
    "\n",
    "        # Number of optimization iterations corresponding to one epoch.\n",
    "        iterations_per_epoch = self.replay_memory.num_used / batch_size\n",
    "\n",
    "        # Minimum number of iterations to perform.\n",
    "        min_iterations = int(iterations_per_epoch * min_epochs)\n",
    "\n",
    "        # Maximum number of iterations to perform.\n",
    "        max_iterations = int(iterations_per_epoch * max_epochs)\n",
    "\n",
    "        # Buffer for storing the loss-values of the most recent batches.\n",
    "        loss_history = np.zeros(100, dtype=float)\n",
    "\n",
    "        for i in range(max_iterations):\n",
    "            # Randomly sample a batch of states and target Q-values\n",
    "            # from the replay-memory. These are the Q-values that we\n",
    "            # want the Neural Network to be able to estimate.\n",
    "            state_batch, q_values_batch = self.replay_memory.random_batch()\n",
    "\n",
    "            # Create a feed-dict for inputting the data to the TensorFlow graph.\n",
    "            # Note that the learning-rate is also in this feed-dict.\n",
    "            feed_dict = {self.x: state_batch,\n",
    "                         self.q_values_new: q_values_batch,\n",
    "                         self.learning_rate: learning_rate}\n",
    "\n",
    "            # Perform one optimization step and get the loss-value.\n",
    "            loss_val, _ = self.session.run([self.loss, self.optimizer],\n",
    "                                           feed_dict=feed_dict)\n",
    "\n",
    "            # Shift the loss-history and assign the new value.\n",
    "            # This causes the loss-history to only hold the most recent values.\n",
    "            loss_history = np.roll(loss_history, 1)\n",
    "            loss_history[0] = loss_val\n",
    "\n",
    "            # Calculate the average loss for the previous batches.\n",
    "            loss_mean = np.mean(loss_history)\n",
    "\n",
    "            # Print status.\n",
    "            pct_epoch = i / iterations_per_epoch\n",
    "            msg = \"\\tIteration: {0} ({1:.2f} epoch), Batch loss: {2:.4f}, Mean loss: {3:.4f}\"\n",
    "            msg = msg.format(i, pct_epoch, loss_val, loss_mean)\n",
    "            print_progress(msg)\n",
    "\n",
    "            # Stop the optimization if we have performed the required number\n",
    "            # of iterations and the loss-value is sufficiently low.\n",
    "            if i > min_iterations and loss_mean < loss_limit:\n",
    "                break\n",
    "\n",
    "        # Print newline.\n",
    "        print()\n",
    "\n",
    "    def get_weights_variable(self, layer_name):\n",
    "        \"\"\"\n",
    "        Return the variable inside the TensorFlow graph for the weights\n",
    "        in the layer with the given name.\n",
    "\n",
    "        Note that the actual values of the variables are not returned,\n",
    "        you must use the function get_variable_value() for that.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.use_pretty_tensor:\n",
    "            # PrettyTensor uses this name for the weights in a conv-layer.\n",
    "            variable_name = 'weights'\n",
    "        else:\n",
    "            # The tf.layers API uses this name for the weights in a conv-layer.\n",
    "            variable_name = 'kernel'\n",
    "\n",
    "        with tf.variable_scope(layer_name, reuse=True):\n",
    "            variable = tf.get_variable(variable_name)\n",
    "\n",
    "        return variable\n",
    "\n",
    "    def get_variable_value(self, variable):\n",
    "        \"\"\"Return the value of a variable inside the TensorFlow graph.\"\"\"\n",
    "\n",
    "        weights = self.session.run(variable)\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def get_layer_tensor(self, layer_name):\n",
    "        \"\"\"\n",
    "        Return the tensor for the output of a layer.\n",
    "        Note that this does not return the actual values,\n",
    "        but instead returns a reference to the tensor\n",
    "        inside the TensorFlow graph. Use get_tensor_value()\n",
    "        to get the actual contents of the tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # The name of the last operation of a layer,\n",
    "        # assuming it uses Relu as the activation-function.\n",
    "        tensor_name = layer_name + \"/Relu:0\"\n",
    "\n",
    "        # Get the tensor with this name.\n",
    "        tensor = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def get_tensor_value(self, tensor, state):\n",
    "        \"\"\"Get the value of a tensor in the Neural Network.\"\"\"\n",
    "\n",
    "        # Create a feed-dict for inputting the state to the Neural Network.\n",
    "        feed_dict = {self.x: [state]}\n",
    "\n",
    "        # Run the TensorFlow session to calculate the value of the tensor.\n",
    "        output = self.session.run(tensor, feed_dict=feed_dict)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_count_states(self):\n",
    "        \"\"\"\n",
    "        Get the number of states that has been processed in the game-environment.\n",
    "        This is not used by the TensorFlow graph. It is just a hack to save and\n",
    "        reload the counter along with the checkpoint-file.\n",
    "        \"\"\"\n",
    "        return self.session.run(self.count_states)\n",
    "\n",
    "    def get_count_episodes(self):\n",
    "        \"\"\"\n",
    "        Get the number of episodes that has been processed in the game-environment.\n",
    "        \"\"\"\n",
    "        return self.session.run(self.count_episodes)\n",
    "\n",
    "    def increase_count_states(self):\n",
    "        \"\"\"\n",
    "        Increase the number of states that has been processed\n",
    "        in the game-environment.\n",
    "        \"\"\"\n",
    "        return self.session.run(self.count_states_increase)\n",
    "\n",
    "    def increase_count_episodes(self):\n",
    "        \"\"\"\n",
    "        Increase the number of episodes that has been processed\n",
    "        in the game-environment.\n",
    "        \"\"\"\n",
    "        return self.session.run(self.count_episodes_increase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    This implements the function for running the game-environment with\n",
    "    an agent that uses Reinforcement Learning. This class also creates\n",
    "    instances of the Replay Memory and Neural Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_name, training, render=False, use_logging=True):\n",
    "        \"\"\"\n",
    "        Create an object-instance. This also creates a new object for the\n",
    "        Replay Memory and the Neural Network.\n",
    "        \n",
    "        Replay Memory will only be allocated if training==True.\n",
    "\n",
    "        :param env_name:\n",
    "            Name of the game-environment in OpenAI Gym.\n",
    "            Examples: 'Breakout-v0' and 'SpaceInvaders-v0'\n",
    "\n",
    "        :param training:\n",
    "            Boolean whether to train the agent and Neural Network (True),\n",
    "            or test the agent by playing a number of episodes of the game (False).\n",
    "        \n",
    "        :param render:\n",
    "            Boolean whether to render the game-images to screen during testing.\n",
    "\n",
    "        :param use_logging:\n",
    "            Boolean whether to use logging to text-files during training.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create the game-environment using OpenAI Gym.\n",
    "        self.env = gym.make(env_name)\n",
    "\n",
    "        # The number of possible actions that the agent may take in every step.\n",
    "        self.num_actions = self.env.action_space.n\n",
    "\n",
    "        # Whether we are training (True) or testing (False).\n",
    "        self.training = training\n",
    "\n",
    "        # Whether to render each image-frame of the game-environment to screen.\n",
    "        self.render = render\n",
    "\n",
    "        # Whether to use logging during training.\n",
    "        self.use_logging = use_logging\n",
    "\n",
    "        if self.use_logging and self.training:\n",
    "            # Used for logging Q-values and rewards during training.\n",
    "            self.log_q_values = LogQValues()\n",
    "            self.log_reward = LogReward()\n",
    "        else:\n",
    "            self.log_q_values = None\n",
    "            self.log_reward = None\n",
    "\n",
    "        # List of string-names for the actions in the game-environment.\n",
    "        self.action_names = self.env.unwrapped.get_action_meanings()\n",
    "\n",
    "        # Epsilon-greedy policy for selecting an action from the Q-values.\n",
    "        # During training the epsilon is decreased linearly over the given\n",
    "        # number of iterations. During testing the fixed epsilon is used.\n",
    "        self.epsilon_greedy = EpsilonGreedy(start_value=1.0,\n",
    "                                            end_value=0.1,\n",
    "                                            num_iterations=1e6,\n",
    "                                            num_actions=self.num_actions,\n",
    "                                            epsilon_testing=0.01)\n",
    "\n",
    "        if self.training:\n",
    "            # The following control-signals are only used during training.\n",
    "\n",
    "            # The learning-rate for the optimizer decreases linearly.\n",
    "            self.learning_rate_control = LinearControlSignal(start_value=1e-3,\n",
    "                                                             end_value=1e-5,\n",
    "                                                             num_iterations=5e6)\n",
    "\n",
    "            # The loss-limit is used to abort the optimization whenever the\n",
    "            # mean batch-loss falls below this limit.\n",
    "            self.loss_limit_control = LinearControlSignal(start_value=0.1,\n",
    "                                                          end_value=0.015,\n",
    "                                                          num_iterations=5e6)\n",
    "\n",
    "            # The maximum number of epochs to perform during optimization.\n",
    "            # This is increased from 5 to 10 epochs, because it was found for\n",
    "            # the Breakout-game that too many epochs could be harmful early\n",
    "            # in the training, as it might cause over-fitting.\n",
    "            # Later in the training we would occasionally get rare events\n",
    "            # and would therefore have to optimize for more iterations\n",
    "            # because the learning-rate had been decreased.\n",
    "            self.max_epochs_control = LinearControlSignal(start_value=5.0,\n",
    "                                                          end_value=10.0,\n",
    "                                                          num_iterations=5e6)\n",
    "\n",
    "            # The fraction of the replay-memory to be used.\n",
    "            # Early in the training, we want to optimize more frequently\n",
    "            # so the Neural Network is trained faster and the Q-values\n",
    "            # are learned and updated more often. Later in the training,\n",
    "            # we need more samples in the replay-memory to have sufficient\n",
    "            # diversity, otherwise the Neural Network will over-fit.\n",
    "            self.replay_fraction = LinearControlSignal(start_value=0.1,\n",
    "                                                       end_value=1.0,\n",
    "                                                       num_iterations=5e6)\n",
    "        else:\n",
    "            # We set these objects to None when they will not be used.\n",
    "            self.learning_rate_control = None\n",
    "            self.loss_limit_control = None\n",
    "            self.max_epochs_control = None\n",
    "            self.replay_fraction = None\n",
    "\n",
    "        if self.training:\n",
    "            # We only create the replay-memory when we are training the agent,\n",
    "            # because it requires a lot of RAM. The image-frames from the\n",
    "            # game-environment are resized to 105 x 80 pixels gray-scale,\n",
    "            # and each state has 2 channels (one for the recent image-frame\n",
    "            # of the game-environment, and one for the motion-trace).\n",
    "            # Each pixel is 1 byte, so this replay-memory needs more than\n",
    "            # 3 GB RAM (105 x 80 x 2 x 200000 bytes).\n",
    "\n",
    "            self.replay_memory = ReplayMemory(size=200000,\n",
    "                                              num_actions=self.num_actions)\n",
    "        else:\n",
    "            self.replay_memory = None\n",
    "\n",
    "        # Create the Neural Network used for estimating Q-values.\n",
    "        self.model = NeuralNetwork(num_actions=self.num_actions,\n",
    "                                   replay_memory=self.replay_memory)\n",
    "\n",
    "        # Log of the rewards obtained in each episode during calls to run()\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def reset_episode_rewards(self):\n",
    "        \"\"\"Reset the log of episode-rewards.\"\"\"\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def get_action_name(self, action):\n",
    "        \"\"\"Return the name of an action.\"\"\"\n",
    "        return self.action_names[action]\n",
    "\n",
    "    def get_lives(self):\n",
    "        \"\"\"Get the number of lives the agent has in the game-environment.\"\"\"\n",
    "        return self.env.unwrapped.ale.lives()\n",
    "\n",
    "    def run(self, num_episodes=None):\n",
    "        \"\"\"\n",
    "        Run the game-environment and use the Neural Network to decide\n",
    "        which actions to take in each step through Q-value estimates.\n",
    "        \n",
    "        :param num_episodes: \n",
    "            Number of episodes to process in the game-environment.\n",
    "            If None then continue forever. This is useful during training\n",
    "            where you might want to stop the training using Ctrl-C instead.\n",
    "        \"\"\"\n",
    "\n",
    "        # This will cause a reset in the first iteration of the following loop.\n",
    "        end_episode = True\n",
    "\n",
    "        # Counter for the number of states we have processed.\n",
    "        # This is stored in the TensorFlow graph so it can be\n",
    "        # saved and reloaded along with the checkpoint.\n",
    "        count_states = self.model.get_count_states()\n",
    "\n",
    "        # Counter for the number of episodes we have processed.\n",
    "        count_episodes = self.model.get_count_episodes()\n",
    "\n",
    "        if num_episodes is None:\n",
    "            # Loop forever by comparing the episode-counter to infinity.\n",
    "            num_episodes = float('inf')\n",
    "        else:\n",
    "            # The episode-counter may not start at zero if training is\n",
    "            # continued from a checkpoint. Take this into account\n",
    "            # when determining the number of iterations to perform.\n",
    "            num_episodes += count_episodes\n",
    "\n",
    "        while count_episodes <= num_episodes:\n",
    "            if end_episode:\n",
    "                # Reset the game-environment and get the first image-frame.\n",
    "                img = self.env.reset()\n",
    "\n",
    "                # Create a new motion-tracer for processing images from the\n",
    "                # game-environment. Initialize with the first image-frame.\n",
    "                # This resets the motion-tracer so the trace starts again.\n",
    "                # This could also be done if end_life==True.\n",
    "                motion_tracer = MotionTracer(img)\n",
    "\n",
    "                # Reset the reward for the entire episode to zero.\n",
    "                # This is only used for printing statistics.\n",
    "                reward_episode = 0.0\n",
    "\n",
    "                # Increase the counter for the number of episodes.\n",
    "                # This counter is stored inside the TensorFlow graph\n",
    "                # so it can be saved and restored with the checkpoint.\n",
    "                count_episodes = self.model.increase_count_episodes()\n",
    "\n",
    "                # Get the number of lives that the agent has left in this episode.\n",
    "                num_lives = self.get_lives()\n",
    "\n",
    "            # Get the state of the game-environment from the motion-tracer.\n",
    "            # The state has two images: (1) The last image-frame from the game\n",
    "            # and (2) a motion-trace that shows movement trajectories.\n",
    "            state = motion_tracer.get_state()\n",
    "\n",
    "            # Use the Neural Network to estimate the Q-values for the state.\n",
    "            # Note that the function assumes an array of states and returns\n",
    "            # a 2-dim array of Q-values, but we just have a single state here.\n",
    "            q_values = self.model.get_q_values(states=[state])[0]\n",
    "\n",
    "            # Determine the action that the agent must take in the game-environment.\n",
    "            # The epsilon is just used for printing further below.\n",
    "            action, epsilon = self.epsilon_greedy.get_action(q_values=q_values,\n",
    "                                                             iteration=count_states,\n",
    "                                                             training=self.training)\n",
    "\n",
    "            # Take a step in the game-environment using the given action.\n",
    "            # Note that in OpenAI Gym, the step-function actually repeats the\n",
    "            # action between 2 and 4 time-steps for Atari games, with the number\n",
    "            # chosen at random.\n",
    "            img, reward, end_episode, info = self.env.step(action=action)\n",
    "\n",
    "            # Process the image from the game-environment in the motion-tracer.\n",
    "            # This will first be used in the next iteration of the loop.\n",
    "            motion_tracer.process(image=img)\n",
    "\n",
    "            # Add the reward for the step to the reward for the entire episode.\n",
    "            reward_episode += reward\n",
    "\n",
    "            # Determine if a life was lost in this step.\n",
    "            num_lives_new = self.get_lives()\n",
    "            end_life = (num_lives_new < num_lives)\n",
    "            num_lives = num_lives_new\n",
    "\n",
    "            # Increase the counter for the number of states that have been processed.\n",
    "            count_states = self.model.increase_count_states()\n",
    "\n",
    "            if not self.training and self.render:\n",
    "                # Render the game-environment to screen.\n",
    "                self.env.render()\n",
    "\n",
    "                # Insert a small pause to slow down the game,\n",
    "                # making it easier to follow for human eyes.\n",
    "                time.sleep(0.01)\n",
    "\n",
    "            # If we want to train the Neural Network to better estimate Q-values.\n",
    "            if self.training:\n",
    "                # Add the state of the game-environment to the replay-memory.\n",
    "                self.replay_memory.add(state=state,\n",
    "                                       q_values=q_values,\n",
    "                                       action=action,\n",
    "                                       reward=reward,\n",
    "                                       end_life=end_life,\n",
    "                                       end_episode=end_episode)\n",
    "\n",
    "                # How much of the replay-memory should be used.\n",
    "                use_fraction = self.replay_fraction.get_value(iteration=count_states)\n",
    "\n",
    "                # When the replay-memory is sufficiently full.\n",
    "                if self.replay_memory.is_full() \\\n",
    "                    or self.replay_memory.used_fraction() > use_fraction:\n",
    "\n",
    "                    # Update all Q-values in the replay-memory through a backwards-sweep.\n",
    "                    self.replay_memory.update_all_q_values()\n",
    "\n",
    "                    # Log statistics for the Q-values to file.\n",
    "                    if self.use_logging:\n",
    "                        self.log_q_values.write(count_episodes=count_episodes,\n",
    "                                                count_states=count_states,\n",
    "                                                q_values=self.replay_memory.q_values)\n",
    "\n",
    "                    # Get the control parameters for optimization of the Neural Network.\n",
    "                    # These are changed linearly depending on the state-counter.\n",
    "                    learning_rate = self.learning_rate_control.get_value(iteration=count_states)\n",
    "                    loss_limit = self.loss_limit_control.get_value(iteration=count_states)\n",
    "                    max_epochs = self.max_epochs_control.get_value(iteration=count_states)\n",
    "\n",
    "                    # Perform an optimization run on the Neural Network so as to\n",
    "                    # improve the estimates for the Q-values.\n",
    "                    # This will sample random batches from the replay-memory.\n",
    "                    self.model.optimize(learning_rate=learning_rate,\n",
    "                                        loss_limit=loss_limit,\n",
    "                                        max_epochs=max_epochs)\n",
    "\n",
    "                    # Save a checkpoint of the Neural Network so we can reload it.\n",
    "                    self.model.save_checkpoint(count_states)\n",
    "\n",
    "                    # Reset the replay-memory. This throws away all the data we have\n",
    "                    # just gathered, so we will have to fill the replay-memory again.\n",
    "                    self.replay_memory.reset()\n",
    "\n",
    "            if end_episode:\n",
    "                # Add the episode's reward to a list for calculating statistics.\n",
    "                self.episode_rewards.append(reward_episode)\n",
    "\n",
    "            # Mean reward of the last 30 episodes.\n",
    "            if len(self.episode_rewards) == 0:\n",
    "                # The list of rewards is empty.\n",
    "                reward_mean = 0.0\n",
    "            else:\n",
    "                reward_mean = np.mean(self.episode_rewards[-30:])\n",
    "\n",
    "            if self.training and end_episode:\n",
    "                # Log reward to file.\n",
    "                if self.use_logging:\n",
    "                    self.log_reward.write(count_episodes=count_episodes,\n",
    "                                          count_states=count_states,\n",
    "                                          reward_episode=reward_episode,\n",
    "                                          reward_mean=reward_mean)\n",
    "\n",
    "                # Print reward to screen.\n",
    "                msg = \"{0:4}:{1}\\t Epsilon: {2:4.2f}\\t Reward: {3:.1f}\\t Episode Mean: {4:.1f}\"\n",
    "                print(msg.format(count_episodes, count_states, epsilon,\n",
    "                                 reward_episode, reward_mean))\n",
    "            elif not self.training and (reward != 0.0 or end_life or end_episode):\n",
    "                # Print Q-values and reward to screen.\n",
    "                msg = \"{0:4}:{1}\\tQ-min: {2:5.3f}\\tQ-max: {3:5.3f}\\tLives: {4}\\tReward: {5:.1f}\\tEpisode Mean: {6:.1f}\"\n",
    "                print(msg.format(count_episodes, count_states, np.min(q_values),\n",
    "                                 np.max(q_values), num_lives, reward_episode, reward_mean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lenovo/Desktop/jitin/DQNExperiment_JKKP/Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "env_name = 'Breakout-v0'\n",
    "\n",
    "update_paths(env_name=env_name)\n",
    "print(checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-18 21:48:23,327] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore last checkpoint ...\n",
      "Failed to restore checkpoint from: /home/lenovo/Desktop/jitin/DQNExperiment_JKKP/Breakout-v0\n",
      "Initializing variables instead.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[34560,1024]\n\t [[Node: layer_fc1_1/weights/RMSProp_1/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@layer_fc1_1/weights\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](layer_fc1_1/weights/RMSProp_1, layer_fc1_1/weights/RMSProp_1/Initializer/zeros)]]\n\nCaused by op 'layer_fc1_1/weights/RMSProp_1/Assign', defined at:\n  File \"/home/lenovo/anaconda3/envs/py35/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/lenovo/anaconda3/envs/py35/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-47-69ff81790fbd>\", line 4, in <module>\n    use_logging=False)\n  File \"<ipython-input-32-0e334858178d>\", line 122, in __init__\n    replay_memory=self.replay_memory)\n  File \"<ipython-input-31-9a7268ecc7c9>\", line 201, in __init__\n    self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 353, in minimize\n    name=name)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 474, in apply_gradients\n    self._create_slots([_get_variable_for(v) for v in var_list])\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/training/rmsprop.py\", line 111, in _create_slots\n    self._zeros_slot(v, \"momentum\", self._name)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 796, in _zeros_slot\n    named_slots[_var_key(var)] = slot_creator.create_zeros_slot(var, op_name)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/training/slot_creator.py\", line 174, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/training/slot_creator.py\", line 148, in create_slot_with_initializer\n    dtype)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/training/slot_creator.py\", line 67, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 1203, in get_variable\n    constraint=constraint)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 1092, in get_variable\n    constraint=constraint)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 425, in get_variable\n    constraint=constraint)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 394, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 805, in _get_single_variable\n    constraint=constraint)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 346, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/ops/state_ops.py\", line 276, in assign\n    validate_shape=validate_shape)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 57, in assign\n    use_locking=use_locking, name=name)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[34560,1024]\n\t [[Node: layer_fc1_1/weights/RMSProp_1/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@layer_fc1_1/weights\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](layer_fc1_1/weights/RMSProp_1, layer_fc1_1/weights/RMSProp_1/Initializer/zeros)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9a7268ecc7c9>\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;31m# Try and load the data in the checkpoint.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlast_chk_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1662\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1663\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't load save_path when it is None.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[34560,1024]\n\t [[Node: layer_fc1_1/weights/RMSProp_1/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@layer_fc1_1/weights\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](layer_fc1_1/weights/RMSProp_1, layer_fc1_1/weights/RMSProp_1/Initializer/zeros)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-69ff81790fbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                  \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                  \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                  use_logging=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-0e334858178d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_name, training, render, use_logging)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# Create the Neural Network used for estimating Q-values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         self.model = NeuralNetwork(num_actions=self.num_actions,\n\u001b[0;32m--> 122\u001b[0;31m                                    replay_memory=self.replay_memory)\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Log of the rewards obtained in each episode during calls to run()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-9a7268ecc7c9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_actions, replay_memory, use_pretty_tensor)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;31m# Load the most recent checkpoint if it exists,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# otherwise initialize all the variables in the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-9a7268ecc7c9>\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to restore checkpoint from:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializing variables instead.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[34560,1024]\n\t [[Node: layer_fc1_1/weights/RMSProp_1/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@layer_fc1_1/weights\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](layer_fc1_1/weights/RMSProp_1, layer_fc1_1/weights/RMSProp_1/Initializer/zeros)]]\n\nCaused by op 'layer_fc1_1/weights/RMSProp_1/Assign', defined at:\n  File \"/home/lenovo/anaconda3/envs/py35/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/lenovo/anaconda3/envs/py35/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-47-69ff81790fbd>\", line 4, in <module>\n    use_logging=False)\n  File \"<ipython-input-32-0e334858178d>\", line 122, in __init__\n    replay_memory=self.replay_memory)\n  File \"<ipython-input-31-9a7268ecc7c9>\", line 201, in __init__\n    self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 353, in minimize\n    name=name)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 474, in apply_gradients\n    self._create_slots([_get_variable_for(v) for v in var_list])\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/training/rmsprop.py\", line 111, in _create_slots\n    self._zeros_slot(v, \"momentum\", self._name)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 796, in _zeros_slot\n    named_slots[_var_key(var)] = slot_creator.create_zeros_slot(var, op_name)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/training/slot_creator.py\", line 174, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/training/slot_creator.py\", line 148, in create_slot_with_initializer\n    dtype)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/training/slot_creator.py\", line 67, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 1203, in get_variable\n    constraint=constraint)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 1092, in get_variable\n    constraint=constraint)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 425, in get_variable\n    constraint=constraint)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 394, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 805, in _get_single_variable\n    constraint=constraint)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 346, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/ops/state_ops.py\", line 276, in assign\n    validate_shape=validate_shape)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 57, in assign\n    use_locking=use_locking, name=name)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/lenovo/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[34560,1024]\n\t [[Node: layer_fc1_1/weights/RMSProp_1/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@layer_fc1_1/weights\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](layer_fc1_1/weights/RMSProp_1, layer_fc1_1/weights/RMSProp_1/Initializer/zeros)]]\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env_name=env_name,\n",
    "                 training=True,\n",
    "                 render=True,\n",
    "                 use_logging=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = agent.model\n",
    "replay_memory = agent.replay_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/.local/lib/python3.5/site-packages/ipykernel_launcher.py:41: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1:181\t Epsilon: 1.00\t Reward: 0.0\t Episode Mean: 0.0\n"
     ]
    }
   ],
   "source": [
    "agent.run(num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_q_values = LogQValues()\n",
    "log_reward = LogReward()\n",
    "\n",
    "log_q_values.read()\n",
    "log_reward.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log_reward.count_states, log_reward.episode, label='Episode Reward')\n",
    "plt.plot(log_reward.count_states, log_reward.mean, label='Mean of 30 episodes')\n",
    "plt.xlabel('State-Count for Game Environment')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
