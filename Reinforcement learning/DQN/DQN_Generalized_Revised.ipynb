{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inkpathak\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# getting in all the imports\n",
    "\n",
    "# Standard libraries for util support\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from collections import namedtuple\n",
    "\n",
    "# libraris for processing and rendering \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Libraries for games and deep learning\n",
    "import gym\n",
    "import keras\n",
    "# import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util functions\n",
    "def time_it(some_function):\n",
    "\n",
    "    \"\"\"\n",
    "    Outputs the time a function takes\n",
    "    to execute.\n",
    "    \"\"\"\n",
    "\n",
    "    def wrapper():\n",
    "        t1 = time.time()\n",
    "        some_function()\n",
    "        t2 = time.time()\n",
    "        return \"Time it took to run the function: \" + str((t2 - t1)) + \"\\n\"\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterators\n",
    "\n",
    "# Eplsilon Greedy Iterator\n",
    "def select_action(possible_actions, iteration, num_actions, num_episode, start_value = 1, end_value = 0.1, \n",
    "                        num_iterations = 5e6, decay = 0.1,repeat = False):\n",
    "    \n",
    "    coeff = ((end_value - start_value) / num_iterations)\n",
    "#     weight = np.exp((-decay*iterarion)/num_iterations)\n",
    "    weight = 1\n",
    "    episode_decay = (1/(num_episode+1))\n",
    "    \n",
    "    if repeat:\n",
    "        iteration %= num_iterations\n",
    "        \n",
    "    if iteration < num_iterations:\n",
    "        epsilon = (iteration * coeff + start_value) * weight * episode_decay\n",
    "    else:\n",
    "        epsilon = end_value * weight * episode_decay\n",
    "    \n",
    "    if np.random.random() < epsilon:\n",
    "        # Select a random action.\n",
    "        action = np.random.randint(low=0, high=num_actions)\n",
    "    else:\n",
    "        # Otherwise select the action that has the highest Q-value.\n",
    "        action = np.argmax(possible_actions)\n",
    "#         print('Selected action {}'.format(action))\n",
    "\n",
    "    return action, epsilon\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0matari_py\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'atari_py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-538b2a14439e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Create the game-environment using OpenAI Gym.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# The number of possible actions that the agent may take in every step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(id)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self, id)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Making new env: %s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;31m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_entry_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_entry_point\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m             \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mentry_point\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEntryPoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x={}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentry_point\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, require, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrequire\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2321\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2322\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2324\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2326\u001b[0m         \u001b[0mResolve\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentry\u001b[0m \u001b[0mpoint\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mits\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2327\u001b[0m         \"\"\"\n\u001b[1;32m-> 2328\u001b[1;33m         \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__name__'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2329\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2330\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\envs\\atari\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matari\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matari_env\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAtariEnv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0matari_py\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mto_ram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)"
     ]
    }
   ],
   "source": [
    "#### Setting the working and environment variables\n",
    "\n",
    "# Game to play with\n",
    "env_name = 'Breakout-v0'\n",
    "\n",
    "# Create the game-environment using OpenAI Gym.\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# The number of possible actions that the agent may take in every step.\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# getting actions names\n",
    "# action_names = env.unwrapped.get_action_meanings()\n",
    "\n",
    "\n",
    "# Height of each image-frame in the state.\n",
    "state_height = 105\n",
    "\n",
    "# Width of each image-frame in the state.\n",
    "state_width = 80\n",
    "\n",
    "# Number of images in the state.\n",
    "state_channels = 2\n",
    "\n",
    "# Number of images in the state.\n",
    "state_channels = 4\n",
    "\n",
    "# Shape of the state-array.\n",
    "state_shape = [state_height, state_width, state_channels]\n",
    "\n",
    "# Set replay memory size based on RAM to be used\n",
    "replay_ram_space = 4\n",
    "\n",
    "# getting replay size based on RAM space available\n",
    "rp_size = int(np.ceil((replay_ram_space*1024*1024)/(state_height*state_width*state_channels*100))*100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Motion tracer for arbitary channels\n",
    "\n",
    "def _pre_process_image(image,state_img_size):\n",
    "    \"\"\"Pre-process a raw image from the game-environment.\"\"\"\n",
    "\n",
    "    # Convert image to gray-scale.\n",
    "    # Get the separate colour-channels.\n",
    "    r, g, b = image[:, :, 0], image[:, :, 1], image[:, :, 2]\n",
    "\n",
    "    # Convert to gray-scale using the Wikipedia formula.\n",
    "    img = 0.2990 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "    # Resize to the desired size using SciPy for convenience.\n",
    "    img = resize(img, output_shape=state_img_size, order = 3, mode='wrap')\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "class MotionTracer:\n",
    "\n",
    "    def __init__(self, image, decay=0.75, num_images=2, state_height=105, state_width = 80):\n",
    "\n",
    "        \n",
    "        # Size of each image in the state.\n",
    "        self.state_img_size = np.array([state_height, state_width])\n",
    "        self.state_shape = [state_height, state_width, num_images] \n",
    "        \n",
    "        # Preprocessing the image\n",
    "        img = _pre_process_image(image,self.state_img_size)\n",
    "  \n",
    "        # initialising image for first set and rest will be zeros\n",
    "#         self.imageset[num_images-1] = img.astype(np.float) \n",
    "        self.last_input = img.astype(np.float)\n",
    "\n",
    "        self.last_output = np.zeros_like(img)\n",
    "        \n",
    "        # Storint the inputs to class\n",
    "        self.decay = decay\n",
    "        self.num_images = num_images\n",
    "\n",
    "    def process(self, image):\n",
    "\n",
    "        img = _pre_process_image(image,self.state_img_size)\n",
    "        img_dif = img - self.last_input\n",
    "        \n",
    "#         [for k in reversed(range(1,num_images-1))]\n",
    "            \n",
    "        # Copy the contents of the input-image to the last input.\n",
    "        self.last_input[:] = img[:]\n",
    "\n",
    "        img_motion = np.where(np.abs(img_dif) > 20, 255.0, 0.0)\n",
    "\n",
    "        # Add some of the previous output. This recurrent formula\n",
    "        # is what gives the trace / tail.\n",
    "        output = img_motion + self.decay * self.last_output\n",
    "\n",
    "        # Ensure the pixel-values are within the allowed bounds.\n",
    "        output = np.clip(output, 0.0, 255.0)\n",
    "\n",
    "        # Set the last output.\n",
    "        self.last_output = output\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_state(self):\n",
    "        \n",
    "        # Stack the last input and output images.\n",
    "        state = np.dstack([self.last_input, self.last_output])\n",
    "\n",
    "        # Convert to 8-bit integer.\n",
    "        # This is done to save space in the replay-memory.\n",
    "        state = state.astype(np.uint8)\n",
    "        return state\n",
    "    \n",
    "    def get_inputs(self):\n",
    "        inputs = np.zeros(shape=[1] + self.state_shape, dtype=np.uint8)\n",
    "        inputs[0:1] = self.get_state()\n",
    "        return inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the neural network architecture\n",
    "\n",
    "from keras.layers import Conv2D, Dense,Flatten, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adagrad\n",
    "\n",
    "def buildmodel(num_actions, batch_size, img_rows=105, img_cols=80, img_channels = 2):\n",
    "    \n",
    "    init = keras.initializers.TruncatedNormal(mean=0.0, stddev=0.02, seed=None)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=16, kernel_size=3, strides=2, padding = 'same',activation = 'relu',\n",
    "                    kernel_initializer = init,data_format=\"channels_last\",\n",
    "                     input_shape=(img_rows, img_cols, img_channels)))\n",
    "    model.add(Conv2D(filters=32, kernel_size=3, strides=2, padding = 'same',activation = 'relu',\n",
    "                    kernel_initializer = init))\n",
    "    model.add(Conv2D(filters=64, kernel_size=2, strides=1, padding = 'same',activation = 'relu',\n",
    "                    kernel_initializer = init))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu',kernel_initializer = init))\n",
    "    model.add(Dense(1024, activation='relu',kernel_initializer = init))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu',kernel_initializer = init))\n",
    "    model.add(Dense(512, activation='relu',kernel_initializer = init))\n",
    "    model.add(Dense(num_actions,activation=None))\n",
    "   \n",
    "    adam = Adagrad(lr=1e-6)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    print(\"Model is built\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One type of Experience foreplay\n",
    "class ExperienceReplay(object):\n",
    "    \n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        # memory[i] = [[state_t, action_t, reward_t, state_t+1], game_over?]\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        \n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = model.output_shape[-1]\n",
    "        env_dim = [self.memory[0][0][0].shape[0], self.memory[0][0][0].shape[1], self.memory[0][0][0].shape[2]]\n",
    "        \n",
    "        \n",
    "#         inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "        inputs = np.zeros(shape=[min(len_memory, batch_size)] + env_dim, dtype=np.uint8)\n",
    "        temp_input = np.zeros(shape=[min(len_memory, batch_size)] + env_dim, dtype=np.uint8)\n",
    "#         print(inputs.shape)\n",
    "        targets = np.zeros((min(len_memory, batch_size), num_actions))\n",
    "        \n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory,size=inputs.shape[0])):\n",
    "            \n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "\n",
    "            inputs[i:i+1] = state_t\n",
    "            temp_input[i:i+1] = self.memory[idx][0][3]\n",
    "            \n",
    "            # There should be no target values for actions not taken.\n",
    "            # Thou shalt not correct actions not taken #deep\n",
    "            targets[i] = model.predict(inputs)[0]\n",
    "            Q_sa = np.max(model.predict(temp_input)[0])\n",
    "            \n",
    "            if game_over:  # if game_over is True\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                # reward_t + gamma * max_a' Q(s', a')\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        return inputs, targets\n",
    "    \n",
    "#     def get_action_from_state(self, state, model):\n",
    "#         env_dim = [self.memory[0][0][0].shape[0], self.memory[0][0][0].shape[1], self.memory[0][0][0].shape[2]]\n",
    "#         inputs = np.zeros(shape=[1] + env_dim, dtype=np.uint8)\n",
    "#         inputs[0:1]= state\n",
    "#         return model.predict(inputs)[0]\n",
    "        \n",
    "    \n",
    "# Another type of Experience foreplay\n",
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dqn(env,model,memory,episodes,iterations,batch_size = 32):\n",
    "    \n",
    "    num_action = env.action_space.n\n",
    "    env.reset()\n",
    "    win_cnt = 0\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        loss = []\n",
    "        game_over = False\n",
    "        env.reset()\n",
    "        \n",
    "        # get initial input\n",
    "#         screen = env.render(mode='rgb_array')\n",
    "#         screen = \n",
    "        \n",
    "        try:\n",
    "            screen = env.render(mode='rgb_array')\n",
    "        except:\n",
    "            screen=env.reset()\n",
    "            print(screen.shape)\n",
    "#         get_screen(env)\n",
    "        motion_tracer = MotionTracer(screen)\n",
    "        \n",
    "        netReward = 0\n",
    "        for itr in range(iterations):\n",
    "            state = motion_tracer.get_state()\n",
    "            possible_actions = model.predict(motion_tracer.get_inputs())[0]\n",
    "            action,_ = select_action(possible_actions=possible_actions, iteration=itr, num_actions=num_action,\n",
    "                                     num_episode=e, num_iterations=iterations)\n",
    "            \n",
    "            img, reward, game_over, info = env.step(action=action)\n",
    "            \n",
    "            if not game_over:\n",
    "                motion_tracer.process(img)\n",
    "                next_state = motion_tracer.get_state()\n",
    "            else:\n",
    "                try:\n",
    "                    motion_tracer.process(np.zeros_like(state))\n",
    "                    next_state = motion_tracer.get_state()\n",
    "                except:\n",
    "                    break\n",
    "            \n",
    "            # Adding to replay memory\n",
    "            memory.remember([state, action, reward, next_state], game_over)\n",
    "\n",
    "            netReward += reward\n",
    "            env.render()\n",
    "            time.sleep(0.05)\n",
    "#             if game_over:\n",
    "#                 break\n",
    "        \n",
    "        inputs, targets = memory.get_batch(model, batch_size=batch_size)\n",
    "        loss.append(model.train_on_batch(inputs, targets))\n",
    "        print(\"Episode {:02d} | Iteration {:03d} | Loss {:.8f} | Win count {}\".format(\n",
    "             e, itr,np.mean(loss), netReward))\n",
    "            \n",
    "        if (e+1) % 100 == 0:\n",
    "            print('Saving Model!')\n",
    "            # Save trained model weights and architecture, this will be used by the visualization code\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 8\n",
    "rp_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is built\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-50f4bd7ad81f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# memory = ReplayMemory(rp_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mrun_dqn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepisodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# Making Model\n",
    "model = buildmodel(num_actions,32)\n",
    "\n",
    "# Making replay memory\n",
    "memory = ExperienceReplay(rp_size)\n",
    "# memory = ReplayMemory(rp_size)\n",
    "\n",
    "run_dqn(env,model,memory,episodes = 1000,iterations =  1000, batch_size = 128 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
