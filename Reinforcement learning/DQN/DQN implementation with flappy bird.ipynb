{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The idea is quite simple, the input is a_t (0 represent don’t flap, 1 represent flap), the API will give you \n",
    "the next frame x_t1_colored, the reward (0.1 if alive, +1 if pass the pipe, -1 if die) and terminal is a boolean \n",
    "flag indicates whether the game is FINISHED or NOT. We also followed DeepMind suggestion to clip the reward \n",
    "between [-1,+1] to improve the stability. I have not yet get a chance to test out different reward functions but \n",
    "it would be an interesting exercise to see how the performance is changed with different reward functions.\n",
    "\n",
    "Interesting readers can modify the reward function in game/wrapped_flappy_bird.py”, \n",
    "under the function **def frame_step(self, input_actions)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrapped_flappy_bird as game\n",
    "x_t1_colored, r_t, terminal = game_state.frame_step(a_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image pre-processing\n",
    "\n",
    "x_t1 = skimage.color.rgb2gray(x_t1_colored) #convert the color image into grayscale\n",
    "x_t1 = skimage.transform.resize(x_t1,(80,80)) # crop down the image size into 80x80 pixel\n",
    "x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range=(0, 255))\n",
    "\n",
    "x_t1 = x_t1.reshape(1, 1, x_t1.shape[0], x_t1.shape[1])\n",
    "s_t1 = np.append(x_t1, s_t[:, :3, :, :], axis=1) #stack 4 frames together before I feed into neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolution Neural Network\n",
    "\n",
    "def buildmodel():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 8, 8, subsample=(4,4),init=lambda shape, name: normal(shape, scale=0.01, name=name), border_mode='same',input_shape=(img_channels,img_rows,img_cols)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 4, 4, subsample=(2,2),init=lambda shape, name: normal(shape, scale=0.01, name=name), border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 3, 3, subsample=(1,1),init=lambda shape, name: normal(shape, scale=0.01, name=name), border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, init=lambda shape, name: normal(shape, scale=0.01, name=name)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2,init=lambda shape, name: normal(shape, scale=0.01, name=name)))\n",
    "   \n",
    "    adam = Adam(lr=1e-6)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    print(\"Model is built\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN\n",
    "if t > OBSERVE:\n",
    "    #sample a minibatch to train on\n",
    "    minibatch = random.sample(D, BATCH)\n",
    "\n",
    "    inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 80, 80, 4\n",
    "    targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "    #Now we do the experience replay\n",
    "    for i in range(0, len(minibatch)):\n",
    "        state_t = minibatch[i][0]\n",
    "        action_t = minibatch[i][1]   #This is action index\n",
    "        reward_t = minibatch[i][2]\n",
    "        state_t1 = minibatch[i][3]\n",
    "        terminal = minibatch[i][4]\n",
    "        # if terminated, only equals reward\n",
    "\n",
    "        inputs[i:i + 1] = state_t    #I saved down s_t\n",
    "\n",
    "        targets[i] = model.predict(state_t)  # Hitting each buttom probability\n",
    "        Q_sa = model.predict(state_t1)\n",
    "\n",
    "        if terminal:\n",
    "            targets[i, action_t] = reward_t\n",
    "        else:\n",
    "            targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "        loss += model.train_on_batch(inputs, targets)\n",
    "\n",
    "    s_t = s_t1\n",
    "    t = t + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploration vs. Exploitation\n",
    "\n",
    "if random.random() <= epsilon:\n",
    "    print(\"----------Random Action----------\")\n",
    "    action_index = random.randrange(ACTIONS)\n",
    "    a_t[action_index] = 1\n",
    "        else:\n",
    "    q = model.predict(s_t)       #input a stack of 4 images, get the prediction\n",
    "    max_Q = np.argmax(q)\n",
    "    action_index = max_Q\n",
    "    a_t[max_Q] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
